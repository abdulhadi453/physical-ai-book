"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[3609],{8198:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"chapter-4/lesson-1","title":"Lesson 4.1.1: Voice Command Processing with Whisper","description":"Overview","source":"@site/docs/chapter-4/lesson-1.md","sourceDirName":"chapter-4","slug":"/chapter-4/lesson-1","permalink":"/docs/chapter-4/lesson-1","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-4/lesson-1.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA) Systems","permalink":"/docs/chapter-4/intro"},"next":{"title":"Lesson 4.1.2: LLM-based Cognitive Planning","permalink":"/docs/chapter-4/lesson-2"}}');var s=r(4848),t=r(8453);const o={},a="Lesson 4.1.1: Voice Command Processing with Whisper",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Voice Processing Architecture",id:"voice-processing-architecture",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Set Up Whisper Environment",id:"step-1-set-up-whisper-environment",level:3},{value:"Step 2: Implement Voice Input Handler",id:"step-2-implement-voice-input-handler",level:3},{value:"Step 3: Create Voice Command Validator",id:"step-3-create-voice-command-validator",level:3},{value:"Step 4: Integrate Components",id:"step-4-integrate-components",level:3},{value:"Practical Exercise",id:"practical-exercise",level:2},{value:"Exercise 1.1: Basic Voice Command Processing",id:"exercise-11-basic-voice-command-processing",level:3},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Whisper Model Selection",id:"whisper-model-selection",level:3},{value:"Confidence Scoring",id:"confidence-scoring",level:3},{value:"Real-time Processing",id:"real-time-processing",level:3},{value:"Common Challenges and Solutions",id:"common-challenges-and-solutions",level:2},{value:"Challenge 1: Audio Quality",id:"challenge-1-audio-quality",level:3},{value:"Challenge 2: Command Ambiguity",id:"challenge-2-command-ambiguity",level:3},{value:"Challenge 3: Real-time Performance",id:"challenge-3-real-time-performance",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-411-voice-command-processing-with-whisper",children:"Lesson 4.1.1: Voice Command Processing with Whisper"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Welcome to Lesson 1 of the Vision-Language-Action (VLA) Systems module! In this lesson, you will implement the foundational component of the VLA system: voice command processing using OpenAI's Whisper for speech-to-text conversion. This component serves as the entry point for the entire VLA pipeline, converting natural language commands from users into structured text that can be processed by the cognitive planning system."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up and configure Whisper for real-time speech-to-text processing"}),"\n",(0,s.jsx)(n.li,{children:"Implement voice input handling with audio preprocessing"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Whisper with the VLA system architecture"}),"\n",(0,s.jsx)(n.li,{children:"Validate voice commands and handle confidence scoring"}),"\n",(0,s.jsx)(n.li,{children:"Process basic voice commands and convert them to text format"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting this lesson, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed Module 1-3 (ROS 2, Digital Twin, NVIDIA Isaac)"}),"\n",(0,s.jsx)(n.li,{children:"Installed Python 3.11+ with necessary packages"}),"\n",(0,s.jsx)(n.li,{children:"Set up OpenAI API access (for Whisper model downloads)"}),"\n",(0,s.jsx)(n.li,{children:"Verified microphone access and permissions"}),"\n",(0,s.jsx)(n.li,{children:"Familiarized yourself with the VLA system architecture"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"voice-processing-architecture",children:"Voice Processing Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The voice processing component follows this architecture:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Microphone \u2192 Audio Preprocessing \u2192 Whisper STT \u2192 Text Validation \u2192 Processed Command\n"})}),"\n",(0,s.jsx)(n.p,{children:"Key considerations include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time processing capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Noise reduction and audio quality enhancement"}),"\n",(0,s.jsx)(n.li,{children:"Confidence scoring for reliability assessment"}),"\n",(0,s.jsx)(n.li,{children:"Error handling for failed recognition attempts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-set-up-whisper-environment",children:"Step 1: Set Up Whisper Environment"}),"\n",(0,s.jsx)(n.p,{children:"First, let's create the necessary directory structure and install dependencies:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Navigate to the VLA source directory\r\ncd physical-ai-book/src/vla\r\n\r\n# Create speech processing directory\r\nmkdir -p speech\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now let's create the Whisper processor module:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# src/vla/speech/whisper_processor.py\r\n\r\nimport whisper\r\nimport torch\r\nimport numpy as np\r\nfrom typing import Optional, Dict, Any\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass WhisperProcessor:\r\n    """\r\n    Handles speech-to-text conversion using OpenAI Whisper model.\r\n    """\r\n\r\n    def __init__(self, model_size: str = "base"):\r\n        """\r\n        Initialize Whisper processor with specified model size.\r\n\r\n        Args:\r\n            model_size: Size of Whisper model (\'tiny\', \'base\', \'small\', \'medium\', \'large\')\r\n        """\r\n        self.model_size = model_size\r\n        self.model = None\r\n        self._load_model()\r\n\r\n    def _load_model(self):\r\n        """Load the Whisper model based on specified size."""\r\n        try:\r\n            # Check if CUDA is available for GPU acceleration\r\n            device = "cuda" if torch.cuda.is_available() else "cpu"\r\n            logger.info(f"Loading Whisper {self.model_size} model on {device}")\r\n\r\n            self.model = whisper.load_model(self.model_size, device=device)\r\n            logger.info("Whisper model loaded successfully")\r\n        except Exception as e:\r\n            logger.error(f"Failed to load Whisper model: {e}")\r\n            raise\r\n\r\n    def transcribe_audio(self, audio_data: np.ndarray, language: str = "en") -> Dict[str, Any]:\r\n        """\r\n        Transcribe audio data to text using Whisper.\r\n\r\n        Args:\r\n            audio_data: Audio data as numpy array (expected 16kHz sample rate)\r\n            language: Language code for transcription (default: \'en\')\r\n\r\n        Returns:\r\n            Dictionary containing transcribed text and confidence metrics\r\n        """\r\n        try:\r\n            # Ensure audio is in the right format (16kHz)\r\n            if len(audio_data) == 0:\r\n                return {\r\n                    "text": "",\r\n                    "confidence": 0.0,\r\n                    "language": language,\r\n                    "success": False,\r\n                    "error": "Empty audio data"\r\n                }\r\n\r\n            # Run transcription\r\n            result = self.model.transcribe(\r\n                audio_data,\r\n                language=language,\r\n                temperature=0.0  # Deterministic output\r\n            )\r\n\r\n            # Extract confidence information (Whisper doesn\'t provide direct confidence)\r\n            # We\'ll use alternative methods to estimate reliability\r\n            confidence = self._estimate_confidence(result)\r\n\r\n            return {\r\n                "text": result["text"].strip(),\r\n                "confidence": confidence,\r\n                "language": language,\r\n                "success": True,\r\n                "segments": result.get("segments", []),\r\n                "processing_time": result.get("processing_time", 0.0)\r\n            }\r\n        except Exception as e:\r\n            logger.error(f"Transcription failed: {e}")\r\n            return {\r\n                "text": "",\r\n                "confidence": 0.0,\r\n                "language": language,\r\n                "success": False,\r\n                "error": str(e)\r\n            }\r\n\r\n    def _estimate_confidence(self, result) -> float:\r\n        """\r\n        Estimate confidence based on various factors since Whisper doesn\'t provide direct confidence.\r\n\r\n        Args:\r\n            result: Whisper transcription result\r\n\r\n        Returns:\r\n            Estimated confidence score (0.0 to 1.0)\r\n        """\r\n        # For now, return a simple confidence estimate\r\n        # In a production system, you might use more sophisticated methods\r\n        text = result.get("text", "")\r\n\r\n        if not text or len(text.strip()) == 0:\r\n            return 0.0\r\n\r\n        # Simple heuristics for confidence estimation\r\n        # - Longer text might indicate more confidence\r\n        # - Presence of common command words\r\n        text_length = len(text)\r\n        confidence = min(0.3 + (text_length * 0.01), 1.0)  # Basic length-based estimate\r\n\r\n        # Adjust based on common command patterns\r\n        command_indicators = ["move", "go", "pick", "place", "grasp", "navigate", "turn", "stop"]\r\n        if any(indicator in text.lower() for indicator in command_indicators):\r\n            confidence = min(confidence + 0.2, 1.0)\r\n\r\n        return confidence\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-implement-voice-input-handler",children:"Step 2: Implement Voice Input Handler"}),"\n",(0,s.jsx)(n.p,{children:"Next, we'll create a module to handle audio capture from the microphone:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# src/vla/speech/voice_input_handler.py\r\n\r\nimport pyaudio\r\nimport numpy as np\r\nimport threading\r\nimport queue\r\nimport time\r\nfrom typing import Optional, Callable\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass VoiceInputHandler:\r\n    """\r\n    Handles audio input from microphone for voice command processing.\r\n    """\r\n\r\n    def __init__(self,\r\n                 sample_rate: int = 16000,\r\n                 chunk_size: int = 1024,\r\n                 max_buffer_size: int = 10):\r\n        """\r\n        Initialize voice input handler.\r\n\r\n        Args:\r\n            sample_rate: Audio sample rate (default: 16kHz for Whisper compatibility)\r\n            chunk_size: Size of audio chunks to process\r\n            max_buffer_size: Maximum number of chunks to buffer\r\n        """\r\n        self.sample_rate = sample_rate\r\n        self.chunk_size = chunk_size\r\n        self.max_buffer_size = max_buffer_size\r\n\r\n        # Audio stream parameters\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n\r\n        # Audio processing\r\n        self.audio = pyaudio.PyAudio()\r\n        self.stream = None\r\n        self.is_recording = False\r\n        self.audio_queue = queue.Queue(maxsize=max_buffer_size)\r\n\r\n        # Callback for processed audio\r\n        self.process_callback: Optional[Callable] = None\r\n\r\n    def start_recording(self, callback: Optional[Callable] = None):\r\n        """\r\n        Start recording audio from microphone.\r\n\r\n        Args:\r\n            callback: Function to call when audio is ready for processing\r\n        """\r\n        if self.is_recording:\r\n            logger.warning("Recording already in progress")\r\n            return\r\n\r\n        self.process_callback = callback\r\n        self.is_recording = True\r\n\r\n        # Open audio stream\r\n        try:\r\n            self.stream = self.audio.open(\r\n                format=self.format,\r\n                channels=self.channels,\r\n                rate=self.sample_rate,\r\n                input=True,\r\n                frames_per_buffer=self.chunk_size\r\n            )\r\n\r\n            logger.info("Started recording from microphone")\r\n\r\n            # Start recording thread\r\n            self.recording_thread = threading.Thread(target=self._record_audio)\r\n            self.recording_thread.daemon = True\r\n            self.recording_thread.start()\r\n\r\n        except Exception as e:\r\n            logger.error(f"Failed to start recording: {e}")\r\n            self.is_recording = False\r\n            raise\r\n\r\n    def stop_recording(self):\r\n        """Stop recording audio."""\r\n        if not self.is_recording:\r\n            return\r\n\r\n        self.is_recording = False\r\n\r\n        if self.stream:\r\n            self.stream.stop_stream()\r\n            self.stream.close()\r\n\r\n        logger.info("Stopped recording")\r\n\r\n    def _record_audio(self):\r\n        """Internal method to record audio in a separate thread."""\r\n        try:\r\n            while self.is_recording:\r\n                # Read audio data\r\n                data = self.stream.read(self.chunk_size, exception_on_overflow=False)\r\n\r\n                # Convert to numpy array\r\n                audio_array = np.frombuffer(data, dtype=np.int16)\r\n                audio_array = audio_array.astype(np.float32) / 32768.0  # Normalize to [-1, 1]\r\n\r\n                # Add to processing queue\r\n                try:\r\n                    self.audio_queue.put_nowait(audio_array)\r\n                except queue.Full:\r\n                    # Drop oldest audio if queue is full\r\n                    try:\r\n                        self.audio_queue.get_nowait()\r\n                        self.audio_queue.put_nowait(audio_array)\r\n                    except queue.Empty:\r\n                        pass\r\n\r\n                # Process accumulated audio if callback is set\r\n                if self.process_callback:\r\n                    self._process_accumulated_audio()\r\n\r\n        except Exception as e:\r\n            logger.error(f"Error in audio recording thread: {e}")\r\n        finally:\r\n            self.is_recording = False\r\n\r\n    def _process_accumulated_audio(self):\r\n        """Process accumulated audio chunks."""\r\n        if self.process_callback and not self.audio_queue.empty():\r\n            # Collect all available audio data\r\n            audio_data = []\r\n            try:\r\n                while True:\r\n                    chunk = self.audio_queue.get_nowait()\r\n                    audio_data.append(chunk)\r\n            except queue.Empty:\r\n                pass\r\n\r\n            if audio_data:\r\n                # Concatenate all chunks\r\n                full_audio = np.concatenate(audio_data)\r\n\r\n                # Call the processing callback\r\n                self.process_callback(full_audio)\r\n\r\n    def get_audio_buffer(self) -> Optional[np.ndarray]:\r\n        """\r\n        Get accumulated audio data from the buffer.\r\n\r\n        Returns:\r\n            Concatenated audio data as numpy array, or None if no data available\r\n        """\r\n        if self.audio_queue.empty():\r\n            return None\r\n\r\n        audio_data = []\r\n        try:\r\n            while True:\r\n                chunk = self.audio_queue.get_nowait()\r\n                audio_data.append(chunk)\r\n        except queue.Empty:\r\n            pass\r\n\r\n        if audio_data:\r\n            return np.concatenate(audio_data)\r\n\r\n        return None\r\n\r\n    def __del__(self):\r\n        """Cleanup audio resources."""\r\n        if hasattr(self, \'audio\') and self.audio:\r\n            self.audio.terminate()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-create-voice-command-validator",children:"Step 3: Create Voice Command Validator"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a validation module to ensure voice commands are properly formatted:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# src/vla/utils/validators.py\r\n\r\nimport re\r\nfrom typing import Dict, Any, List\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass VoiceCommandValidator:\r\n    """\r\n    Validates voice commands to ensure they meet VLA system requirements.\r\n    """\r\n\r\n    def __init__(self):\r\n        # Define valid command patterns\r\n        self.command_patterns = [\r\n            # Navigation commands\r\n            r"move\\s+(forward|backward|left|right|up|down)",\r\n            r"go\\s+to\\s+\\w+",\r\n            r"navigate\\s+to\\s+\\w+",\r\n            r"turn\\s+(left|right|around)",\r\n\r\n            # Manipulation commands\r\n            r"pick\\s+up\\s+\\w+",\r\n            r"grasp\\s+\\w+",\r\n            r"take\\s+\\w+",\r\n            r"place\\s+\\w+\\s+on\\s+\\w+",\r\n            r"put\\s+\\w+\\s+on\\s+\\w+",\r\n\r\n            # Object interaction\r\n            r"find\\s+\\w+",\r\n            r"look\\s+at\\s+\\w+",\r\n            r"identify\\s+\\w+",\r\n\r\n            # Complex commands\r\n            r"pick\\s+up\\s+\\w+\\s+and\\s+place\\s+it\\s+on\\s+\\w+",\r\n            r"go\\s+to\\s+\\w+\\s+and\\s+pick\\s+up\\s+\\w+"\r\n        ]\r\n\r\n    def validate_command(self, command_text: str, confidence: float) -> Dict[str, Any]:\r\n        """\r\n        Validate a voice command.\r\n\r\n        Args:\r\n            command_text: The transcribed command text\r\n            confidence: Confidence score from speech recognition\r\n\r\n        Returns:\r\n            Dictionary with validation results\r\n        """\r\n        result = {\r\n            "is_valid": False,\r\n            "errors": [],\r\n            "warnings": [],\r\n            "processed_command": None,\r\n            "command_type": None\r\n        }\r\n\r\n        # Check if text is empty\r\n        if not command_text or len(command_text.strip()) == 0:\r\n            result["errors"].append("Command text is empty")\r\n            return result\r\n\r\n        # Check confidence threshold\r\n        if confidence < 0.5:\r\n            result["warnings"].append(f"Low confidence score: {confidence:.2f}")\r\n\r\n        # Check for basic command structure\r\n        if len(command_text.split()) < 2:\r\n            result["errors"].append("Command too short - minimum 2 words required")\r\n\r\n        # Check against valid patterns\r\n        command_lower = command_text.lower()\r\n        matched_pattern = False\r\n\r\n        for pattern in self.command_patterns:\r\n            if re.search(pattern, command_lower):\r\n                matched_pattern = True\r\n                # Determine command type based on pattern\r\n                if any(word in command_lower for word in ["move", "go", "navigate", "turn"]):\r\n                    result["command_type"] = "navigation"\r\n                elif any(word in command_lower for word in ["pick", "grasp", "take", "place", "put"]):\r\n                    result["command_type"] = "manipulation"\r\n                elif any(word in command_lower for word in ["find", "look", "identify"]):\r\n                    result["command_type"] = "inspection"\r\n                else:\r\n                    result["command_type"] = "general"\r\n                break\r\n\r\n        if not matched_pattern:\r\n            result["warnings"].append("Command pattern not recognized, may not be supported")\r\n\r\n        # If we have warnings but no errors, consider it valid with warnings\r\n        if not result["errors"]:\r\n            result["is_valid"] = True\r\n            result["processed_command"] = command_text.strip()\r\n\r\n        return result\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-integrate-components",children:"Step 4: Integrate Components"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a main module that integrates all voice processing components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# src/vla/speech/vla_voice_processor.py\r\n\r\nimport numpy as np\r\nimport time\r\nfrom typing import Dict, Any, Optional, Callable\r\nimport logging\r\n\r\nfrom src.vla.speech.whisper_processor import WhisperProcessor\r\nfrom src.vla.speech.voice_input_handler import VoiceInputHandler\r\nfrom src.vla.utils.validators import VoiceCommandValidator\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass VLAVoiceProcessor:\r\n    """\r\n    Main voice processing module for the VLA system.\r\n    Integrates Whisper, voice input, and validation components.\r\n    """\r\n\r\n    def __init__(self, model_size: str = "base"):\r\n        """\r\n        Initialize the VLA voice processor.\r\n\r\n        Args:\r\n            model_size: Whisper model size to use\r\n        """\r\n        self.whisper_processor = WhisperProcessor(model_size=model_size)\r\n        self.voice_handler = VoiceInputHandler()\r\n        self.validator = VoiceCommandValidator()\r\n\r\n        self.is_active = False\r\n        self.command_callback: Optional[Callable] = None\r\n\r\n    def start_listening(self, command_callback: Callable[[Dict[str, Any]], None]):\r\n        """\r\n        Start listening for voice commands.\r\n\r\n        Args:\r\n            command_callback: Function to call when a command is processed\r\n        """\r\n        self.command_callback = command_callback\r\n        self.is_active = True\r\n\r\n        # Start voice input with processing callback\r\n        self.voice_handler.start_recording(self._process_audio_chunk)\r\n\r\n    def stop_listening(self):\r\n        """Stop listening for voice commands."""\r\n        self.is_active = False\r\n        self.voice_handler.stop_recording()\r\n\r\n    def _process_audio_chunk(self, audio_data: np.ndarray):\r\n        """\r\n        Process an audio chunk through the voice pipeline.\r\n\r\n        Args:\r\n            audio_data: Audio data to process\r\n        """\r\n        if not self.is_active:\r\n            return\r\n\r\n        try:\r\n            # Transcribe audio to text\r\n            transcription_result = self.whisper_processor.transcribe_audio(audio_data)\r\n\r\n            if transcription_result["success"] and transcription_result["text"]:\r\n                # Validate the command\r\n                validation_result = self.validator.validate_command(\r\n                    transcription_result["text"],\r\n                    transcription_result["confidence"]\r\n                )\r\n\r\n                # Create command result\r\n                command_result = {\r\n                    "text": transcription_result["text"],\r\n                    "confidence": transcription_result["confidence"],\r\n                    "is_valid": validation_result["is_valid"],\r\n                    "validation_errors": validation_result["errors"],\r\n                    "validation_warnings": validation_result["warnings"],\r\n                    "command_type": validation_result["command_type"],\r\n                    "timestamp": time.time(),\r\n                    "processing_details": {\r\n                        "transcription_time": transcription_result.get("processing_time", 0.0),\r\n                        "segments": transcription_result.get("segments", [])\r\n                    }\r\n                }\r\n\r\n                # Call the command callback if available\r\n                if self.command_callback:\r\n                    self.command_callback(command_result)\r\n\r\n        except Exception as e:\r\n            logger.error(f"Error processing audio chunk: {e}")\r\n\r\n    def process_voice_command(self, audio_data: np.ndarray) -> Dict[str, Any]:\r\n        """\r\n        Process a single voice command synchronously.\r\n\r\n        Args:\r\n            audio_data: Audio data to process\r\n\r\n        Returns:\r\n            Dictionary with command processing results\r\n        """\r\n        # Transcribe audio\r\n        transcription_result = self.whisper_processor.transcribe_audio(audio_data)\r\n\r\n        if not transcription_result["success"]:\r\n            return {\r\n                "success": False,\r\n                "error": transcription_result.get("error", "Transcription failed"),\r\n                "confidence": 0.0\r\n            }\r\n\r\n        # Validate command\r\n        validation_result = self.validator.validate_command(\r\n            transcription_result["text"],\r\n            transcription_result["confidence"]\r\n        )\r\n\r\n        return {\r\n            "success": True,\r\n            "text": transcription_result["text"],\r\n            "confidence": transcription_result["confidence"],\r\n            "is_valid": validation_result["is_valid"],\r\n            "validation_errors": validation_result["errors"],\r\n            "validation_warnings": validation_result["warnings"],\r\n            "command_type": validation_result["command_type"],\r\n            "timestamp": time.time()\r\n        }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-11-basic-voice-command-processing",children:"Exercise 1.1: Basic Voice Command Processing"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Setup"}),": Create a simple test script that initializes the Whisper processor and voice input handler."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implementation"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# test_voice_processing.py\r\nimport time\r\nimport numpy as np\r\nfrom src.vla.speech.vla_voice_processor import VLAVoiceProcessor\r\n\r\ndef main():\r\n    # Initialize the VLA voice processor\r\n    voice_processor = VLAVoiceProcessor(model_size="base")\r\n\r\n    print("VLA Voice Processing system initialized")\r\n    print("Say a command like \'move forward\' or \'pick up the cube\'")\r\n\r\n    # Callback function for processed commands\r\n    def command_callback(command_result):\r\n        print(f"\\n--- New Command Processed ---")\r\n        print(f"Text: \'{command_result[\'text\']}\'")\r\n        print(f"Confidence: {command_result[\'confidence\']:.2f}")\r\n        print(f"Valid: {command_result[\'is_valid\']}")\r\n        print(f"Type: {command_result[\'command_type\']}")\r\n\r\n        if command_result[\'validation_warnings\']:\r\n            print(f"Warnings: {command_result[\'validation_warnings\']}")\r\n        if command_result[\'validation_errors\']:\r\n            print(f"Errors: {command_result[\'validation_errors\']}")\r\n\r\n    # Start listening\r\n    voice_processor.start_listening(command_callback)\r\n\r\n    try:\r\n        # Keep the program running for 30 seconds\r\n        print("\\nListening for voice commands (30 seconds)...")\r\n        time.sleep(30)\r\n    except KeyboardInterrupt:\r\n        print("\\nStopping...")\r\n    finally:\r\n        voice_processor.stop_listening()\r\n        print("Voice processing stopped")\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Testing"}),': Run the test script and speak simple commands like "move forward" or "pick up the red cube". Observe the transcription, confidence scores, and validation results.']}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"whisper-model-selection",children:"Whisper Model Selection"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tiny/Small"}),": Faster but less accurate, good for real-time applications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base/Medium"}),": Balance of speed and accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large"}),": Most accurate but slower, best for high-quality requirements"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"confidence-scoring",children:"Confidence Scoring"}),"\n",(0,s.jsx)(n.p,{children:"Whisper doesn't provide direct confidence scores, so we estimate reliability using:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Text length and structure"}),"\n",(0,s.jsx)(n.li,{children:"Presence of common command patterns"}),"\n",(0,s.jsx)(n.li,{children:"Audio quality indicators"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"real-time-processing",children:"Real-time Processing"}),"\n",(0,s.jsx)(n.p,{children:"The voice input handler uses threading to capture audio continuously while allowing other system components to operate concurrently."}),"\n",(0,s.jsx)(n.h2,{id:"common-challenges-and-solutions",children:"Common Challenges and Solutions"}),"\n",(0,s.jsx)(n.h3,{id:"challenge-1-audio-quality",children:"Challenge 1: Audio Quality"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Background noise affects transcription quality.\r\n",(0,s.jsx)(n.strong,{children:"Solution"}),": Implement noise reduction algorithms or use higher-quality microphones."]}),"\n",(0,s.jsx)(n.h3,{id:"challenge-2-command-ambiguity",children:"Challenge 2: Command Ambiguity"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),': Natural language can be ambiguous ("that thing over there").\r\n',(0,s.jsx)(n.strong,{children:"Solution"}),": Use visual context to disambiguate references (covered in Lesson 3)."]}),"\n",(0,s.jsx)(n.h3,{id:"challenge-3-real-time-performance",children:"Challenge 3: Real-time Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Processing delays make the system feel unresponsive.\r\n",(0,s.jsx)(n.strong,{children:"Solution"}),": Optimize model size, use GPU acceleration, implement audio buffering."]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What is the primary role of the Whisper processor in the VLA system?"}),"\n",(0,s.jsx)(n.li,{children:"How does the system estimate confidence when Whisper doesn't provide direct confidence scores?"}),"\n",(0,s.jsx)(n.li,{children:"Why is it important to validate voice commands before passing them to the planning system?"}),"\n",(0,s.jsx)(n.li,{children:"What are the advantages of using threading for audio capture?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this lesson, you have implemented the foundational voice command processing system for the VLA system. You learned how to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Set up and configure Whisper for speech-to-text conversion"}),"\n",(0,s.jsx)(n.li,{children:"Handle real-time audio input from a microphone"}),"\n",(0,s.jsx)(n.li,{children:"Validate voice commands for proper structure and content"}),"\n",(0,s.jsx)(n.li,{children:"Integrate all components into a cohesive voice processing pipeline"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This voice processing system serves as the entry point for the entire VLA pipeline, converting natural language commands into structured text that can be processed by subsequent components. In the next lesson, you will integrate this voice processing system with LLM-based cognitive planning to interpret commands and generate action sequences."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(6540);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);