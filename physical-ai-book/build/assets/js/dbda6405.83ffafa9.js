"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[1895],{3687:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"chapter-4/lesson-3","title":"Lesson 4.1.3: Visual Perception Integration","description":"Overview","source":"@site/docs/chapter-4/lesson-3.md","sourceDirName":"chapter-4","slug":"/chapter-4/lesson-3","permalink":"/docs/chapter-4/lesson-3","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-4/lesson-3.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.1.2: LLM-based Cognitive Planning","permalink":"/docs/chapter-4/lesson-2"},"next":{"title":"Lesson 4.1.4: ROS 2 Action Execution","permalink":"/docs/chapter-4/lesson-4"}}');var o=r(4848),i=r(8453);const s={},a="Lesson 4.1.3: Visual Perception Integration",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Visual Perception Architecture",id:"visual-perception-architecture",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Set Up Object Detection Module",id:"step-1-set-up-object-detection-module",level:3},{value:"Step 2: Implement Perception Pipeline",id:"step-2-implement-perception-pipeline",level:3},{value:"Step 3: Create Vision Processor",id:"step-3-create-vision-processor",level:3},{value:"Step 4: Integrate with VLA System",id:"step-4-integrate-with-vla-system",level:3},{value:"Practical Exercise",id:"practical-exercise",level:2},{value:"Exercise 3.1: Visual Perception Integration",id:"exercise-31-visual-perception-integration",level:3},{value:"Key Concepts",id:"key-concepts",level:2},{value:"3D Position Estimation",id:"3d-position-estimation",level:3},{value:"Object Recognition and Context",id:"object-recognition-and-context",level:3},{value:"Real-time Processing",id:"real-time-processing",level:3},{value:"Common Challenges and Solutions",id:"common-challenges-and-solutions",level:2},{value:"Challenge 1: Depth Estimation",id:"challenge-1-depth-estimation",level:3},{value:"Challenge 2: Object Occlusion",id:"challenge-2-object-occlusion",level:3},{value:"Challenge 3: Lighting Conditions",id:"challenge-3-lighting-conditions",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lesson-413-visual-perception-integration",children:"Lesson 4.1.3: Visual Perception Integration"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Welcome to Lesson 3 of the Vision-Language-Action (VLA) Systems module! In this lesson, you will implement the visual perception component of the VLA system that integrates computer vision techniques with the cognitive planning system to enable object detection, spatial reasoning, and visual context understanding. This component provides critical information that allows the VLA system to understand its environment and execute commands that reference specific objects or locations."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement object detection using computer vision libraries"}),"\n",(0,o.jsx)(n.li,{children:"Integrate visual perception with the cognitive planning system"}),"\n",(0,o.jsx)(n.li,{children:"Perform 3D position estimation for object manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Handle spatial relationships between detected objects"}),"\n",(0,o.jsx)(n.li,{children:"Create a perception pipeline for real-time processing"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before starting this lesson, ensure you have:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Completed Lesson 1-2 (Voice Processing and Cognitive Planning)"}),"\n",(0,o.jsx)(n.li,{children:"Installed computer vision dependencies (OpenCV, PyTorch, Ultralytics YOLO)"}),"\n",(0,o.jsx)(n.li,{children:"Familiarized yourself with the VLA system architecture"}),"\n",(0,o.jsx)(n.li,{children:"Understood basic concepts of object detection and computer vision"}),"\n",(0,o.jsx)(n.li,{children:"Set up appropriate camera or simulation environment"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"visual-perception-architecture",children:"Visual Perception Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The visual perception component follows this architecture:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Camera Input \u2192 Object Detection \u2192 3D Position Estimation \u2192 Object Context \u2192 Cognitive Planner\n"})}),"\n",(0,o.jsx)(n.p,{children:"Key considerations include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Real-time object detection and classification"}),"\n",(0,o.jsx)(n.li,{children:"3D position estimation relative to robot"}),"\n",(0,o.jsx)(n.li,{children:"Spatial relationship computation between objects"}),"\n",(0,o.jsx)(n.li,{children:"Integration with cognitive planning for command disambiguation"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,o.jsx)(n.h3,{id:"step-1-set-up-object-detection-module",children:"Step 1: Set Up Object Detection Module"}),"\n",(0,o.jsx)(n.p,{children:"First, let's create the core object detection module:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# src/vla/vision/object_detector.py\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nfrom typing import List, Dict, Any, Optional, Tuple\r\nfrom src.vla.models.detected_object import DetectedObject, Point3D, BoundingBox\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass ObjectDetector:\r\n    \"\"\"\r\n    Object detection module using YOLO or similar computer vision models.\r\n    \"\"\"\r\n\r\n    def __init__(self, model_path: Optional[str] = None, confidence_threshold: float = 0.5):\r\n        \"\"\"\r\n        Initialize object detector.\r\n\r\n        Args:\r\n            model_path: Path to pre-trained model (uses default YOLOv8n if None)\r\n            confidence_threshold: Minimum confidence for detection\r\n        \"\"\"\r\n        self.confidence_threshold = confidence_threshold\r\n        self.model = self._load_model(model_path)\r\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n\r\n        # COCO dataset class names (first 80 classes)\r\n        self.class_names = [\r\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\r\n            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\r\n            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\r\n            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\r\n            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\r\n            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\r\n            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\r\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\r\n            'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\r\n            'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\r\n        ]\r\n\r\n    def _load_model(self, model_path: Optional[str]):\r\n        \"\"\"\r\n        Load the object detection model.\r\n\r\n        Args:\r\n            model_path: Path to model file (None for default)\r\n\r\n        Returns:\r\n            Loaded model object\r\n        \"\"\"\r\n        try:\r\n            # Using YOLOv8 as an example (you might need to install ultralytics)\r\n            from ultralytics import YOLO\r\n\r\n            if model_path:\r\n                model = YOLO(model_path)\r\n            else:\r\n                # Use default YOLOv8n model\r\n                model = YOLO('yolov8n.pt')\r\n\r\n            # Move model to appropriate device\r\n            model.to(self.device)\r\n            logger.info(f\"Object detection model loaded on {self.device}\")\r\n            return model\r\n        except ImportError:\r\n            logger.warning(\"Ultralytics not available, using mock detector\")\r\n            return None\r\n        except Exception as e:\r\n            logger.error(f\"Failed to load object detection model: {e}\")\r\n            raise\r\n\r\n    def detect_objects(self, image: np.ndarray) -> List[DetectedObject]:\r\n        \"\"\"\r\n        Detect objects in an image.\r\n\r\n        Args:\r\n            image: Input image as numpy array (BGR format from OpenCV)\r\n\r\n        Returns:\r\n            List of DetectedObject instances\r\n        \"\"\"\r\n        if self.model is None:\r\n            # Mock implementation for testing\r\n            return self._mock_detection(image)\r\n\r\n        try:\r\n            # Perform detection\r\n            results = self.model(image, conf=self.confidence_threshold)\r\n\r\n            detected_objects = []\r\n            for result in results:\r\n                boxes = result.boxes\r\n                if boxes is not None:\r\n                    for box in boxes:\r\n                        # Extract bounding box coordinates\r\n                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\r\n                        confidence = float(box.conf[0])\r\n                        class_id = int(box.cls[0])\r\n\r\n                        # Create bounding box\r\n                        bbox = BoundingBox(\r\n                            x_min=int(x1),\r\n                            y_min=int(y1),\r\n                            x_max=int(x2),\r\n                            y_max=int(y2)\r\n                        )\r\n\r\n                        # Get class name\r\n                        class_name = self.class_names[class_id] if class_id < len(self.class_names) else f\"unknown_{class_id}\"\r\n\r\n                        # Create DetectedObject instance\r\n                        detected_obj = DetectedObject(\r\n                            id=f\"obj_{len(detected_objects)}\",\r\n                            class_name=class_name,\r\n                            confidence=confidence,\r\n                            bbox=bbox,\r\n                            position_3d=Point3D(x=0.0, y=0.0, z=0.0),  # Will be updated with 3D estimation\r\n                            dimensions=None,  # Will be estimated separately\r\n                            color=self._estimate_color(image, bbox),\r\n                            is_graspable=self._is_graspable(class_name)\r\n                        )\r\n\r\n                        detected_objects.append(detected_obj)\r\n\r\n            return detected_objects\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Object detection failed: {e}\")\r\n            return []\r\n\r\n    def _estimate_color(self, image: np.ndarray, bbox: BoundingBox) -> str:\r\n        \"\"\"\r\n        Estimate the dominant color in a bounding box region.\r\n\r\n        Args:\r\n            image: Input image\r\n            bbox: Bounding box coordinates\r\n\r\n        Returns:\r\n            Estimated color name\r\n        \"\"\"\r\n        try:\r\n            # Extract region of interest\r\n            roi = image[bbox.y_min:bbox.y_max, bbox.x_min:bbox.x_max]\r\n\r\n            if roi.size == 0:\r\n                return \"unknown\"\r\n\r\n            # Convert BGR to HSV for better color analysis\r\n            hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\r\n\r\n            # Calculate histogram\r\n            hist = cv2.calcHist([hsv], [0, 1], None, [50, 50], [0, 180, 0, 256])\r\n            hist = cv2.normalize(hist, hist).flatten()\r\n\r\n            # Find dominant hue\r\n            dominant_hue = np.argmax(hist) * (180 / 50)  # Scale back to hue range\r\n\r\n            # Map hue to color names\r\n            if 0 <= dominant_hue < 15 or dominant_hue >= 165:\r\n                return \"red\"\r\n            elif 15 <= dominant_hue < 45:\r\n                return \"orange\"\r\n            elif 45 <= dominant_hue < 75:\r\n                return \"yellow\"\r\n            elif 75 <= dominant_hue < 105:\r\n                return \"green\"\r\n            elif 105 <= dominant_hue < 135:\r\n                return \"blue\"\r\n            elif 135 <= dominant_hue < 165:\r\n                return \"purple\"\r\n            else:\r\n                return \"unknown\"\r\n\r\n        except Exception as e:\r\n            logger.warning(f\"Color estimation failed: {e}\")\r\n            return \"unknown\"\r\n\r\n    def _is_graspable(self, class_name: str) -> bool:\r\n        \"\"\"\r\n        Determine if an object is likely graspable based on class name.\r\n\r\n        Args:\r\n            class_name: Object class name\r\n\r\n        Returns:\r\n            True if object is likely graspable\r\n        \"\"\"\r\n        graspable_classes = [\r\n            'bottle', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\r\n            'orange', 'hot dog', 'pizza', 'donut', 'cake', 'book', 'cell phone',\r\n            'teddy bear', 'scissors', 'remote', 'mouse', 'keyboard', 'laptop'\r\n        ]\r\n        return class_name in graspable_classes\r\n\r\n    def _mock_detection(self, image: np.ndarray) -> List[DetectedObject]:\r\n        \"\"\"\r\n        Mock detection for testing when model is not available.\r\n\r\n        Args:\r\n            image: Input image\r\n\r\n        Returns:\r\n            List of mock DetectedObject instances\r\n        \"\"\"\r\n        logger.warning(\"Using mock object detection - install ultralytics for real detection\")\r\n\r\n        # Create some mock objects for testing\r\n        mock_objects = [\r\n            DetectedObject(\r\n                id=\"mock_cube_1\",\r\n                class_name=\"cube\",\r\n                confidence=0.85,\r\n                bbox=BoundingBox(x_min=100, y_min=100, x_max=150, y_max=150),\r\n                position_3d=Point3D(x=1.0, y=0.5, z=0.0),\r\n                dimensions=None,\r\n                color=\"red\",\r\n                is_graspable=True\r\n            ),\r\n            DetectedObject(\r\n                id=\"mock_sphere_1\",\r\n                class_name=\"sphere\",\r\n                confidence=0.78,\r\n                bbox=BoundingBox(x_min=200, y_min=120, x_max=250, y_max=170),\r\n                position_3d=Point3D(x=1.2, y=0.8, z=0.0),\r\n                dimensions=None,\r\n                color=\"blue\",\r\n                is_graspable=True\r\n            )\r\n        ]\r\n        return mock_objects\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-2-implement-perception-pipeline",children:"Step 2: Implement Perception Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"Now let's create the perception pipeline that integrates detection with 3D position estimation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# src/vla/vision/perception_pipeline.py\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom typing import List, Dict, Any, Optional\r\nfrom src.vla.models.detected_object import DetectedObject\r\nfrom src.vla.models.perception_data import PerceptionData\r\nfrom src.vla.vision.object_detector import ObjectDetector\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass PerceptionPipeline:\r\n    """\r\n    Complete perception pipeline that processes camera input and provides\r\n    object detection results with 3D position estimation.\r\n    """\r\n\r\n    def __init__(self,\r\n                 object_detector: ObjectDetector,\r\n                 camera_matrix: Optional[np.ndarray] = None,\r\n                 distortion_coeffs: Optional[np.ndarray] = None):\r\n        """\r\n        Initialize perception pipeline.\r\n\r\n        Args:\r\n            object_detector: Object detection module\r\n            camera_matrix: Camera intrinsic matrix (if None, uses mock values)\r\n            distortion_coeffs: Camera distortion coefficients (if None, assumes no distortion)\r\n        """\r\n        self.object_detector = object_detector\r\n        self.camera_matrix = camera_matrix or self._get_mock_camera_matrix()\r\n        self.distortion_coeffs = distortion_coeffs or np.zeros((4, 1))\r\n\r\n    def _get_mock_camera_matrix(self) -> np.ndarray:\r\n        """\r\n        Create a mock camera matrix for testing.\r\n\r\n        Returns:\r\n            Mock camera intrinsic matrix\r\n        """\r\n        # Standard camera matrix for 640x480 image with 60 degree FOV\r\n        return np.array([\r\n            [640, 0, 320],  # fx, 0, cx\r\n            [0, 640, 240],  # 0, fy, cy\r\n            [0, 0, 1]       # 0, 0, 1\r\n        ])\r\n\r\n    def process_frame(self,\r\n                     image: np.ndarray,\r\n                     robot_pose: Optional[Dict[str, Any]] = None) -> PerceptionData:\r\n        """\r\n        Process a single camera frame to detect objects and estimate 3D positions.\r\n\r\n        Args:\r\n            image: Input camera image (BGR format)\r\n            robot_pose: Current robot pose (position and orientation)\r\n\r\n        Returns:\r\n            PerceptionData with detected objects and metadata\r\n        """\r\n        try:\r\n            # Detect objects in the image\r\n            detected_objects = self.object_detector.detect_objects(image)\r\n\r\n            # Estimate 3D positions for detected objects\r\n            objects_with_3d = self._estimate_3d_positions(detected_objects, image, robot_pose)\r\n\r\n            # Create PerceptionData instance\r\n            perception_data = PerceptionData(\r\n                id=f"perception_{int(time.time())}",\r\n                timestamp=time.time(),\r\n                objects=objects_with_3d,\r\n                camera_pose=robot_pose,  # Simplified - in reality would be separate\r\n                field_of_view={"horizontal": 60, "vertical": 45},  # degrees\r\n                image_data="",  # In practice, this might be a path or encoded data\r\n                confidence_threshold=self.object_detector.confidence_threshold\r\n            )\r\n\r\n            return perception_data\r\n\r\n        except Exception as e:\r\n            logger.error(f"Perception pipeline processing failed: {e}")\r\n            # Return empty perception data on failure\r\n            return PerceptionData(\r\n                id=f"perception_{int(time.time())}",\r\n                timestamp=time.time(),\r\n                objects=[],\r\n                camera_pose=robot_pose,\r\n                field_of_view={"horizontal": 60, "vertical": 45},\r\n                image_data="",\r\n                confidence_threshold=self.object_detector.confidence_threshold\r\n            )\r\n\r\n    def _estimate_3d_positions(self,\r\n                             detected_objects: List[DetectedObject],\r\n                             image: np.ndarray,\r\n                             robot_pose: Optional[Dict[str, Any]] = None) -> List[DetectedObject]:\r\n        """\r\n        Estimate 3D positions of detected objects using camera parameters.\r\n\r\n        Args:\r\n            detected_objects: List of 2D detected objects\r\n            image: Original image for size reference\r\n            robot_pose: Robot\'s current pose in the environment\r\n\r\n        Returns:\r\n            List of DetectedObject with updated 3D positions\r\n        """\r\n        # For simplicity, we\'ll use a basic depth estimation based on object size\r\n        # In a real system, you\'d use stereo vision, depth sensors, or other methods\r\n\r\n        processed_objects = []\r\n\r\n        for obj in detected_objects:\r\n            # Create a copy of the object to modify\r\n            updated_obj = obj\r\n\r\n            # Estimate depth based on object size in image\r\n            # Larger objects are likely closer, smaller objects are likely farther\r\n            image_height, image_width = image.shape[:2]\r\n            bbox_width = obj.bbox.x_max - obj.bbox.x_min\r\n            bbox_height = obj.bbox.y_max - obj.bbox.y_min\r\n\r\n            # Normalize by image dimensions\r\n            norm_width = bbox_width / image_width\r\n            norm_height = bbox_height / image_height\r\n\r\n            # Simple depth estimation (in meters) - this is a simplification\r\n            # In reality, you\'d use proper geometric calculations with camera parameters\r\n            depth = max(0.1, 2.0 - (norm_width + norm_height) * 2.0)  # 0.1m to 2.0m range\r\n\r\n            # Calculate 3D position based on 2D position and estimated depth\r\n            center_x = (obj.bbox.x_min + obj.bbox.x_max) / 2.0\r\n            center_y = (obj.bbox.y_min + obj.bbox.y_max) / 2.0\r\n\r\n            # Convert 2D image coordinates to 3D world coordinates\r\n            # This is a simplified transformation\r\n            x_3d = (center_x - image_width / 2.0) * depth / (image_width / 2.0)  # Scale by depth\r\n            y_3d = (center_y - image_height / 2.0) * depth / (image_height / 2.0)  # Scale by depth\r\n\r\n            # Apply robot pose transformation if available\r\n            if robot_pose:\r\n                # This is a simplified transformation\r\n                # In practice, you\'d use proper coordinate frame transformations\r\n                robot_x = robot_pose.get("x", 0.0)\r\n                robot_y = robot_pose.get("y", 0.0)\r\n                robot_theta = robot_pose.get("theta", 0.0)  # rotation in radians\r\n\r\n                # Rotate and translate the object position relative to robot\r\n                cos_theta = np.cos(robot_theta)\r\n                sin_theta = np.sin(robot_theta)\r\n\r\n                world_x = robot_x + x_3d * cos_theta - y_3d * sin_theta\r\n                world_y = robot_y + x_3d * sin_theta + y_3d * cos_theta\r\n            else:\r\n                world_x, world_y = x_3d, y_3d\r\n\r\n            # Update the object with 3D position\r\n            updated_obj.position_3d = Point3D(x=world_x, y=world_y, z=depth)\r\n\r\n            processed_objects.append(updated_obj)\r\n\r\n        return processed_objects\r\n\r\n    def get_spatial_relationships(self,\r\n                                objects: List[DetectedObject],\r\n                                reference_object_id: str) -> Dict[str, List[DetectedObject]]:\r\n        """\r\n        Compute spatial relationships between objects.\r\n\r\n        Args:\r\n            objects: List of detected objects\r\n            reference_object_id: ID of reference object for spatial relationships\r\n\r\n        Returns:\r\n            Dictionary mapping spatial relationships to lists of objects\r\n        """\r\n        relationships = {\r\n            "left_of": [],\r\n            "right_of": [],\r\n            "in_front_of": [],\r\n            "behind": [],\r\n            "above": [],\r\n            "below": [],\r\n            "near": [],\r\n            "far_from": []\r\n        }\r\n\r\n        # Find reference object\r\n        ref_obj = None\r\n        for obj in objects:\r\n            if obj.id == reference_object_id:\r\n                ref_obj = obj\r\n                break\r\n\r\n        if not ref_obj:\r\n            logger.warning(f"Reference object {reference_object_id} not found")\r\n            return relationships\r\n\r\n        # Compute relationships with all other objects\r\n        for obj in objects:\r\n            if obj.id == reference_object_id:\r\n                continue\r\n\r\n            # Calculate relative position\r\n            dx = obj.position_3d.x - ref_obj.position_3d.x\r\n            dy = obj.position_3d.y - ref_obj.position_3d.y\r\n            dz = obj.position_3d.z - ref_obj.position_3d.z\r\n\r\n            # Calculate distance\r\n            distance = np.sqrt(dx**2 + dy**2 + dz**2)\r\n\r\n            # Determine spatial relationships\r\n            if dx < -0.1:  # More than 10cm to the left\r\n                relationships["left_of"].append(obj)\r\n            elif dx > 0.1:  # More than 10cm to the right\r\n                relationships["right_of"].append(obj)\r\n\r\n            if dy < -0.1:  # More than 10cm in front\r\n                relationships["in_front_of"].append(obj)\r\n            elif dy > 0.1:  # More than 10cm behind\r\n                relationships["behind"].append(obj)\r\n\r\n            if dz > 0.1:  # More than 10cm above\r\n                relationships["above"].append(obj)\r\n            elif dz < -0.1:  # More than 10cm below\r\n                relationships["below"].append(obj)\r\n\r\n            if distance < 0.5:  # Within 50cm\r\n                relationships["near"].append(obj)\r\n            else:  # Farther than 50cm\r\n                relationships["far_from"].append(obj)\r\n\r\n        return relationships\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-3-create-vision-processor",children:"Step 3: Create Vision Processor"}),"\n",(0,o.jsx)(n.p,{children:"Now let's create the vision processor that integrates with the cognitive planning system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# src/vla/vision/vision_processor.py\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom typing import List, Dict, Any, Optional\r\nfrom src.vla.models.detected_object import DetectedObject\r\nfrom src.vla.models.perception_data import PerceptionData\r\nfrom src.vla.vision.perception_pipeline import PerceptionPipeline\r\nfrom src.vla.vision.object_detector import ObjectDetector\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass VisionProcessor:\r\n    """\r\n    High-level vision processing that integrates with the VLA system.\r\n    """\r\n\r\n    def __init__(self, perception_pipeline: PerceptionPipeline):\r\n        """\r\n        Initialize vision processor.\r\n\r\n        Args:\r\n            perception_pipeline: Perception pipeline for object detection\r\n        """\r\n        self.perception_pipeline = perception_pipeline\r\n        self.last_perception_data = None\r\n\r\n    def process_camera_input(self,\r\n                           image: np.ndarray,\r\n                           robot_pose: Optional[Dict[str, Any]] = None) -> PerceptionData:\r\n        """\r\n        Process camera input to detect objects and provide perception data.\r\n\r\n        Args:\r\n            image: Camera image input\r\n            robot_pose: Current robot pose information\r\n\r\n        Returns:\r\n            PerceptionData with detected objects and relationships\r\n        """\r\n        # Process the frame through the perception pipeline\r\n        perception_data = self.perception_pipeline.process_frame(image, robot_pose)\r\n\r\n        # Store for later use\r\n        self.last_perception_data = perception_data\r\n\r\n        return perception_data\r\n\r\n    def find_object_by_description(self,\r\n                                 description: str,\r\n                                 perception_data: PerceptionData) -> Optional[DetectedObject]:\r\n        """\r\n        Find an object that matches a description in the current perception data.\r\n\r\n        Args:\r\n            description: Natural language description of the object\r\n            perception_data: Current perception data to search\r\n\r\n        Returns:\r\n            DetectedObject that matches the description, or None if not found\r\n        """\r\n        description_lower = description.lower()\r\n\r\n        # Look for matches based on various attributes\r\n        for obj in perception_data.objects:\r\n            obj_desc = f"{obj.color} {obj.class_name}".lower()\r\n\r\n            # Direct match\r\n            if description_lower in obj_desc:\r\n                return obj\r\n\r\n            # Color match\r\n            if obj.color.lower() in description_lower:\r\n                # If looking for a specific color, return first object with that color\r\n                return obj\r\n\r\n            # Class name match\r\n            if obj.class_name.lower() in description_lower:\r\n                return obj\r\n\r\n        # If no direct matches, try more sophisticated matching\r\n        for obj in perception_data.objects:\r\n            # Check if this might be what the user meant\r\n            if self._is_possible_match(description_lower, obj):\r\n                return obj\r\n\r\n        return None\r\n\r\n    def _is_possible_match(self, description: str, obj: DetectedObject) -> bool:\r\n        """\r\n        Determine if an object is a possible match for a description.\r\n\r\n        Args:\r\n            description: Natural language description\r\n            obj: Detected object to check\r\n\r\n        Returns:\r\n            True if object might match the description\r\n        """\r\n        # Simple heuristics for possible matches\r\n        if "red" in description and obj.color == "red":\r\n            return True\r\n        if "blue" in description and obj.color == "blue":\r\n            return True\r\n        if "green" in description and obj.color == "green":\r\n            return True\r\n        if "small" in description and self._is_small_object(obj):\r\n            return True\r\n        if "large" in description and self._is_large_object(obj):\r\n            return True\r\n\r\n        # Class-based matching\r\n        if "box" in description and "box" in obj.class_name:\r\n            return True\r\n        if "cube" in description and "cube" in obj.class_name:\r\n            return True\r\n        if "cylinder" in description and "bottle" in obj.class_name:\r\n            return True  # Bottles are roughly cylindrical\r\n\r\n        return False\r\n\r\n    def _is_small_object(self, obj: DetectedObject) -> bool:\r\n        """\r\n        Determine if an object is small based on its bounding box size.\r\n\r\n        Args:\r\n            obj: Detected object\r\n\r\n        Returns:\r\n            True if object is small\r\n        """\r\n        if obj.bbox:\r\n            area = (obj.bbox.x_max - obj.bbox.x_min) * (obj.bbox.y_max - obj.bbox.y_min)\r\n            # Consider objects with area < 1000 pixels as small (adjust as needed)\r\n            return area < 1000\r\n        return False\r\n\r\n    def _is_large_object(self, obj: DetectedObject) -> bool:\r\n        """\r\n        Determine if an object is large based on its bounding box size.\r\n\r\n        Args:\r\n            obj: Detected object\r\n\r\n        Returns:\r\n            True if object is large\r\n        """\r\n        if obj.bbox:\r\n            area = (obj.bbox.x_max - obj.bbox.x_min) * (obj.bbox.y_max - obj.bbox.y_min)\r\n            # Consider objects with area > 5000 pixels as large (adjust as needed)\r\n            return area > 5000\r\n        return False\r\n\r\n    def get_object_for_command(self,\r\n                             command: str,\r\n                             perception_data: PerceptionData) -> Optional[DetectedObject]:\r\n        """\r\n        Identify the object referenced in a command based on current perception.\r\n\r\n        Args:\r\n            command: Natural language command\r\n            perception_data: Current perception data\r\n\r\n        Returns:\r\n            DetectedObject referenced in the command, or None\r\n        """\r\n        # Extract object reference from command\r\n        # This is a simplified approach - in practice, you\'d use more sophisticated NLP\r\n        command_lower = command.lower()\r\n\r\n        # Look for color + class patterns (e.g., "red cube", "blue bottle")\r\n        for obj in perception_data.objects:\r\n            if obj.color and obj.class_name:\r\n                color_class_pattern = f"{obj.color} {obj.class_name}".lower()\r\n                if color_class_pattern in command_lower:\r\n                    return obj\r\n\r\n        # Look for class names only\r\n        for obj in perception_data.objects:\r\n            if obj.class_name.lower() in command_lower:\r\n                return obj\r\n\r\n        # Look for color adjectives\r\n        color_keywords = ["red", "blue", "green", "yellow", "orange", "purple", "pink", "brown", "black", "white"]\r\n        for color in color_keywords:\r\n            if color in command_lower:\r\n                # Return first object with matching color\r\n                for obj in perception_data.objects:\r\n                    if obj.color.lower() == color:\r\n                        return obj\r\n\r\n        # If no specific object found, return the closest one\r\n        if perception_data.objects:\r\n            # Return object with highest confidence or closest to center\r\n            return max(perception_data.objects, key=lambda o: o.confidence)\r\n\r\n        return None\r\n\r\n    def visualize_detections(self,\r\n                           image: np.ndarray,\r\n                           perception_data: PerceptionData) -> np.ndarray:\r\n        """\r\n        Draw bounding boxes and labels on the image for visualization.\r\n\r\n        Args:\r\n            image: Original image\r\n            perception_data: Perception data with detected objects\r\n\r\n        Returns:\r\n            Image with visualized detections\r\n        """\r\n        # Create a copy of the image to draw on\r\n        vis_image = image.copy()\r\n\r\n        for obj in perception_data.objects:\r\n            if obj.bbox:\r\n                # Draw bounding box\r\n                cv2.rectangle(\r\n                    vis_image,\r\n                    (obj.bbox.x_min, obj.bbox.y_min),\r\n                    (obj.bbox.x_max, obj.bbox.y_max),\r\n                    (0, 255, 0),  # Green color\r\n                    2\r\n                )\r\n\r\n                # Draw label\r\n                label = f"{obj.class_name} ({obj.color}) {obj.confidence:.2f}"\r\n                cv2.putText(\r\n                    vis_image,\r\n                    label,\r\n                    (obj.bbox.x_min, obj.bbox.y_min - 10),\r\n                    cv2.FONT_HERSHEY_SIMPLEX,\r\n                    0.5,\r\n                    (0, 255, 0),\r\n                    1\r\n                )\r\n\r\n        return vis_image\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-4-integrate-with-vla-system",children:"Step 4: Integrate with VLA System"}),"\n",(0,o.jsx)(n.p,{children:"Now let's create a module that integrates the vision system with the cognitive planning:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# src/vla/integration/vla_vision_integrator.py\r\n\r\nfrom typing import Dict, Any, Optional\r\nfrom src.vla.vision.vision_processor import VisionProcessor\r\nfrom src.vla.models.detected_object import DetectedObject\r\nfrom src.vla.models.perception_data import PerceptionData\r\nfrom src.vla.llm.cognitive_planner import ProcessedIntent\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass VLAVisionIntegrator:\r\n    """\r\n    Integrates visual perception with the broader VLA system.\r\n    """\r\n\r\n    def __init__(self, vision_processor: VisionProcessor):\r\n        """\r\n        Initialize the VLA vision integrator.\r\n\r\n        Args:\r\n            vision_processor: Vision processor instance\r\n        """\r\n        self.vision_processor = vision_processor\r\n\r\n    def process_camera_frame(self,\r\n                           image: np.ndarray,\r\n                           robot_pose: Optional[Dict[str, Any]] = None) -> PerceptionData:\r\n        """\r\n        Process a camera frame through the vision system.\r\n\r\n        Args:\r\n            image: Camera image to process\r\n            robot_pose: Current robot pose information\r\n\r\n        Returns:\r\n            PerceptionData with detected objects and metadata\r\n        """\r\n        logger.info("Processing camera frame through vision system")\r\n\r\n        try:\r\n            # Process the image through the vision pipeline\r\n            perception_data = self.vision_processor.process_camera_input(image, robot_pose)\r\n\r\n            logger.info(f"Detected {len(perception_data.objects)} objects in frame")\r\n            return perception_data\r\n\r\n        except Exception as e:\r\n            logger.error(f"Camera frame processing failed: {e}")\r\n            raise\r\n\r\n    def enhance_intent_with_vision(self,\r\n                                  intent: ProcessedIntent,\r\n                                  perception_data: PerceptionData) -> ProcessedIntent:\r\n        """\r\n        Enhance a processed intent with visual context from perception data.\r\n\r\n        Args:\r\n            intent: Original processed intent\r\n            perception_data: Current perception data\r\n\r\n        Returns:\r\n            Enhanced ProcessedIntent with visual context\r\n        """\r\n        logger.info(f"Enhancing intent with visual context for {len(perception_data.objects)} objects")\r\n\r\n        # Update context objects with perception data\r\n        intent.context_objects = perception_data.objects\r\n\r\n        # Update spatial constraints based on detected objects\r\n        if intent.spatial_constraints:\r\n            # If the command mentioned spatial relationships, find corresponding objects\r\n            for obj in perception_data.objects:\r\n                # Example: if command said "left of red cube", find the red cube\r\n                if intent.spatial_constraints.get("relative_position") == "left_of":\r\n                    if obj.color == "red" and "cube" in obj.class_name.lower():\r\n                        # Associate the object with the spatial constraint\r\n                        intent.spatial_constraints["reference_object"] = obj\r\n\r\n        return intent\r\n\r\n    def resolve_command_ambiguity(self,\r\n                                 command: str,\r\n                                 perception_data: PerceptionData) -> Optional[DetectedObject]:\r\n        """\r\n        Resolve ambiguity in a command by using visual context.\r\n\r\n        Args:\r\n            command: Natural language command that may be ambiguous\r\n            perception_data: Current perception data\r\n\r\n        Returns:\r\n            Resolved object reference, or None if still ambiguous\r\n        """\r\n        # Look for ambiguous references in the command\r\n        ambiguous_indicators = ["that", "it", "thing", "there", "this"]\r\n        command_lower = command.lower()\r\n\r\n        if any(indicator in command_lower for indicator in ambiguous_indicators):\r\n            # Try to disambiguate based on spatial context\r\n            # For example, if command says "pick up that" and there\'s a red cube nearby\r\n            for obj in perception_data.objects:\r\n                if obj.confidence > 0.7:  # Only consider high-confidence detections\r\n                    # Could implement more sophisticated disambiguation logic here\r\n                    # based on spatial relationships, salience, etc.\r\n                    return obj\r\n\r\n        # If no ambiguity, return None\r\n        return None\r\n\r\n    def validate_action_feasibility(self,\r\n                                   action_parameters: Dict[str, Any],\r\n                                   perception_data: PerceptionData) -> Dict[str, Any]:\r\n        """\r\n        Validate if an action is feasible given current perception data.\r\n\r\n        Args:\r\n            action_parameters: Parameters for the action to validate\r\n            perception_data: Current perception data\r\n\r\n        Returns:\r\n            Dictionary with validation results\r\n        """\r\n        validation_result = {\r\n            "is_feasible": True,\r\n            "issues": [],\r\n            "suggestions": [],\r\n            "confidence": 0.0\r\n        }\r\n\r\n        # Check if target object exists in environment\r\n        target_object_id = action_parameters.get("object_id")\r\n        if target_object_id:\r\n            target_exists = any(obj.id == target_object_id for obj in perception_data.objects)\r\n            if not target_exists:\r\n                validation_result["is_feasible"] = False\r\n                validation_result["issues"].append(f"Target object {target_object_id} not found in environment")\r\n\r\n                # Try to find similar objects\r\n                target_class = action_parameters.get("object_class", "")\r\n                similar_objects = [\r\n                    obj for obj in perception_data.objects\r\n                    if target_class.lower() in obj.class_name.lower()\r\n                ]\r\n\r\n                if similar_objects:\r\n                    validation_result["suggestions"].append(\r\n                        f"Similar objects found: {[obj.id for obj in similar_objects]}"\r\n                    )\r\n\r\n        # Check navigation feasibility\r\n        target_position = action_parameters.get("target_position")\r\n        if target_position:\r\n            # In a real system, you\'d check navigation maps and obstacles\r\n            # For now, just ensure position is reasonable\r\n            if (abs(target_position.get("x", 0)) > 10 or\r\n                abs(target_position.get("y", 0)) > 10 or\r\n                abs(target_position.get("z", 0)) > 10):\r\n                validation_result["issues"].append("Target position seems too far away")\r\n\r\n        # Calculate overall confidence based on object detection confidence\r\n        if perception_data.objects:\r\n            avg_confidence = sum(obj.confidence for obj in perception_data.objects) / len(perception_data.objects)\r\n            validation_result["confidence"] = avg_confidence\r\n\r\n        return validation_result\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,o.jsx)(n.h3,{id:"exercise-31-visual-perception-integration",children:"Exercise 3.1: Visual Perception Integration"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Setup"}),": Create a test script that integrates the vision components with the cognitive planner."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Implementation"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# test_vision_integration.py\r\nimport cv2\r\nimport numpy as np\r\nimport time\r\nfrom src.vla.vision.object_detector import ObjectDetector\r\nfrom src.vla.vision.perception_pipeline import PerceptionPipeline\r\nfrom src.vla.vision.vision_processor import VisionProcessor\r\nfrom src.vla.integration.vla_vision_integrator import VLAVisionIntegrator\r\n\r\ndef main():\r\n    # Initialize vision components\r\n    detector = ObjectDetector(confidence_threshold=0.5)\r\n    pipeline = PerceptionPipeline(detector)\r\n    vision_processor = VisionProcessor(pipeline)\r\n    vision_integrator = VLAVisionIntegrator(vision_processor)\r\n\r\n    print("Vision processing system initialized")\r\n\r\n    # Create a mock image for testing (in reality, this would come from a camera)\r\n    # Create a simple test image with some shapes\r\n    test_image = np.zeros((480, 640, 3), dtype=np.uint8)\r\n\r\n    # Draw some shapes to simulate objects\r\n    cv2.rectangle(test_image, (100, 100), (150, 150), (0, 0, 255), -1)  # Red square\r\n    cv2.circle(test_image, (200, 200), 25, (255, 0, 0), -1)  # Blue circle\r\n    cv2.rectangle(test_image, (300, 100), (350, 180), (0, 255, 0), -1)  # Green rectangle\r\n\r\n    # Process the test image\r\n    robot_pose = {"x": 0.0, "y": 0.0, "theta": 0.0}\r\n    perception_data = vision_processor.process_camera_input(test_image, robot_pose)\r\n\r\n    print(f"Detected {len(perception_data.objects)} objects:")\r\n    for i, obj in enumerate(perception_data.objects):\r\n        print(f"  {i+1}. {obj.class_name} ({obj.color}) at {obj.position_3d.x:.2f}, {obj.position_3d.y:.2f}, {obj.position_3d.z:.2f}")\r\n\r\n    # Test object finding\r\n    target_obj = vision_processor.find_object_by_description("red object", perception_data)\r\n    if target_obj:\r\n        print(f"Found target object: {target_obj.class_name} ({target_obj.color})")\r\n\r\n    # Test command-based object selection\r\n    command = "Pick up the red cube"\r\n    command_obj = vision_processor.get_object_for_command(command, perception_data)\r\n    if command_obj:\r\n        print(f"Object for command \'{command}\': {command_obj.class_name} ({command_obj.color})")\r\n\r\n    # Visualize the results\r\n    vis_image = vision_processor.visualize_detections(test_image, perception_data)\r\n\r\n    # Save the visualization\r\n    cv2.imwrite("vision_test_output.png", vis_image)\r\n    print("Visualization saved as vision_test_output.png")\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Testing"}),": Run the test script to see how the vision system detects objects, estimates their 3D positions, and relates them to natural language commands."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,o.jsx)(n.h3,{id:"3d-position-estimation",children:"3D Position Estimation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Monocular Depth Estimation"}),": Using object size and context to estimate depth"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stereo Vision"}),": Using multiple cameras to calculate depth"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Depth Sensors"}),": Using LiDAR or RGB-D cameras for direct depth measurement"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Coordinate Transformations"}),": Converting between camera frame and world frame"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"object-recognition-and-context",children:"Object Recognition and Context"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Class-based Recognition"}),": Identifying objects by their category"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attribute-based Recognition"}),": Identifying objects by color, size, shape"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contextual Recognition"}),": Using spatial relationships and scene context"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"real-time-processing",children:"Real-time Processing"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Frame Rate Optimization"}),": Processing images at sufficient frame rates for real-time interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Detection Confidence"}),": Balancing accuracy with processing speed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Management"}),": Efficient use of computational resources"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"common-challenges-and-solutions",children:"Common Challenges and Solutions"}),"\n",(0,o.jsx)(n.h3,{id:"challenge-1-depth-estimation",children:"Challenge 1: Depth Estimation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Estimating 3D positions from 2D images is inherently ambiguous.\r\n",(0,o.jsx)(n.strong,{children:"Solution"}),": Use multiple sensors (stereo, depth cameras) or learn depth from context."]}),"\n",(0,o.jsx)(n.h3,{id:"challenge-2-object-occlusion",children:"Challenge 2: Object Occlusion"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Objects may be partially hidden or occluded.\r\n",(0,o.jsx)(n.strong,{children:"Solution"}),": Implement temporal consistency tracking and prediction."]}),"\n",(0,o.jsx)(n.h3,{id:"challenge-3-lighting-conditions",children:"Challenge 3: Lighting Conditions"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Performance varies with lighting conditions.\r\n",(0,o.jsx)(n.strong,{children:"Solution"}),": Use illumination-invariant features and preprocessing."]}),"\n",(0,o.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"How does the system estimate 3D positions from 2D image coordinates?"}),"\n",(0,o.jsx)(n.li,{children:'What factors influence the "graspable" property of detected objects?'}),"\n",(0,o.jsx)(n.li,{children:"How does the vision processor identify objects referenced in natural language commands?"}),"\n",(0,o.jsx)(n.li,{children:"What are the key components of the perception pipeline?"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this lesson, you have implemented the visual perception component of the VLA system. You learned how to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement object detection using computer vision libraries"}),"\n",(0,o.jsx)(n.li,{children:"Integrate visual perception with the cognitive planning system"}),"\n",(0,o.jsx)(n.li,{children:"Perform 3D position estimation for object manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Handle spatial relationships between detected objects"}),"\n",(0,o.jsx)(n.li,{children:"Create a perception pipeline for real-time processing"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The visual perception system provides critical environmental context that enables the VLA system to understand its surroundings and execute commands that reference specific objects or locations. In the next lesson, you will implement the ROS 2 action execution system that will carry out the planned actions in simulation."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const o={},i=t.createContext(o);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);