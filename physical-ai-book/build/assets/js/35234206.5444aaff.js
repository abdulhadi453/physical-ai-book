"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[5606],{1624:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"chapter-3/perception-pipeline","title":"Isaac ROS Perception Pipeline Implementation","description":"Overview","source":"@site/docs/chapter-3/perception-pipeline.md","sourceDirName":"chapter-3","slug":"/chapter-3/perception-pipeline","permalink":"/docs/chapter-3/perception-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-3/perception-pipeline.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Simulation Environment Configuration","permalink":"/docs/chapter-3/simulation-setup"},"next":{"title":"Multimodal Sensor Integration in Isaac Sim","permalink":"/docs/chapter-3/sensor-integration"}}');var t=r(4848),s=r(8453);const o={},a="Isaac ROS Perception Pipeline Implementation",c={},p=[{value:"Overview",id:"overview",level:2},{value:"Introduction to Perception Pipelines",id:"introduction-to-perception-pipelines",level:2},{value:"What is a Perception Pipeline?",id:"what-is-a-perception-pipeline",level:3},{value:"Components of a Perception Pipeline",id:"components-of-a-perception-pipeline",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Perception Pipeline Architecture",id:"perception-pipeline-architecture",level:2},{value:"Data Flow in Perception Systems",id:"data-flow-in-perception-systems",level:3},{value:"Isaac ROS Perception Pipeline Components",id:"isaac-ros-perception-pipeline-components",level:3},{value:"Implementing Basic Perception Pipeline",id:"implementing-basic-perception-pipeline",level:2},{value:"1. Create Perception Pipeline Package",id:"1-create-perception-pipeline-package",level:3},{value:"2. Create Perception Pipeline Node",id:"2-create-perception-pipeline-node",level:3},{value:"3. Create Setup File for the Package",id:"3-create-setup-file-for-the-package",level:3},{value:"4. Create Package XML",id:"4-create-package-xml",level:3},{value:"Advanced Perception Pipeline Components",id:"advanced-perception-pipeline-components",level:2},{value:"1. Semantic Segmentation Pipeline",id:"1-semantic-segmentation-pipeline",level:3},{value:"2. Depth Estimation Pipeline",id:"2-depth-estimation-pipeline",level:3},{value:"Perception Pipeline Launch Files",id:"perception-pipeline-launch-files",level:2},{value:"1. Create Launch Directory",id:"1-create-launch-directory",level:3},{value:"2. Create Basic Perception Pipeline Launch File",id:"2-create-basic-perception-pipeline-launch-file",level:3},{value:"Isaac ROS Perception Pipeline Optimization",id:"isaac-ros-perception-pipeline-optimization",level:2},{value:"1. GPU Optimization Configuration",id:"1-gpu-optimization-configuration",level:3},{value:"Building and Testing the Perception Pipeline",id:"building-and-testing-the-perception-pipeline",level:2},{value:"1. Build the Perception Pipeline Package",id:"1-build-the-perception-pipeline-package",level:3},{value:"2. Create Test Script for Perception Pipeline",id:"2-create-test-script-for-perception-pipeline",level:3},{value:"3. Run Perception Pipeline Test",id:"3-run-perception-pipeline-test",level:3},{value:"Performance Monitoring and Evaluation",id:"performance-monitoring-and-evaluation",level:2},{value:"1. Create Performance Monitoring Script",id:"1-create-performance-monitoring-script",level:3},{value:"2. Add Performance Monitor to Setup Files",id:"2-add-performance-monitor-to-setup-files",level:3},{value:"Troubleshooting Perception Pipeline",id:"troubleshooting-perception-pipeline",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Issue: &quot;CUDA out of memory error&quot;",id:"issue-cuda-out-of-memory-error",level:4},{value:"Issue: &quot;Perception pipeline not receiving images&quot;",id:"issue-perception-pipeline-not-receiving-images",level:4},{value:"Issue: &quot;High latency in perception&quot;",id:"issue-high-latency-in-perception",level:4},{value:"Verification Checklist",id:"verification-checklist",level:2},{value:"Next Steps",id:"next-steps",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"isaac-ros-perception-pipeline-implementation",children:"Isaac ROS Perception Pipeline Implementation"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This document provides detailed instructions for implementing a basic perception pipeline using Isaac ROS. The perception pipeline is crucial for enabling robots to understand and interact with their environment through AI-powered perception capabilities."}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-perception-pipelines",children:"Introduction to Perception Pipelines"}),"\n",(0,t.jsx)(n.h3,{id:"what-is-a-perception-pipeline",children:"What is a Perception Pipeline?"}),"\n",(0,t.jsx)(n.p,{children:"A perception pipeline in robotics processes sensor data to extract meaningful information about the environment. In the context of Isaac ROS, perception pipelines leverage GPU acceleration to achieve real-time performance for complex AI tasks."}),"\n",(0,t.jsx)(n.h3,{id:"components-of-a-perception-pipeline",children:"Components of a Perception Pipeline"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Interface"}),": Receives raw sensor data from cameras, LiDAR, IMU, etc."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Preprocessing"}),": Normalizes and formats data for AI models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AI Inference"}),": Runs deep learning models on the data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Postprocessing"}),": Converts model outputs to actionable information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Decision Interface"}),": Provides information to decision-making systems"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before implementing the perception pipeline, ensure you have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed Isaac ROS bridge setup"}),"\n",(0,t.jsx)(n.li,{children:"Isaac Sim running with appropriate sensors"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 Humble environment sourced"}),"\n",(0,t.jsx)(n.li,{children:"Isaac ROS workspace built successfully"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"perception-pipeline-architecture",children:"Perception Pipeline Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"data-flow-in-perception-systems",children:"Data Flow in Perception Systems"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[Sensors] -> [Preprocessing] -> [AI Model] -> [Postprocessing] -> [Decision Making]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-perception-pipeline-components",children:"Isaac ROS Perception Pipeline Components"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Image Pipeline"}),": For camera image processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Stereo DNN"}),": For depth estimation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Detection NITROS"}),": For object detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS AprilTag"}),": For fiducial marker detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Bi3D"}),": For 3D segmentation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementing-basic-perception-pipeline",children:"Implementing Basic Perception Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"1-create-perception-pipeline-package",children:"1. Create Perception Pipeline Package"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ros_ws/src\r\nsource /opt/ros/humble/setup.bash\r\n\r\n# Create a new package for perception pipeline\r\nros2 pkg create --build-type ament_python isaac_ros_perception_pipeline --dependencies rclpy sensor_msgs vision_msgs cv_bridge geometry_msgs\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-create-perception-pipeline-node",children:"2. Create Perception Pipeline Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'cat > ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/isaac_ros_perception_pipeline/perception_pipeline.py << \'EOF\'\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom geometry_msgs.msg import Twist\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nfrom std_msgs.msg import String\r\n\r\n\r\nclass PerceptionPipeline(Node):\r\n    def __init__(self):\r\n        super().__init__(\'perception_pipeline\')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to camera images\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/color/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Subscribe to camera info\r\n        self.camera_info_subscription = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/color/camera_info\',\r\n            self.camera_info_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for detections\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray,\r\n            \'/object_detections\',\r\n            10\r\n        )\r\n\r\n        # Publisher for processed images\r\n        self.processed_image_publisher = self.create_publisher(\r\n            Image,\r\n            \'/processed_image\',\r\n            10\r\n        )\r\n\r\n        # Publisher for perception status\r\n        self.status_publisher = self.create_publisher(\r\n            String,\r\n            \'/perception_status\',\r\n            10\r\n        )\r\n\r\n        # Initialize YOLOv5 model\r\n        self.model = torch.hub.load(\'ultralytics/yolov5\', \'yolov5s\', pretrained=True)\r\n        self.model.eval()\r\n\r\n        # Transformation for input images\r\n        self.transform = transforms.Compose([\r\n            transforms.ToPILImage(),\r\n            transforms.Resize((640, 640)),\r\n            transforms.ToTensor()\r\n        ])\r\n\r\n        # Store camera info\r\n        self.camera_info = None\r\n        self.image_width = 640\r\n        self.image_height = 480\r\n\r\n        # Confidence threshold for detections\r\n        self.confidence_threshold = 0.5\r\n\r\n        self.get_logger().info(\'Perception Pipeline initialized\')\r\n\r\n    def camera_info_callback(self, msg):\r\n        """Callback for camera info"""\r\n        self.camera_info = msg\r\n        self.image_width = msg.width\r\n        self.image_height = msg.height\r\n\r\n    def image_callback(self, image_msg):\r\n        """Callback for camera images"""\r\n        try:\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, "bgr8")\r\n\r\n            # Run perception pipeline\r\n            detections, processed_image = self.run_perception_pipeline(cv_image)\r\n\r\n            # Publish detections\r\n            if detections is not None:\r\n                self.detection_publisher.publish(detections)\r\n\r\n            # Publish processed image\r\n            processed_img_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding="bgr8")\r\n            processed_img_msg.header = image_msg.header\r\n            self.processed_image_publisher.publish(processed_img_msg)\r\n\r\n            # Publish status\r\n            status_msg = String()\r\n            status_msg.data = f"Processed {len(detections.detections) if detections else 0} objects"\r\n            self.status_publisher.publish(status_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error in image callback: {e}\')\r\n\r\n    def run_perception_pipeline(self, cv_image):\r\n        """Run the complete perception pipeline"""\r\n        try:\r\n            # Preprocess image\r\n            input_tensor = self.preprocess_image(cv_image)\r\n\r\n            # Run AI inference\r\n            results = self.run_ai_inference(input_tensor)\r\n\r\n            # Postprocess results\r\n            detections = self.postprocess_results(results, cv_image.shape[0], cv_image.shape[1])\r\n\r\n            # Draw detections on image\r\n            processed_image = self.draw_detections(cv_image, detections)\r\n\r\n            return detections, processed_image\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error in perception pipeline: {e}\')\r\n            return None, cv_image\r\n\r\n    def preprocess_image(self, cv_image):\r\n        """Preprocess image for AI model"""\r\n        # Convert to RGB if needed\r\n        if cv_image.shape[2] == 3:\r\n            rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\r\n        else:\r\n            rgb_image = cv_image\r\n\r\n        # Convert to tensor\r\n        input_tensor = self.transform(rgb_image)\r\n        input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\r\n\r\n        return input_batch\r\n\r\n    def run_ai_inference(self, input_batch):\r\n        """Run AI model inference"""\r\n        with torch.no_grad():\r\n            results = self.model(input_batch)\r\n        return results\r\n\r\n    def postprocess_results(self, results, img_height, img_width):\r\n        """Postprocess AI model results"""\r\n        from vision_msgs.msg import Detection2D, ObjectHypothesisWithPose, ObjectHypothesis, BoundingBox2D\r\n\r\n        detections_msg = Detection2DArray()\r\n        detections_msg.header.stamp = self.get_clock().now().to_msg()\r\n        detections_msg.header.frame_id = "camera_frame"\r\n\r\n        # Extract bounding boxes and labels\r\n        for detection in results.xyxy[0]:  # x1, y1, x2, y2, confidence, class\r\n            x1, y1, x2, y2, conf, cls = detection\r\n\r\n            if conf > self.confidence_threshold:  # Confidence threshold\r\n                detection_2d = Detection2D()\r\n\r\n                # Set bounding box\r\n                bbox = BoundingBox2D()\r\n                bbox.size_x = float(x2 - x1)\r\n                bbox.size_y = float(y2 - y1)\r\n                bbox.center.x = float(x1 + (x2 - x1) / 2)\r\n                bbox.center.y = float(y1 + (y2 - y1) / 2)\r\n                detection_2d.bbox = bbox\r\n\r\n                # Set confidence and class\r\n                detection_2d.results.append(\r\n                    ObjectHypothesisWithPose(\r\n                        hypothesis=ObjectHypothesis(\r\n                            id=int(cls),\r\n                            score=float(conf)\r\n                        )\r\n                    )\r\n                )\r\n\r\n                detections_msg.detections.append(detection_2d)\r\n\r\n        return detections_msg\r\n\r\n    def draw_detections(self, image, detections):\r\n        """Draw detection results on image"""\r\n        if detections is None:\r\n            return image\r\n\r\n        output_image = image.copy()\r\n\r\n        for detection in detections.detections:\r\n            # Get bounding box coordinates\r\n            center_x = int(detection.bbox.center.x)\r\n            center_y = int(detection.bbox.center.y)\r\n            size_x = int(detection.bbox.size_x)\r\n            size_y = int(detection.bbox.size_y)\r\n\r\n            # Calculate top-left and bottom-right corners\r\n            x1 = center_x - size_x // 2\r\n            y1 = center_y - size_y // 2\r\n            x2 = center_x + size_x // 2\r\n            y2 = center_y + size_y // 2\r\n\r\n            # Draw bounding box\r\n            cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\r\n\r\n            # Add confidence text\r\n            if detection.results:\r\n                confidence = detection.results[0].hypothesis.score\r\n                cv2.putText(output_image, f\'{confidence:.2f}\',\r\n                           (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,\r\n                           0.5, (0, 255, 0), 1)\r\n\r\n        return output_image\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    perception_pipeline = PerceptionPipeline()\r\n\r\n    try:\r\n        rclpy.spin(perception_pipeline)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        perception_pipeline.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\nEOF\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-create-setup-file-for-the-package",children:"3. Create Setup File for the Package"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/setup.py << 'EOF'\r\nfrom setuptools import setup\r\n\r\npackage_name = 'isaac_ros_perception_pipeline'\r\n\r\nsetup(\r\n    name=package_name,\r\n    version='0.0.1',\r\n    packages=[package_name],\r\n    data_files=[\r\n        ('share/ament_index/resource_index/packages',\r\n            ['resource/' + package_name]),\r\n        ('share/' + package_name, ['package.xml']),\r\n    ],\r\n    install_requires=['setuptools'],\r\n    zip_safe=True,\r\n    maintainer='Your Name',\r\n    maintainer_email='your.email@example.com',\r\n    description='Basic perception pipeline using Isaac ROS',\r\n    license='Apache License 2.0',\r\n    tests_require=['pytest'],\r\n    entry_points={\r\n        'console_scripts': [\r\n            'perception_pipeline = isaac_ros_perception_pipeline.perception_pipeline:main',\r\n        ],\r\n    },\r\n)\r\nEOF\n"})}),"\n",(0,t.jsx)(n.h3,{id:"4-create-package-xml",children:"4. Create Package XML"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'cat > ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/package.xml << \'EOF\'\r\n<?xml version="1.0"?>\r\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\r\n<package format="3">\r\n  <name>isaac_ros_perception_pipeline</name>\r\n  <version>0.0.1</version>\r\n  <description>Basic perception pipeline using Isaac ROS</description>\r\n  <maintainer email="your.email@example.com">Your Name</maintainer>\r\n  <license>Apache License 2.0</license>\r\n\r\n  <depend>rclpy</depend>\r\n  <depend>sensor_msgs</depend>\r\n  <depend>vision_msgs</depend>\r\n  <depend>cv_bridge</depend>\r\n  <depend>geometry_msgs</depend>\r\n  <depend>std_msgs</depend>\r\n\r\n  <test_depend>ament_copyright</test_depend>\r\n  <test_depend>ament_flake8</test_depend>\r\n  <test_depend>ament_pep257</test_depend>\r\n  <test_depend>python3-pytest</test_depend>\r\n\r\n  <export>\r\n    <build_type>ament_python</build_type>\r\n  </export>\r\n</package>\r\nEOF\n'})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-perception-pipeline-components",children:"Advanced Perception Pipeline Components"}),"\n",(0,t.jsx)(n.h3,{id:"1-semantic-segmentation-pipeline",children:"1. Semantic Segmentation Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/isaac_ros_perception_pipeline/semantic_segmentation.py << 'EOF'\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nimport numpy as np\r\nimport cv2\r\n\r\n\r\nclass SemanticSegmentation(Node):\r\n    def __init__(self):\r\n        super().__init__('semantic_segmentation')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to camera images\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for segmentation masks\r\n        self.mask_publisher = self.create_publisher(\r\n            Image,\r\n            '/segmentation_mask',\r\n            10\r\n        )\r\n\r\n        # Publisher for segmented image overlay\r\n        self.overlay_publisher = self.create_publisher(\r\n            Image,\r\n            '/segmentation_overlay',\r\n            10\r\n        )\r\n\r\n        # Load segmentation model (using torchvision's FCN-ResNet101)\r\n        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)\r\n        self.model.eval()\r\n\r\n        # Define class names for visualization\r\n        self.class_names = [\r\n            'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\r\n            'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\r\n            'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\r\n            'train', 'tvmonitor'\r\n        ]\r\n\r\n        # Color palette for segmentation visualization\r\n        self.color_palette = np.random.randint(0, 255, size=(len(self.class_names), 3))\r\n\r\n        self.get_logger().info('Semantic Segmentation node initialized')\r\n\r\n    def image_callback(self, image_msg):\r\n        \"\"\"Callback for camera images\"\"\"\r\n        try:\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\r\n\r\n            # Run segmentation\r\n            segmentation, overlay = self.run_segmentation(cv_image)\r\n\r\n            # Publish segmentation mask\r\n            if segmentation is not None:\r\n                mask_msg = self.bridge.cv2_to_imgmsg(segmentation, encoding=\"mono8\")\r\n                mask_msg.header = image_msg.header\r\n                self.mask_publisher.publish(mask_msg)\r\n\r\n            # Publish overlay image\r\n            if overlay is not None:\r\n                overlay_msg = self.bridge.cv2_to_imgmsg(overlay, encoding=\"bgr8\")\r\n                overlay_msg.header = image_msg.header\r\n                self.overlay_publisher.publish(overlay_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in segmentation: {e}')\r\n\r\n    def run_segmentation(self, cv_image):\r\n        \"\"\"Run semantic segmentation on the image\"\"\"\r\n        try:\r\n            # Preprocess image\r\n            input_tensor = transforms.functional.to_tensor(cv_image)\r\n            input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\r\n\r\n            # Run segmentation\r\n            with torch.no_grad():\r\n                output = self.model(input_tensor)['out'][0]\r\n                segmentation = output.argmax(0).cpu().numpy()\r\n\r\n            # Create colored segmentation mask\r\n            colored_mask = self.color_palette[segmentation].astype(np.uint8)\r\n\r\n            # Create overlay image\r\n            overlay = cv2.addWeighted(cv_image, 0.7, colored_mask, 0.3, 0)\r\n\r\n            return segmentation, overlay\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in segmentation pipeline: {e}')\r\n            return None, cv_image\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    segmentation_node = SemanticSegmentation()\r\n\r\n    try:\r\n        rclpy.spin(segmentation_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        segmentation_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-depth-estimation-pipeline",children:"2. Depth Estimation Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/isaac_ros_perception_pipeline/depth_estimation.py << 'EOF'\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nimport numpy as np\r\nimport cv2\r\n\r\n\r\nclass DepthEstimation(Node):\r\n    def __init__(self):\r\n        super().__init__('depth_estimation')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to depth images\r\n        self.depth_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/depth/image_rect_raw',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for processed depth maps\r\n        self.depth_publisher = self.create_publisher(\r\n            Image,\r\n            '/estimated_depth',\r\n            10\r\n        )\r\n\r\n        # Publisher for point cloud (as image for visualization)\r\n        self.pointcloud_publisher = self.create_publisher(\r\n            Image,\r\n            '/pointcloud_visualization',\r\n            10\r\n        )\r\n\r\n        # Load depth estimation model (using MiDaS)\r\n        try:\r\n            self.model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\", pretrained=True)\r\n            self.model.eval()\r\n        except Exception as e:\r\n            self.get_logger().warn(f'Could not load MiDaS model: {e}')\r\n            self.model = None\r\n\r\n        # Transformation for depth estimation\r\n        self.transform = transforms.Compose([\r\n            transforms.Resize((384, 384)),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n        ])\r\n\r\n        self.get_logger().info('Depth Estimation node initialized')\r\n\r\n    def depth_callback(self, image_msg):\r\n        \"\"\"Callback for depth images\"\"\"\r\n        try:\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, \"passthrough\")\r\n\r\n            if self.model is not None:\r\n                # Run depth estimation\r\n                depth_map = self.run_depth_estimation(cv_image)\r\n\r\n                # Publish depth map\r\n                if depth_map is not None:\r\n                    depth_msg = self.bridge.cv2_to_imgmsg(depth_map, encoding=\"mono16\")\r\n                    depth_msg.header = image_msg.header\r\n                    self.depth_publisher.publish(depth_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in depth estimation: {e}')\r\n\r\n    def run_depth_estimation(self, cv_image):\r\n        \"\"\"Run depth estimation on the image\"\"\"\r\n        try:\r\n            # Convert to RGB if needed\r\n            if len(cv_image.shape) == 2:  # Grayscale\r\n                rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_GRAY2RGB)\r\n            else:\r\n                rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\r\n\r\n            # Preprocess image\r\n            input_tensor = self.transform(rgb_image)\r\n            input_batch = input_tensor.unsqueeze(0)\r\n\r\n            # Run depth estimation\r\n            with torch.no_grad():\r\n                prediction = self.model(input_batch)\r\n\r\n            # Normalize depth prediction\r\n            depth_map = prediction.squeeze().cpu().numpy()\r\n            depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\r\n            depth_map = (depth_map * 65535).astype(np.uint16)  # Convert to 16-bit\r\n\r\n            return depth_map\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in depth estimation pipeline: {e}')\r\n            return cv_image\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    depth_node = DepthEstimation()\r\n\r\n    try:\r\n        rclpy.spin(depth_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        depth_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(n.h2,{id:"perception-pipeline-launch-files",children:"Perception Pipeline Launch Files"}),"\n",(0,t.jsx)(n.h3,{id:"1-create-launch-directory",children:"1. Create Launch Directory"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/launch\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-create-basic-perception-pipeline-launch-file",children:"2. Create Basic Perception Pipeline Launch File"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/launch/basic_perception_pipeline.launch.py << 'EOF'\r\nimport launch\r\nfrom launch_ros.actions import Node\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom ament_index_python.packages import get_package_share_directory\r\nimport os\r\n\r\n\r\ndef generate_launch_description():\r\n    \"\"\"Launch file for basic perception pipeline.\"\"\"\r\n\r\n    # Declare launch arguments\r\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\r\n    namespace = LaunchConfiguration('namespace', default='')\r\n\r\n    # Perception pipeline node\r\n    perception_pipeline_node = Node(\r\n        package='isaac_ros_perception_pipeline',\r\n        executable='perception_pipeline',\r\n        name='perception_pipeline',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n        }],\r\n        remappings=[\r\n            ('/camera/color/image_raw', '/camera/color/image_raw'),\r\n            ('/object_detections', '/object_detections'),\r\n        ]\r\n    )\r\n\r\n    # Semantic segmentation node\r\n    semantic_segmentation_node = Node(\r\n        package='isaac_ros_perception_pipeline',\r\n        executable='semantic_segmentation',\r\n        name='semantic_segmentation',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n        }],\r\n        remappings=[\r\n            ('/camera/color/image_raw', '/camera/color/image_raw'),\r\n            ('/segmentation_mask', '/segmentation_mask'),\r\n        ]\r\n    )\r\n\r\n    # Depth estimation node\r\n    depth_estimation_node = Node(\r\n        package='isaac_ros_perception_pipeline',\r\n        executable='depth_estimation',\r\n        name='depth_estimation',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n        }],\r\n        remappings=[\r\n            ('/camera/depth/image_rect_raw', '/camera/depth/image_rect_raw'),\r\n            ('/estimated_depth', '/estimated_depth'),\r\n        ]\r\n    )\r\n\r\n    return launch.LaunchDescription([\r\n        DeclareLaunchArgument(\r\n            'use_sim_time',\r\n            default_value='true',\r\n            description='Use simulation clock if true'),\r\n        DeclareLaunchArgument(\r\n            'namespace',\r\n            default_value='',\r\n            description='Namespace for the nodes'),\r\n        perception_pipeline_node,\r\n        semantic_segmentation_node,\r\n        depth_estimation_node\r\n    ])\r\nEOF\n"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-perception-pipeline-optimization",children:"Isaac ROS Perception Pipeline Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"1-gpu-optimization-configuration",children:"1. GPU Optimization Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'cat > ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/config/perception_optimization.yaml << \'EOF\'\r\n# Isaac ROS Perception Pipeline Optimization Configuration\r\n\r\nperception_pipeline:\r\n  # GPU memory management\r\n  gpu_memory:\r\n    max_memory_allocation: 80%\r\n    memory_pool_size: 512MB\r\n    enable_memory_pool: true\r\n\r\n  # Model optimization\r\n  model_optimization:\r\n    enable_tensorrt: true\r\n    precision: "fp16"\r\n    max_batch_size: 1\r\n    dynamic_shapes: true\r\n\r\n  # Pipeline optimization\r\n  pipeline:\r\n    enable_async_processing: true\r\n    max_queue_size: 10\r\n    enable_batching: false\r\n    input_buffer_size: 2\r\n    output_buffer_size: 2\r\n\r\n  # Performance parameters\r\n  performance:\r\n    target_fps: 30\r\n    max_latency_ms: 100\r\n    enable_profiling: true\r\n    profile_output_file: "/tmp/perception_profile.json"\r\n\r\nsemantic_segmentation:\r\n  # GPU memory management\r\n  gpu_memory:\r\n    max_memory_allocation: 60%\r\n    memory_pool_size: 256MB\r\n    enable_memory_pool: true\r\n\r\n  # Model optimization\r\n  model_optimization:\r\n    enable_tensorrt: true\r\n    precision: "fp16"\r\n    max_batch_size: 1\r\n    dynamic_shapes: false\r\n\r\ndepth_estimation:\r\n  # GPU memory management\r\n  gpu_memory:\r\n    max_memory_allocation: 40%\r\n    memory_pool_size: 256MB\r\n    enable_memory_pool: true\r\n\r\n  # Model optimization\r\n  model_optimization:\r\n    enable_tensorrt: true\r\n    precision: "fp16"\r\n    max_batch_size: 1\r\n    dynamic_shapes: true\r\nEOF\n'})}),"\n",(0,t.jsx)(n.h2,{id:"building-and-testing-the-perception-pipeline",children:"Building and Testing the Perception Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"1-build-the-perception-pipeline-package",children:"1. Build the Perception Pipeline Package"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ros_ws\r\nsource /opt/ros/humble/setup.bash\r\ncolcon build --packages-select isaac_ros_perception_pipeline\r\nsource install/setup.bash\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-create-test-script-for-perception-pipeline",children:"2. Create Test Script for Perception Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'cat > ~/test_perception_pipeline.sh << \'EOF\'\r\n#!/bin/bash\r\n\r\n# Test script for Isaac ROS Perception Pipeline\r\n\r\necho "Testing Isaac ROS Perception Pipeline..."\r\n\r\n# Source ROS environment\r\nsource /opt/ros/humble/setup.bash\r\nsource ~/isaac_ros_ws/install/setup.bash\r\n\r\n# Check if perception pipeline package is available\r\necho "Checking perception pipeline package..."\r\nros2 pkg list | grep perception_pipeline\r\n\r\nif [ $? -eq 0 ]; then\r\n    echo "\u2713 Perception pipeline package found"\r\nelse\r\n    echo "\u2717 Perception pipeline package not found"\r\n    exit 1\r\nfi\r\n\r\n# Check if Isaac ROS perception packages are available\r\necho "Checking Isaac ROS perception packages..."\r\nros2 pkg list | grep isaac_ros\r\n\r\nif [ $? -eq 0 ]; then\r\n    echo "\u2713 Isaac ROS packages found"\r\nelse\r\n    echo "\u2717 Isaac ROS packages not found"\r\n    exit 1\r\nfi\r\n\r\n# Test perception pipeline launch\r\necho "Testing perception pipeline launch..."\r\ntimeout 15s ros2 launch isaac_ros_perception_pipeline basic_perception_pipeline.launch.py use_sim_time:=true &\r\n\r\n# Wait a moment for launch\r\nsleep 10\r\n\r\n# Check if perception nodes are running\r\necho "Checking running perception nodes..."\r\nros2 node list | grep -i perception\r\n\r\n# Check perception topics\r\necho "Checking perception topics..."\r\nros2 topic list | grep -i perception\r\n\r\n# Kill the test launch\r\npkill -f "basic_perception_pipeline.launch.py"\r\n\r\necho "Isaac ROS Perception Pipeline test completed."\r\nEOF\r\n\r\n# Make executable\r\nchmod +x ~/test_perception_pipeline.sh\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-run-perception-pipeline-test",children:"3. Run Perception Pipeline Test"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/test_perception_pipeline.sh\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-monitoring-and-evaluation",children:"Performance Monitoring and Evaluation"}),"\n",(0,t.jsx)(n.h3,{id:"1-create-performance-monitoring-script",children:"1. Create Performance Monitoring Script"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/perception_performance_monitor.py << 'EOF'\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom std_msgs.msg import Float32\r\nimport time\r\nfrom collections import deque\r\n\r\n\r\nclass PerceptionPerformanceMonitor(Node):\r\n    def __init__(self):\r\n        super().__init__('perception_performance_monitor')\r\n\r\n        # Subscribe to perception pipeline topics\r\n        self.detection_subscription = self.create_subscription(\r\n            Detection2DArray,\r\n            '/object_detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers for performance metrics\r\n        self.fps_publisher = self.create_publisher(Float32, '/perception_fps', 10)\r\n        self.latency_publisher = self.create_publisher(Float32, '/perception_latency', 10)\r\n        self.detection_rate_publisher = self.create_publisher(Float32, '/detection_rate', 10)\r\n\r\n        # Performance tracking\r\n        self.frame_times = deque(maxlen=30)  # Last 30 frames for FPS calculation\r\n        self.detection_times = deque(maxlen=30)  # Last 30 detections for latency\r\n        self.last_image_time = None\r\n        self.last_detection_time = None\r\n        self.detection_count = 0\r\n        self.image_count = 0\r\n\r\n        # Setup timer for periodic metrics publishing\r\n        self.timer = self.create_timer(1.0, self.publish_metrics)\r\n\r\n        self.get_logger().info('Perception Performance Monitor initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Track image reception for FPS calculation\"\"\"\r\n        current_time = self.get_clock().now().nanoseconds / 1e9\r\n        self.image_count += 1\r\n\r\n        if self.last_image_time is not None:\r\n            frame_time = current_time - self.last_image_time\r\n            self.frame_times.append(frame_time)\r\n\r\n        self.last_image_time = current_time\r\n\r\n    def detection_callback(self, msg):\r\n        \"\"\"Track detection for latency and rate calculation\"\"\"\r\n        detection_time = self.get_clock().now().nanoseconds / 1e9\r\n        self.detection_count += 1\r\n\r\n        # Calculate latency if we have corresponding image time\r\n        if self.last_image_time is not None:\r\n            latency = detection_time - self.last_image_time\r\n            self.detection_times.append(latency)\r\n\r\n        self.last_detection_time = detection_time\r\n\r\n    def publish_metrics(self):\r\n        \"\"\"Publish performance metrics\"\"\"\r\n        # Calculate FPS\r\n        if len(self.frame_times) > 0:\r\n            avg_frame_time = sum(self.frame_times) / len(self.frame_times)\r\n            fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0.0\r\n            fps_msg = Float32()\r\n            fps_msg.data = fps\r\n            self.fps_publisher.publish(fps_msg)\r\n\r\n        # Calculate average latency\r\n        if len(self.detection_times) > 0:\r\n            avg_latency = sum(self.detection_times) / len(self.detection_times)\r\n            latency_msg = Float32()\r\n            latency_msg.data = avg_latency\r\n            self.latency_publisher.publish(latency_msg)\r\n\r\n        # Calculate detection rate\r\n        detection_rate_msg = Float32()\r\n        detection_rate_msg.data = self.detection_count\r\n        self.detection_rate_publisher.publish(detection_rate_msg)\r\n\r\n        # Reset counters for next period\r\n        self.detection_count = 0\r\n\r\n    def get_performance_summary(self):\r\n        \"\"\"Get a summary of performance metrics\"\"\"\r\n        summary = {\r\n            'fps': 0.0,\r\n            'avg_latency': 0.0,\r\n            'detection_rate': 0.0\r\n        }\r\n\r\n        if len(self.frame_times) > 0:\r\n            avg_frame_time = sum(self.frame_times) / len(self.frame_times)\r\n            summary['fps'] = 1.0 / avg_frame_time if avg_frame_time > 0 else 0.0\r\n\r\n        if len(self.detection_times) > 0:\r\n            summary['avg_latency'] = sum(self.detection_times) / len(self.detection_times)\r\n\r\n        summary['detection_rate'] = self.detection_count\r\n\r\n        return summary\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    monitor = PerceptionPerformanceMonitor()\r\n\r\n    try:\r\n        rclpy.spin(monitor)\r\n    except KeyboardInterrupt:\r\n        summary = monitor.get_performance_summary()\r\n        print(f\"Performance Summary: {summary}\")\r\n    finally:\r\n        monitor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-add-performance-monitor-to-setup-files",children:"2. Add Performance Monitor to Setup Files"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Add to setup.py entry points\r\nsed -i '/entry_points=/a\\        \"perception_performance_monitor = isaac_ros_perception_pipeline.perception_performance_monitor:main,\"' ~/isaac_ros_ws/src/isaac_ros_perception_pipeline/setup.py\n"})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-perception-pipeline",children:"Troubleshooting Perception Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,t.jsx)(n.h4,{id:"issue-cuda-out-of-memory-error",children:'Issue: "CUDA out of memory error"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Reduce model batch size or input resolution"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check GPU memory usage\r\nnvidia-smi\r\n\r\n# Reduce input size in perception pipeline\r\n# Modify the transform in the perception pipeline to use smaller images\n"})}),"\n",(0,t.jsx)(n.h4,{id:"issue-perception-pipeline-not-receiving-images",children:'Issue: "Perception pipeline not receiving images"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Check topic remapping and Isaac Sim camera configuration"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check available topics\r\nros2 topic list | grep camera\r\n\r\n# Check if Isaac Sim is publishing images\r\nros2 topic echo /camera/color/image_raw --field data --field header\n"})}),"\n",(0,t.jsx)(n.h4,{id:"issue-high-latency-in-perception",children:'Issue: "High latency in perception"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Optimize model and pipeline settings"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check current performance\r\nros2 topic echo /perception_fps\r\n\r\n# Adjust optimization parameters in config file\n"})}),"\n",(0,t.jsx)(n.h2,{id:"verification-checklist",children:"Verification Checklist"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Perception pipeline package created and built successfully"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Basic object detection pipeline implemented"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Semantic segmentation pipeline implemented"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Depth estimation pipeline implemented"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Launch files created for pipeline"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Optimization configuration applied"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance monitoring implemented"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test script created and functional"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Troubleshooting guide reviewed"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"After implementing the basic perception pipeline:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test the pipeline"})," with Isaac Sim"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimize performance"})," based on monitoring results"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrate with navigation system"})," for Lesson 2"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create perception exercises"})," for student practice"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The Isaac ROS perception pipeline is now implemented and ready for Module 3, providing students with a foundation for AI-powered robot perception using NVIDIA Isaac platform."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);