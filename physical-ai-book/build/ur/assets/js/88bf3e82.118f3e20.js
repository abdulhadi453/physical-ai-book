"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[1705],{8453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>o});var t=i(6540);const s={},r=t.createContext(s);function c(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),t.createElement(r.Provider,{value:n},e.children)}},8575:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>p,frontMatter:()=>c,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"chapter-4/exercises","title":"VLA Practical Exercises","description":"Overview","source":"@site/docs/chapter-4/exercises.md","sourceDirName":"chapter-4","slug":"/chapter-4/exercises","permalink":"/ur/docs/chapter-4/exercises","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-4/exercises.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Integration Guide: Vision-Language-Action (VLA) Systems","permalink":"/ur/docs/chapter-4/integration-guide"},"next":{"title":"VLA Assessment Questions","permalink":"/ur/docs/chapter-4/assessments"}}');var s=i(4848),r=i(8453);const c={},o="VLA Practical Exercises",a={},l=[{value:"Overview",id:"overview",level:2},{value:"Exercise 1: Voice Command Processing",id:"exercise-1-voice-command-processing",level:2},{value:"Exercise 1.1: Basic Whisper Integration",id:"exercise-11-basic-whisper-integration",level:3},{value:"Exercise 1.2: Voice Command Validation",id:"exercise-12-voice-command-validation",level:3},{value:"Exercise 2: LLM Cognitive Planning",id:"exercise-2-llm-cognitive-planning",level:2},{value:"Exercise 2.1: Simple Command Planning",id:"exercise-21-simple-command-planning",level:3},{value:"Exercise 2.2: Complex Task Decomposition",id:"exercise-22-complex-task-decomposition",level:3},{value:"Exercise 3: Visual Perception Integration",id:"exercise-3-visual-perception-integration",level:2},{value:"Exercise 3.1: Object Detection Test",id:"exercise-31-object-detection-test",level:3},{value:"Exercise 3.2: 3D Position Estimation",id:"exercise-32-3d-position-estimation",level:3},{value:"Exercise 4: Action Execution",id:"exercise-4-action-execution",level:2},{value:"Exercise 4.1: Single Action Execution",id:"exercise-41-single-action-execution",level:3},{value:"Exercise 4.2: Action Sequence Execution",id:"exercise-42-action-sequence-execution",level:3},{value:"Exercise 5: Complete VLA Integration",id:"exercise-5-complete-vla-integration",level:2},{value:"Exercise 5.1: End-to-End Command Processing",id:"exercise-51-end-to-end-command-processing",level:3},{value:"Exercise 5.2: Capstone Task Simulation",id:"exercise-52-capstone-task-simulation",level:3},{value:"Advanced Exercises",id:"advanced-exercises",level:2},{value:"Exercise A1: Performance Optimization",id:"exercise-a1-performance-optimization",level:3},{value:"Exercise A2: Error Handling Enhancement",id:"exercise-a2-error-handling-enhancement",level:3},{value:"Exercise A3: Custom Object Training",id:"exercise-a3-custom-object-training",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Technical Implementation (40%)",id:"technical-implementation-40",level:3},{value:"Functionality (35%)",id:"functionality-35",level:3},{value:"Performance (15%)",id:"performance-15",level:3},{value:"Documentation (10%)",id:"documentation-10",level:3},{value:"Submission Requirements",id:"submission-requirements",level:2},{value:"Resources and References",id:"resources-and-references",level:2},{value:"Troubleshooting Tips",id:"troubleshooting-tips",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"vla-practical-exercises",children:"VLA Practical Exercises"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This document contains hands-on exercises for Module 4: Vision-Language-Action (VLA) Systems. These exercises are designed to reinforce the concepts learned in each lesson and provide practical experience with implementing and using the VLA system components."}),"\n",(0,s.jsx)(n.h2,{id:"exercise-1-voice-command-processing",children:"Exercise 1: Voice Command Processing"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-11-basic-whisper-integration",children:"Exercise 1.1: Basic Whisper Integration"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Set up and test the Whisper speech-to-text system."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Initialize the Whisper processor with the 'base' model"}),"\n",(0,s.jsx)(n.li,{children:"Create a simple audio recording script that captures voice input"}),"\n",(0,s.jsx)(n.li,{children:"Process the recorded audio through Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Display the transcribed text and confidence score"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from src.vla.speech.whisper_processor import WhisperProcessor\r\nfrom src.vla.speech.voice_input_handler import VoiceInputHandler\r\nimport time\r\n\r\n# Initialize Whisper processor\r\nwhisper_proc = WhisperProcessor(model_size="base")\r\n\r\n# Record and transcribe\r\ndef record_and_transcribe():\r\n    print("Recording for 5 seconds...")\r\n    # Use voice_input_handler to record audio\r\n    # Process with whisper_proc.transcribe_audio()\r\n    # Print results\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Transcribed text with confidence score above 0.5"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Successfully transcribe at least 3 different voice commands with >0.6 confidence"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-12-voice-command-validation",children:"Exercise 1.2: Voice Command Validation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Implement and test voice command validation."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create a voice command validation system"}),"\n",(0,s.jsx)(n.li,{children:"Test various command formats and invalid inputs"}),"\n",(0,s.jsx)(n.li,{children:"Verify that valid commands pass validation"}),"\n",(0,s.jsx)(n.li,{children:"Ensure invalid commands are properly rejected"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from src.vla.utils.validators import VoiceCommandValidator\r\n\r\nvalidator = VoiceCommandValidator()\r\n\r\ntest_commands = [\r\n    "move forward",  # Valid\r\n    "pick up the red cube",  # Valid\r\n    "a",  # Invalid - too short\r\n    "xyz abc def ghi jkl"  # Valid length but invalid pattern\r\n]\r\n\r\nfor cmd in test_commands:\r\n    result = validator.validate_command(cmd, confidence=0.8)\r\n    print(f"Command: \'{cmd}\' -> Valid: {result[\'is_valid\']}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Valid commands marked as True, invalid as False"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Correctly validate at least 80% of test cases"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-2-llm-cognitive-planning",children:"Exercise 2: LLM Cognitive Planning"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-21-simple-command-planning",children:"Exercise 2.1: Simple Command Planning"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Plan actions for simple navigation commands using the LLM."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Initialize the LLM client and cognitive planner"}),"\n",(0,s.jsx)(n.li,{children:'Create a simple navigation command (e.g., "Go to the table")'}),"\n",(0,s.jsx)(n.li,{children:"Generate an action sequence using the planner"}),"\n",(0,s.jsx)(n.li,{children:"Verify the action sequence is appropriate for the command"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from src.vla.llm.llm_client import LLMClient\r\nfrom src.vla.llm.cognitive_planner import CognitivePlanner\r\n\r\n# Initialize components\r\nllm_client = LLMClient()  # Requires API key\r\nplanner = CognitivePlanner(llm_client)\r\n\r\n# Plan a simple command\r\ncommand = "Go to the table"\r\nintent = planner.plan_command(command)\r\n\r\nprint(f"Command: {command}")\r\nprint(f"Intent type: {intent.intent_type}")\r\nprint(f"Action sequence:")\r\nfor i, action in enumerate(intent.action_sequence):\r\n    print(f"  {i+1}. {action.action_type.value}: {action.parameters}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Action sequence with NAVIGATE_TO action and appropriate parameters"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Generate correct action sequences for 5 different navigation commands"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-22-complex-task-decomposition",children:"Exercise 2.2: Complex Task Decomposition"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Decompose complex multi-step commands into action sequences."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'Create a complex command (e.g., "Go to kitchen, find red cup, bring to table")'}),"\n",(0,s.jsx)(n.li,{children:"Plan the command using the cognitive planner"}),"\n",(0,s.jsx)(n.li,{children:"Verify the action sequence has multiple steps in logical order"}),"\n",(0,s.jsx)(n.li,{children:"Test with different complex commands"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Test complex command\r\ncomplex_command = "Go to the kitchen, find a red cup, pick it up, and bring it to the table"\r\n\r\n# Plan with mock objects for context\r\nfrom src.vla.models.detected_object import DetectedObject, Point3D\r\n\r\nmock_objects = [\r\n    DetectedObject(\r\n        id="cup_1",\r\n        class_name="cup",\r\n        confidence=0.85,\r\n        bbox=None,\r\n        position_3d=Point3D(x=1.0, y=2.0, z=0.0),\r\n        dimensions=None,\r\n        color="red",\r\n        is_graspable=True\r\n    )\r\n]\r\n\r\nintent = planner.plan_command(complex_command, context_objects=mock_objects)\r\nprint(f"Complex command planned with {len(intent.action_sequence)} actions")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Multi-step action sequence with navigation, manipulation, and navigation again"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Successfully plan 3 different complex commands with appropriate action sequences"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-3-visual-perception-integration",children:"Exercise 3: Visual Perception Integration"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-31-object-detection-test",children:"Exercise 3.1: Object Detection Test"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Test object detection with sample images."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Initialize the object detector"}),"\n",(0,s.jsx)(n.li,{children:"Create or use sample images with known objects"}),"\n",(0,s.jsx)(n.li,{children:"Run detection on the images"}),"\n",(0,s.jsx)(n.li,{children:"Verify detection accuracy and confidence scores"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\r\nfrom src.vla.vision.object_detector import ObjectDetector\r\n\r\ndetector = ObjectDetector(confidence_threshold=0.5)\r\n\r\n# Load a test image\r\ntest_image = cv2.imread("path/to/test/image.jpg")\r\n\r\n# Detect objects\r\ndetected_objects = detector.detect_objects(test_image)\r\n\r\nprint(f"Detected {len(detected_objects)} objects:")\r\nfor obj in detected_objects:\r\n    print(f"  - {obj.class_name} ({obj.color}) at {obj.bbox.x_min}, {obj.bbox.y_min}")\r\n    print(f"    Confidence: {obj.confidence:.2f}, Graspable: {obj.is_graspable}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": List of detected objects with class names, positions, and properties"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Detect at least 5 different object types with >0.6 confidence"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-32-3d-position-estimation",children:"Exercise 3.2: 3D Position Estimation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Test 3D position estimation from 2D images."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Initialize the perception pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Process images with known object positions"}),"\n",(0,s.jsx)(n.li,{children:"Verify 3D position estimates are reasonable"}),"\n",(0,s.jsx)(n.li,{children:"Test with different object distances"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from src.vla.vision.perception_pipeline import PerceptionPipeline\r\nfrom src.vla.vision.object_detector import ObjectDetector\r\n\r\ndetector = ObjectDetector()\r\npipeline = PerceptionPipeline(detector)\r\n\r\n# Process a frame (using mock robot pose)\r\nrobot_pose = {"x": 0.0, "y": 0.0, "theta": 0.0}\r\nperception_data = pipeline.process_frame(test_image, robot_pose)\r\n\r\nprint("Objects with 3D positions:")\r\nfor obj in perception_data.objects:\r\n    pos = obj.position_3d\r\n    print(f"  {obj.class_name}: ({pos.x:.2f}, {pos.y:.2f}, {pos.z:.2f})")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Objects with estimated 3D coordinates relative to robot"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Estimate positions within 0.2m accuracy for test objects"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-4-action-execution",children:"Exercise 4: Action Execution"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-41-single-action-execution",children:"Exercise 4.1: Single Action Execution"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Execute a single action in simulation."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Initialize the action executor"}),"\n",(0,s.jsx)(n.li,{children:"Create a simple action (e.g., navigation to specific coordinates)"}),"\n",(0,s.jsx)(n.li,{children:"Execute the action"}),"\n",(0,s.jsx)(n.li,{children:"Monitor execution status and completion"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from src.vla.ros2.action_executor import ActionExecutor\r\nfrom src.vla.models.action_step import ActionStep, ActionStepType\r\n\r\nexecutor = ActionExecutor()\r\n\r\n# Create a simple navigation action\r\nnav_action = ActionStep(\r\n    id="nav_test",\r\n    action_type=ActionStepType.NAVIGATE_TO,\r\n    parameters={"target_position": {"x": 1.0, "y": 0.5, "z": 0.0}},\r\n    timeout=10,\r\n    required_objects=[],\r\n    preconditions=[],\r\n    expected_outcomes=[]\r\n)\r\n\r\n# Execute the action\r\ndef status_callback(state):\r\n    print(f"Execution status: {state.status.value}, Progress: {state.progress:.2f}")\r\n\r\nexecution_id = executor.execute_action_sequence([nav_action], callback=status_callback)\r\nprint(f"Started execution: {execution_id}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Action execution with status updates and completion"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Successfully execute 5 different action types with >80% success rate"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-42-action-sequence-execution",children:"Exercise 4.2: Action Sequence Execution"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Execute a sequence of actions and monitor progress."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create an action sequence (e.g., navigate \u2192 grasp \u2192 navigate \u2192 place)"}),"\n",(0,s.jsx)(n.li,{children:"Execute the sequence"}),"\n",(0,s.jsx)(n.li,{children:"Monitor progress and state changes"}),"\n",(0,s.jsx)(n.li,{children:"Handle any failures gracefully"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from src.vla.models.action_step import ActionStep, ActionStepType\r\n\r\n# Create multi-step sequence\r\naction_sequence = [\r\n    ActionStep(\r\n        id="nav_to_object",\r\n        action_type=ActionStepType.NAVIGATE_TO,\r\n        parameters={"target_position": {"x": 1.0, "y": 1.0, "z": 0.0}},\r\n        timeout=10,\r\n        required_objects=[],\r\n        preconditions=[],\r\n        expected_outcomes=[]\r\n    ),\r\n    ActionStep(\r\n        id="grasp_object",\r\n        action_type=ActionStepType.GRASP_OBJECT,\r\n        parameters={"object_id": "test_object"},\r\n        timeout=8,\r\n        required_objects=["test_object"],\r\n        preconditions=[],\r\n        expected_outcomes=[]\r\n    ),\r\n    ActionStep(\r\n        id="nav_to_destination",\r\n        action_type=ActionStepType.NAVIGATE_TO,\r\n        parameters={"target_position": {"x": 2.0, "y": 0.0, "z": 0.0}},\r\n        timeout=10,\r\n        required_objects=[],\r\n        preconditions=[],\r\n        expected_outcomes=[]\r\n    ),\r\n    ActionStep(\r\n        id="place_object",\r\n        action_type=ActionStepType.RELEASE_OBJECT,\r\n        parameters={},\r\n        timeout=5,\r\n        required_objects=[],\r\n        preconditions=[],\r\n        expected_outcomes=[]\r\n    )\r\n]\r\n\r\n# Execute sequence\r\nexecution_id = executor.execute_action_sequence(action_sequence, callback=status_callback)\r\nprint(f"Multi-step execution started: {execution_id}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Sequential execution with progress tracking"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Execute 3 different multi-step sequences with >75% success rate"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-5-complete-vla-integration",children:"Exercise 5: Complete VLA Integration"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-51-end-to-end-command-processing",children:"Exercise 5.1: End-to-End Command Processing"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Process a complete voice command through the entire VLA pipeline."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up the complete VLA system with all components"}),"\n",(0,s.jsx)(n.li,{children:"Provide a natural language command (or use direct text input for testing)"}),"\n",(0,s.jsx)(n.li,{children:"Track the command through speech processing \u2192 planning \u2192 perception \u2192 execution"}),"\n",(0,s.jsx)(n.li,{children:"Verify the final outcome matches the command intent"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from src.vla.integration.vla_system import VLASystem\r\nfrom src.vla.integration.feedback_processor import FeedbackProcessor\r\nfrom src.vla.speech.whisper_processor import WhisperProcessor\r\nfrom src.vla.llm.llm_client import LLMClient\r\nfrom src.vla.vision.object_detector import ObjectDetector\r\nfrom src.vla.ros2.action_executor import ActionExecutor\r\n\r\n# Initialize all components\r\nwhisper_proc = WhisperProcessor(model_size="base")\r\nllm_client = LLMClient()  # Requires API key\r\nobject_detector = ObjectDetector()\r\naction_executor = ActionExecutor()\r\n\r\nvla_system = VLASystem(whisper_proc, llm_client, object_detector, action_executor)\r\nfeedback_processor = FeedbackProcessor()\r\n\r\n# Process a command end-to-end\r\ncommand_text = "Go to the table and pick up the red cube"\r\nexecution_id = vla_system.process_command_direct(command_text)\r\n\r\nprint(f"End-to-end command processed: {command_text}")\r\nprint(f"Execution ID: {execution_id}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Complete pipeline execution from command to action"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Successfully process 5 different commands through the full pipeline"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-52-capstone-task-simulation",children:"Exercise 5.2: Capstone Task Simulation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Simulate the capstone autonomous task with simplified components."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up a simplified version of the capstone task"}),"\n",(0,s.jsx)(n.li,{children:"Use mock components where real hardware/simulation isn't available"}),"\n",(0,s.jsx)(n.li,{children:"Execute a multi-step task similar to the capstone project"}),"\n",(0,s.jsx)(n.li,{children:"Validate each step and the overall task completion"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Template"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from src.vla.capstone.autonomous_task import AutonomousHumanoidTask\r\n\r\n# Create simplified capstone task\r\ncapstone_task = AutonomousHumanoidTask(vla_system, feedback_processor)\r\n\r\n# Define capstone-like sequence\r\ncapstone_commands = [\r\n    "Navigate to the kitchen area",\r\n    "Look for a red object",\r\n    "Grasp the red object",\r\n    "Move to the drop-off location",\r\n    "Place the object"\r\n]\r\n\r\nprint("Starting simplified capstone task...")\r\nfor cmd in capstone_commands:\r\n    print(f"Executing: {cmd}")\r\n    vla_system.process_command_direct(cmd)\r\n    # Add appropriate delays between commands\r\n    import time\r\n    time.sleep(3)\r\n\r\nprint("Simplified capstone task completed")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),": Multi-step task execution with feedback at each stage"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessment"}),": Complete the simplified capstone task with at least 80% step success rate"]}),"\n",(0,s.jsx)(n.h2,{id:"advanced-exercises",children:"Advanced Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-a1-performance-optimization",children:"Exercise A1: Performance Optimization"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Optimize the VLA system for better performance."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Measure current performance of each component"}),"\n",(0,s.jsx)(n.li,{children:"Identify bottlenecks in the pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Implement optimizations (model size, threading, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Re-measure performance and calculate improvements"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-a2-error-handling-enhancement",children:"Exercise A2: Error Handling Enhancement"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Improve error handling and recovery in the VLA system."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Introduce various error conditions in the pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Implement comprehensive error detection"}),"\n",(0,s.jsx)(n.li,{children:"Add recovery mechanisms for common failures"}),"\n",(0,s.jsx)(n.li,{children:"Test system resilience under error conditions"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-a3-custom-object-training",children:"Exercise A3: Custom Object Training"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Train a custom object detector for specific objects."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Collect images of custom objects"}),"\n",(0,s.jsx)(n.li,{children:"Label the images for training"}),"\n",(0,s.jsx)(n.li,{children:"Train a custom YOLO model"}),"\n",(0,s.jsx)(n.li,{children:"Integrate the custom model into the VLA system"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,s.jsx)(n.h3,{id:"technical-implementation-40",children:"Technical Implementation (40%)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Correct implementation of all components"}),"\n",(0,s.jsx)(n.li,{children:"Proper integration between modules"}),"\n",(0,s.jsx)(n.li,{children:"Error handling and validation"}),"\n",(0,s.jsx)(n.li,{children:"Code quality and documentation"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"functionality-35",children:"Functionality (35%)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Successful execution of planned actions"}),"\n",(0,s.jsx)(n.li,{children:"Accurate voice command processing"}),"\n",(0,s.jsx)(n.li,{children:"Proper object detection and manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Multi-step task completion"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-15",children:"Performance (15%)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Response time efficiency"}),"\n",(0,s.jsx)(n.li,{children:"Accuracy of processing"}),"\n",(0,s.jsx)(n.li,{children:"Resource utilization"}),"\n",(0,s.jsx)(n.li,{children:"System stability"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"documentation-10",children:"Documentation (10%)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Clear code comments"}),"\n",(0,s.jsx)(n.li,{children:"Proper function documentation"}),"\n",(0,s.jsx)(n.li,{children:"Test results and validation"}),"\n",(0,s.jsx)(n.li,{children:"Troubleshooting notes"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"submission-requirements",children:"Submission Requirements"}),"\n",(0,s.jsx)(n.p,{children:"For each exercise, submit:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Code files"}),": Complete implementation files"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Test results"}),": Output and validation results"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance metrics"}),": Timing and accuracy measurements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Documentation"}),": Brief explanation of implementation approach"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reflection"}),": What was learned and challenges faced"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"resources-and-references",children:"Resources and References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"VLA System Architecture Diagram"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 Action Interface Documentation"}),"\n",(0,s.jsx)(n.li,{children:"Whisper API Documentation"}),"\n",(0,s.jsx)(n.li,{children:"YOLO Object Detection Guide"}),"\n",(0,s.jsx)(n.li,{children:"NVIDIA Isaac Sim Integration Guide"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-tips",children:"Troubleshooting Tips"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"If audio input fails, check microphone permissions and configuration"}),"\n",(0,s.jsx)(n.li,{children:"If LLM calls fail, verify API key and network connectivity"}),"\n",(0,s.jsx)(n.li,{children:"If ROS 2 nodes don't communicate, ensure proper environment sourcing"}),"\n",(0,s.jsx)(n.li,{children:"If Isaac Sim connection fails, check server status and network ports"}),"\n",(0,s.jsx)(n.li,{children:"For object detection issues, verify image format and model compatibility"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Complete these exercises in order to build a comprehensive understanding of the VLA system. Each exercise builds upon the previous ones, so ensure you have working implementations before proceeding to the next exercise."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);