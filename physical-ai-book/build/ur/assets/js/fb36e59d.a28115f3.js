"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[9913],{7546:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>f,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-3/performance-monitoring","title":"Performance Monitoring Tools for Isaac Sim AI-Robot Systems","description":"Overview","source":"@site/docs/chapter-3/performance-monitoring.md","sourceDirName":"chapter-3","slug":"/chapter-3/performance-monitoring","permalink":"/ur/docs/chapter-3/performance-monitoring","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-3/performance-monitoring.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Safety and Reliability Framework for Isaac Sim AI-Robot Systems","permalink":"/ur/docs/chapter-3/safety-framework"},"next":{"title":"Module 3 Summary: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/ur/docs/chapter-3/summary"}}');var t=n(4848),a=n(8453);const i={},o="Performance Monitoring Tools for Isaac Sim AI-Robot Systems",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Introduction to Performance Monitoring",id:"introduction-to-performance-monitoring",level:2},{value:"What is Performance Monitoring in Isaac Sim?",id:"what-is-performance-monitoring-in-isaac-sim",level:3},{value:"Key Performance Metrics",id:"key-performance-metrics",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"System Resource Monitoring",id:"system-resource-monitoring",level:2},{value:"1. Creating System Resource Monitor Node",id:"1-creating-system-resource-monitor-node",level:3},{value:"2. Creating AI Model Performance Monitor",id:"2-creating-ai-model-performance-monitor",level:3},{value:"Navigation Performance Monitoring",id:"navigation-performance-monitoring",level:2},{value:"1. Creating Navigation Performance Monitor",id:"1-creating-navigation-performance-monitor",level:3},{value:"Communication Performance Monitoring",id:"communication-performance-monitoring",level:2},{value:"1. Creating Communication Monitor",id:"1-creating-communication-monitor",level:3},{value:"Performance Dashboard and Visualization",id:"performance-dashboard-and-visualization",level:2},{value:"1. Creating Performance Dashboard",id:"1-creating-performance-dashboard",level:3},{value:"Performance Monitoring Launch Files",id:"performance-monitoring-launch-files",level:2},{value:"1. Creating Performance Monitoring Package",id:"1-creating-performance-monitoring-package",level:3},{value:"2. Creating Setup Files for Performance Package",id:"2-creating-setup-files-for-performance-package",level:3},{value:"3. Creating Performance Monitoring Launch File",id:"3-creating-performance-monitoring-launch-file",level:3},{value:"Performance Logging and Analysis",id:"performance-logging-and-analysis",level:2},{value:"1. Creating Performance Logger",id:"1-creating-performance-logger",level:3},{value:"Performance Alerting System",id:"performance-alerting-system",level:2},{value:"1. Creating Performance Alert System",id:"1-creating-performance-alert-system",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"1. Creating Performance Monitoring Test Script",id:"1-creating-performance-monitoring-test-script",level:3},{value:"2. Running Performance Monitoring Test",id:"2-running-performance-monitoring-test",level:3},{value:"Troubleshooting Performance Monitoring",id:"troubleshooting-performance-monitoring",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Issue: &quot;Performance monitoring nodes consuming too many resources&quot;",id:"issue-performance-monitoring-nodes-consuming-too-many-resources",level:4},{value:"Issue: &quot;No data being published to performance topics&quot;",id:"issue-no-data-being-published-to-performance-topics",level:4},{value:"Issue: &quot;Python packages not found for monitoring&quot;",id:"issue-python-packages-not-found-for-monitoring",level:4},{value:"Verification Checklist",id:"verification-checklist",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"performance-monitoring-tools-for-isaac-sim-ai-robot-systems",children:"Performance Monitoring Tools for Isaac Sim AI-Robot Systems"})}),"\n",(0,t.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(r.p,{children:"This document provides comprehensive instructions for setting up and using performance monitoring tools for the NVIDIA Isaac Sim AI-robot systems in Module 3. The focus is on monitoring system performance, resource utilization, and AI model efficiency to ensure optimal operation of the AI-robot brain system."}),"\n",(0,t.jsx)(r.h2,{id:"introduction-to-performance-monitoring",children:"Introduction to Performance Monitoring"}),"\n",(0,t.jsx)(r.h3,{id:"what-is-performance-monitoring-in-isaac-sim",children:"What is Performance Monitoring in Isaac Sim?"}),"\n",(0,t.jsx)(r.p,{children:"Performance monitoring in Isaac Sim involves tracking and analyzing various metrics to ensure that the AI-robot system operates efficiently and reliably. This includes:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"System resource utilization (CPU, GPU, memory)"}),"\n",(0,t.jsx)(r.li,{children:"AI model performance (inference time, accuracy, throughput)"}),"\n",(0,t.jsx)(r.li,{children:"Sensor data processing (latency, frequency, quality)"}),"\n",(0,t.jsx)(r.li,{children:"Navigation and control performance (path efficiency, success rate)"}),"\n",(0,t.jsx)(r.li,{children:"Communication efficiency (ROS topic latency, bandwidth)"}),"\n"]}),"\n",(0,t.jsx)(r.h3,{id:"key-performance-metrics",children:"Key Performance Metrics"}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Computational Performance"}),": CPU/GPU utilization, memory usage"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"AI Model Performance"}),": Inference time, frames per second (FPS)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Sensor Performance"}),": Data frequency, latency, accuracy"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Navigation Performance"}),": Path efficiency, success rate, safety"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"System Integration"}),": End-to-end latency, resource sharing"]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(r.p,{children:"Before setting up performance monitoring, ensure you have:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Completed Isaac Sim and Isaac ROS setup"}),"\n",(0,t.jsx)(r.li,{children:"Basic understanding of ROS 2 topics and services"}),"\n",(0,t.jsx)(r.li,{children:"Familiarity with system monitoring tools"}),"\n",(0,t.jsx)(r.li,{children:"NVIDIA GPU with proper monitoring capabilities"}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"system-resource-monitoring",children:"System Resource Monitoring"}),"\n",(0,t.jsx)(r.h3,{id:"1-creating-system-resource-monitor-node",children:"1. Creating System Resource Monitor Node"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_performance_monitor/performance_monitor/system_monitor.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import Float32, String\r\nfrom sensor_msgs.msg import BatteryState\r\nimport psutil\r\nimport GPUtil\r\nimport os\r\nimport time\r\nfrom collections import deque\r\n\r\nclass SystemMonitor(Node):\r\n    def __init__(self):\r\n        super().__init__('system_monitor')\r\n\r\n        # Publishers for system metrics\r\n        self.cpu_usage_publisher = self.create_publisher(Float32, '/system/cpu_usage', 10)\r\n        self.memory_usage_publisher = self.create_publisher(Float32, '/system/memory_usage', 10)\r\n        self.gpu_usage_publisher = self.create_publisher(Float32, '/system/gpu_usage', 10)\r\n        self.gpu_memory_publisher = self.create_publisher(Float32, '/system/gpu_memory_usage', 10)\r\n        self.disk_usage_publisher = self.create_publisher(Float32, '/system/disk_usage', 10)\r\n        self.temperature_publisher = self.create_publisher(Float32, '/system/cpu_temperature', 10)\r\n        self.status_publisher = self.create_publisher(String, '/system/status', 10)\r\n\r\n        # Performance tracking\r\n        self.cpu_history = deque(maxlen=100)\r\n        self.memory_history = deque(maxlen=100)\r\n        self.gpu_history = deque(maxlen=100)\r\n\r\n        # Setup timer for periodic monitoring\r\n        self.monitor_timer = self.create_timer(1.0, self.monitor_system)\r\n\r\n        # Setup timer for status publishing\r\n        self.status_timer = self.create_timer(5.0, self.publish_status)\r\n\r\n        self.get_logger().info('System Monitor initialized')\r\n\r\n    def monitor_system(self):\r\n        \"\"\"Monitor system resources and publish metrics\"\"\"\r\n        try:\r\n            # CPU usage\r\n            cpu_percent = psutil.cpu_percent()\r\n            cpu_msg = Float32()\r\n            cpu_msg.data = float(cpu_percent)\r\n            self.cpu_usage_publisher.publish(cpu_msg)\r\n\r\n            # Memory usage\r\n            memory = psutil.virtual_memory()\r\n            memory_msg = Float32()\r\n            memory_msg.data = float(memory.percent)\r\n            self.memory_usage_publisher.publish(memory_msg)\r\n\r\n            # GPU usage (if available)\r\n            gpus = GPUtil.getGPUs()\r\n            if gpus:\r\n                gpu = gpus[0]  # Assuming single GPU\r\n                gpu_msg = Float32()\r\n                gpu_msg.data = float(gpu.load * 100)\r\n                self.gpu_usage_publisher.publish(gpu_msg)\r\n\r\n                gpu_memory_msg = Float32()\r\n                gpu_memory_msg.data = float(gpu.memoryUtil * 100)\r\n                self.gpu_memory_publisher.publish(gpu_memory_msg)\r\n\r\n            # Disk usage\r\n            disk_usage = psutil.disk_usage('/')\r\n            disk_msg = Float32()\r\n            disk_msg.data = float((disk_usage.used / disk_usage.total) * 100)\r\n            self.disk_usage_publisher.publish(disk_msg)\r\n\r\n            # Store history for trend analysis\r\n            self.cpu_history.append(cpu_percent)\r\n            self.memory_history.append(memory.percent)\r\n            if gpus:\r\n                self.gpu_history.append(gpu.load * 100)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error monitoring system: {e}')\r\n\r\n    def publish_status(self):\r\n        \"\"\"Publish overall system status\"\"\"\r\n        try:\r\n            status_msg = String()\r\n\r\n            # Determine status based on resource usage\r\n            cpu_avg = sum(self.cpu_history) / len(self.cpu_history) if self.cpu_history else 0\r\n            memory_avg = sum(self.memory_history) / len(self.memory_history) if self.memory_history else 0\r\n\r\n            if cpu_avg > 80 or memory_avg > 80:\r\n                status_msg.data = f\"WARNING: High resource usage - CPU: {cpu_avg:.1f}%, Memory: {memory_avg:.1f}%\"\r\n            else:\r\n                status_msg.data = f\"OK: CPU: {cpu_avg:.1f}%, Memory: {memory_avg:.1f}%\"\r\n\r\n            self.status_publisher.publish(status_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error publishing status: {e}')\r\n\r\n    def get_system_summary(self):\r\n        \"\"\"Get a summary of system performance\"\"\"\r\n        summary = {\r\n            'timestamp': time.time(),\r\n            'cpu_avg': sum(self.cpu_history) / len(self.cpu_history) if self.cpu_history else 0,\r\n            'memory_avg': sum(self.memory_history) / len(self.memory_history) if self.memory_history else 0,\r\n            'gpu_avg': sum(self.gpu_history) / len(self.gpu_history) if self.gpu_history else 0,\r\n            'cpu_peak': max(self.cpu_history) if self.cpu_history else 0,\r\n            'memory_peak': max(self.memory_history) if self.memory_history else 0\r\n        }\r\n        return summary\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    monitor = SystemMonitor()\r\n\r\n    try:\r\n        rclpy.spin(monitor)\r\n    except KeyboardInterrupt:\r\n        summary = monitor.get_system_summary()\r\n        print(f\"System Summary: {summary}\")\r\n    finally:\r\n        monitor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(r.h3,{id:"2-creating-ai-model-performance-monitor",children:"2. Creating AI Model Performance Monitor"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_performance_monitor/performance_monitor/model_performance_monitor.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import Float32, String\r\nfrom vision_msgs.msg import Detection2DArray\r\nimport time\r\nfrom collections import deque\r\nimport threading\r\n\r\nclass ModelPerformanceMonitor(Node):\r\n    def __init__(self):\r\n        super().__init__('model_performance_monitor')\r\n\r\n        # Subscribe to model input/output topics\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.detection_subscription = self.create_subscription(\r\n            Detection2DArray,\r\n            '/object_detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers for performance metrics\r\n        self.fps_publisher = self.create_publisher(Float32, '/model/fps', 10)\r\n        self.latency_publisher = self.create_publisher(Float32, '/model/latency', 10)\r\n        self.throughput_publisher = self.create_publisher(Float32, '/model/throughput', 10)\r\n        self.accuracy_publisher = self.create_publisher(Float32, '/model/accuracy', 10)\r\n        self.status_publisher = self.create_publisher(String, '/model/status', 10)\r\n\r\n        # Performance tracking\r\n        self.frame_times = deque(maxlen=30)  # Last 30 frames for FPS calculation\r\n        self.processing_times = deque(maxlen=30)  # Last 30 processing times\r\n        self.detection_counts = deque(maxlen=30)  # Detection counts per frame\r\n        self.last_image_time = None\r\n        self.image_count = 0\r\n        self.detection_count = 0\r\n\r\n        # Setup timer for periodic metrics publishing\r\n        self.timer = self.create_timer(1.0, self.publish_metrics)\r\n\r\n        self.get_logger().info('Model Performance Monitor initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Track image processing for performance measurement\"\"\"\r\n        current_time = self.get_clock().now().nanoseconds / 1e9\r\n        self.image_count += 1\r\n\r\n        if self.last_image_time is not None:\r\n            frame_time = current_time - self.last_image_time\r\n            self.frame_times.append(frame_time)\r\n\r\n        self.last_image_time = current_time\r\n\r\n    def detection_callback(self, msg):\r\n        \"\"\"Track detection processing\"\"\"\r\n        self.detection_count += 1\r\n        self.detection_counts.append(len(msg.detections))\r\n\r\n        # Calculate processing time if we have corresponding image time\r\n        if self.last_image_time is not None:\r\n            current_time = self.get_clock().now().nanoseconds / 1e9\r\n            processing_time = current_time - self.last_image_time\r\n            self.processing_times.append(processing_time)\r\n\r\n    def publish_metrics(self):\r\n        \"\"\"Publish performance metrics\"\"\"\r\n        # Calculate FPS\r\n        fps = 0.0\r\n        if len(self.frame_times) > 0:\r\n            avg_frame_time = sum(self.frame_times) / len(self.frame_times)\r\n            fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0.0\r\n\r\n        fps_msg = Float32()\r\n        fps_msg.data = fps\r\n        self.fps_publisher.publish(fps_msg)\r\n\r\n        # Calculate average processing latency\r\n        avg_latency = 0.0\r\n        if len(self.processing_times) > 0:\r\n            avg_latency = sum(self.processing_times) / len(self.processing_times)\r\n\r\n        latency_msg = Float32()\r\n        latency_msg.data = avg_latency\r\n        self.latency_publisher.publish(latency_msg)\r\n\r\n        # Calculate throughput (detections per second)\r\n        throughput = self.detection_count\r\n        throughput_msg = Float32()\r\n        throughput_msg.data = float(throughput)\r\n        self.throughput_publisher.publish(throughput_msg)\r\n\r\n        # Calculate average detections per frame\r\n        avg_detections = 0.0\r\n        if len(self.detection_counts) > 0:\r\n            avg_detections = sum(self.detection_counts) / len(self.detection_counts)\r\n\r\n        accuracy_msg = Float32()\r\n        accuracy_msg.data = avg_detections  # Using avg detections as a proxy for activity\r\n        self.accuracy_publisher.publish(accuracy_msg)\r\n\r\n        # Publish status\r\n        status_msg = String()\r\n        status_msg.data = f\"FPS: {fps:.2f}, Latency: {avg_latency*1000:.2f}ms, Avg Detections: {avg_detections:.2f}\"\r\n        self.status_publisher.publish(status_msg)\r\n\r\n        # Reset counters for next period\r\n        self.detection_count = 0\r\n\r\n    def get_performance_summary(self):\r\n        \"\"\"Get a summary of performance metrics\"\"\"\r\n        summary = {\r\n            'fps_avg': 0.0,\r\n            'latency_avg_ms': 0.0,\r\n            'throughput_avg': 0.0,\r\n            'detections_per_frame_avg': 0.0,\r\n            'total_samples': len(self.frame_times)\r\n        }\r\n\r\n        if len(self.frame_times) > 0:\r\n            avg_frame_time = sum(self.frame_times) / len(self.frame_times)\r\n            summary['fps_avg'] = 1.0 / avg_frame_time if avg_frame_time > 0 else 0.0\r\n\r\n        if len(self.processing_times) > 0:\r\n            summary['latency_avg_ms'] = (sum(self.processing_times) / len(self.processing_times)) * 1000\r\n\r\n        if len(self.detection_counts) > 0:\r\n            summary['detections_per_frame_avg'] = sum(self.detection_counts) / len(self.detection_counts)\r\n\r\n        return summary\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    monitor = ModelPerformanceMonitor()\r\n\r\n    try:\r\n        rclpy.spin(monitor)\r\n    except KeyboardInterrupt:\r\n        summary = monitor.get_performance_summary()\r\n        print(f\"Model Performance Summary: {summary}\")\r\n    finally:\r\n        monitor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(r.h2,{id:"navigation-performance-monitoring",children:"Navigation Performance Monitoring"}),"\n",(0,t.jsx)(r.h3,{id:"1-creating-navigation-performance-monitor",children:"1. Creating Navigation Performance Monitor"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:'cat > ~/isaac_ros_ws/src/isaac_performance_monitor/performance_monitor/navigation_monitor.py << \'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom nav_msgs.msg import Path, Odometry\r\nfrom geometry_msgs.msg import PoseStamped, Twist\r\nfrom std_msgs.msg import Float32, String\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom rclpy.action import ActionClient\r\nfrom rclpy.qos import QoSProfile\r\nimport math\r\nfrom collections import deque\r\nimport time\r\n\r\nclass NavigationPerformanceMonitor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'navigation_performance_monitor\')\r\n\r\n        # Subscribers for navigation-related topics\r\n        self.path_subscription = self.create_subscription(\r\n            Path,\r\n            \'/plan\',\r\n            self.path_callback,\r\n            10\r\n        )\r\n\r\n        self.odom_subscription = self.create_subscription(\r\n            Odometry,\r\n            \'/odom\',\r\n            self.odom_callback,\r\n            10\r\n        )\r\n\r\n        self.cmd_vel_subscription = self.create_subscription(\r\n            Twist,\r\n            \'/cmd_vel\',\r\n            self.cmd_vel_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers for navigation metrics\r\n        self.path_efficiency_publisher = self.create_publisher(Float32, \'/nav/path_efficiency\', 10)\r\n        self.success_rate_publisher = self.create_publisher(Float32, \'/nav/success_rate\', 10)\r\n        self.time_to_goal_publisher = self.create_publisher(Float32, \'/nav/time_to_goal\', 10)\r\n        self.safety_score_publisher = self.create_publisher(Float32, \'/nav/safety_score\', 10)\r\n        self.status_publisher = self.create_publisher(String, \'/nav/status\', 10)\r\n\r\n        # Navigation tracking\r\n        self.start_pose = None\r\n        self.goal_pose = None\r\n        self.current_pose = None\r\n        self.path_length = 0.0\r\n        self.actual_distance = 0.0\r\n        self.start_time = None\r\n        self.navigation_active = False\r\n        self.success_count = 0\r\n        self.attempt_count = 0\r\n        self.collision_count = 0\r\n        self.path_history = []\r\n        self.velocity_history = deque(maxlen=100)\r\n\r\n        # Setup timer for periodic metrics publishing\r\n        self.timer = self.create_timer(1.0, self.publish_metrics)\r\n\r\n        # Navigation action client\r\n        self.nav_to_pose_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\r\n\r\n        self.get_logger().info(\'Navigation Performance Monitor initialized\')\r\n\r\n    def path_callback(self, msg):\r\n        """Track planned path for efficiency calculation"""\r\n        if len(msg.poses) > 1:\r\n            # Calculate path length\r\n            total_length = 0.0\r\n            for i in range(len(msg.poses) - 1):\r\n                p1 = msg.poses[i].pose.position\r\n                p2 = msg.poses[i+1].pose.position\r\n                segment_length = math.sqrt(\r\n                    (p2.x - p1.x)**2 + (p2.y - p1.y)**2\r\n                )\r\n                total_length += segment_length\r\n\r\n            self.path_length = total_length\r\n\r\n    def odom_callback(self, msg):\r\n        """Track robot position for navigation metrics"""\r\n        self.current_pose = msg.pose.pose\r\n\r\n        if self.start_pose and self.current_pose:\r\n            # Calculate distance traveled\r\n            dx = self.current_pose.position.x - self.start_pose.position.x\r\n            dy = self.current_pose.position.y - self.start_pose.position.y\r\n            self.actual_distance = math.sqrt(dx*dx + dy*dy)\r\n\r\n    def cmd_vel_callback(self, msg):\r\n        """Track velocity for safety metrics"""\r\n        speed = math.sqrt(msg.linear.x**2 + msg.linear.y**2)\r\n        self.velocity_history.append(speed)\r\n\r\n    def start_navigation_tracking(self, start_pose, goal_pose):\r\n        """Start tracking navigation performance"""\r\n        self.start_pose = start_pose\r\n        self.goal_pose = goal_pose\r\n        self.start_time = time.time()\r\n        self.navigation_active = True\r\n        self.path_length = 0.0\r\n        self.actual_distance = 0.0\r\n\r\n    def finish_navigation_tracking(self, success):\r\n        """Finish navigation tracking and update statistics"""\r\n        if self.start_time:\r\n            elapsed_time = time.time() - self.start_time\r\n            self.time_to_goal_publisher.publish(Float32(data=float(elapsed_time)))\r\n\r\n        self.navigation_active = False\r\n        self.attempt_count += 1\r\n\r\n        if success:\r\n            self.success_count += 1\r\n\r\n    def publish_metrics(self):\r\n        """Publish navigation performance metrics"""\r\n        # Calculate path efficiency\r\n        efficiency = 0.0\r\n        if self.path_length > 0 and self.actual_distance > 0:\r\n            efficiency = self.path_length / self.actual_distance\r\n\r\n        efficiency_msg = Float32()\r\n        efficiency_msg.data = efficiency\r\n        self.path_efficiency_publisher.publish(efficiency_msg)\r\n\r\n        # Calculate success rate\r\n        success_rate = 0.0\r\n        if self.attempt_count > 0:\r\n            success_rate = (self.success_count / self.attempt_count) * 100\r\n\r\n        success_msg = Float32()\r\n        success_msg.data = success_rate\r\n        self.success_rate_publisher.publish(success_msg)\r\n\r\n        # Calculate safety score (simplified)\r\n        avg_velocity = sum(self.velocity_history) / len(self.velocity_history) if self.velocity_history else 0.0\r\n        # Safety score: lower velocities = higher safety\r\n        safety_score = max(0, 100 - (avg_velocity * 10))  # Adjust scale as needed\r\n\r\n        safety_msg = Float32()\r\n        safety_msg.data = safety_score\r\n        self.safety_score_publisher.publish(safety_msg)\r\n\r\n        # Publish status\r\n        status_msg = String()\r\n        status_msg.data = f"Efficiency: {efficiency:.2f}, Success Rate: {success_rate:.1f}%, Safety: {safety_score:.1f}"\r\n        self.status_publisher.publish(status_msg)\r\n\r\n    def get_navigation_summary(self):\r\n        """Get a summary of navigation performance"""\r\n        return {\r\n            \'success_rate_percent\': (self.success_count / self.attempt_count * 100) if self.attempt_count > 0 else 0,\r\n            \'avg_path_efficiency\': sum([p.length for p in self.path_history]) / len(self.path_history) if self.path_history else 0,\r\n            \'avg_time_to_goal\': 0,  # Would need to track multiple navigation attempts\r\n            \'collision_count\': self.collision_count,\r\n            \'total_attempts\': self.attempt_count\r\n        }\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    monitor = NavigationPerformanceMonitor()\r\n\r\n    try:\r\n        rclpy.spin(monitor)\r\n    except KeyboardInterrupt:\r\n        summary = monitor.get_navigation_summary()\r\n        print(f"Navigation Performance Summary: {summary}")\r\n    finally:\r\n        monitor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\nEOF\n'})}),"\n",(0,t.jsx)(r.h2,{id:"communication-performance-monitoring",children:"Communication Performance Monitoring"}),"\n",(0,t.jsx)(r.h3,{id:"1-creating-communication-monitor",children:"1. Creating Communication Monitor"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_performance_monitor/performance_monitor/communication_monitor.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String, Float32\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu\r\nimport time\r\nfrom collections import deque, defaultdict\r\n\r\nclass CommunicationMonitor(Node):\r\n    def __init__(self):\r\n        super().__init__('communication_monitor')\r\n\r\n        # Topic-specific subscribers with timing\r\n        self.topic_stats = defaultdict(lambda: {\r\n            'count': 0,\r\n            'latency_history': deque(maxlen=100),\r\n            'frequency_history': deque(maxlen=100),\r\n            'last_timestamp': None,\r\n            'last_received': None\r\n        })\r\n\r\n        # Subscribe to common topics\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.create_message_callback('/camera/color/image_raw'),\r\n            10\r\n        )\r\n\r\n        self.lidar_subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.create_message_callback('/scan'),\r\n            10\r\n        )\r\n\r\n        self.imu_subscription = self.create_subscription(\r\n            Imu,\r\n            '/imu',\r\n            self.create_message_callback('/imu'),\r\n            10\r\n        )\r\n\r\n        # Publishers for communication metrics\r\n        self.latency_publisher = self.create_publisher(Float32, '/comm/avg_latency', 10)\r\n        self.frequency_publisher = self.create_publisher(Float32, '/comm/frequency', 10)\r\n        self.status_publisher = self.create_publisher(String, '/comm/status', 10)\r\n\r\n        # Setup timer for periodic metrics publishing\r\n        self.timer = self.create_timer(1.0, self.publish_metrics)\r\n\r\n        self.get_logger().info('Communication Monitor initialized')\r\n\r\n    def create_message_callback(self, topic_name):\r\n        \"\"\"Create a callback function for a specific topic\"\"\"\r\n        def callback(msg):\r\n            current_time = self.get_clock().now().nanoseconds / 1e9\r\n\r\n            # Calculate latency (time from message creation to reception)\r\n            if hasattr(msg.header, 'stamp'):\r\n                msg_time = msg.header.stamp.sec + msg.header.stamp.nanosec / 1e9\r\n                latency = current_time - msg_time\r\n                if latency >= 0:  # Only consider positive latencies\r\n                    self.topic_stats[topic_name]['latency_history'].append(latency)\r\n\r\n            # Update statistics\r\n            self.topic_stats[topic_name]['count'] += 1\r\n            self.topic_stats[topic_name]['last_received'] = current_time\r\n\r\n        return callback\r\n\r\n    def publish_metrics(self):\r\n        \"\"\"Publish communication performance metrics\"\"\"\r\n        # Calculate average latency across all topics\r\n        total_latency = 0\r\n        latency_count = 0\r\n\r\n        for topic, stats in self.topic_stats.items():\r\n            if stats['latency_history']:\r\n                avg_topic_latency = sum(stats['latency_history']) / len(stats['latency_history'])\r\n                total_latency += avg_topic_latency\r\n                latency_count += 1\r\n\r\n        avg_latency = total_latency / latency_count if latency_count > 0 else 0.0\r\n\r\n        latency_msg = Float32()\r\n        latency_msg.data = avg_latency\r\n        self.latency_publisher.publish(latency_msg)\r\n\r\n        # Calculate average frequency across all topics\r\n        total_freq = 0\r\n        freq_count = 0\r\n\r\n        for topic, stats in self.topic_stats.items():\r\n            if stats['last_received'] and stats['last_timestamp']:\r\n                # Calculate frequency based on time intervals\r\n                time_diff = stats['last_received'] - stats['last_timestamp']\r\n                if time_diff > 0:\r\n                    freq = 1.0 / time_diff\r\n                    stats['frequency_history'].append(freq)\r\n                    total_freq += freq\r\n                    freq_count += 1\r\n\r\n            # Update timestamp for next calculation\r\n            stats['last_timestamp'] = stats['last_received']\r\n\r\n        avg_freq = total_freq / freq_count if freq_count > 0 else 0.0\r\n\r\n        freq_msg = Float32()\r\n        freq_msg.data = avg_freq\r\n        self.frequency_publisher.publish(freq_msg)\r\n\r\n        # Publish status\r\n        status_msg = String()\r\n        status_msg.data = f\"Avg Latency: {avg_latency*1000:.2f}ms, Avg Freq: {avg_freq:.2f}Hz\"\r\n        self.status_publisher.publish(status_msg)\r\n\r\n    def get_communication_summary(self):\r\n        \"\"\"Get a summary of communication performance\"\"\"\r\n        summary = {\r\n            'topics_monitored': list(self.topic_stats.keys()),\r\n            'avg_latency_ms': 0.0,\r\n            'avg_frequency': 0.0,\r\n            'message_counts': {}\r\n        }\r\n\r\n        # Calculate overall statistics\r\n        total_latency = 0\r\n        latency_count = 0\r\n        total_freq = 0\r\n        freq_count = 0\r\n\r\n        for topic, stats in self.topic_stats.items():\r\n            summary['message_counts'][topic] = stats['count']\r\n\r\n            if stats['latency_history']:\r\n                avg_topic_latency = sum(stats['latency_history']) / len(stats['latency_history'])\r\n                total_latency += avg_topic_latency\r\n                latency_count += 1\r\n\r\n            if stats['frequency_history']:\r\n                avg_topic_freq = sum(stats['frequency_history']) / len(stats['frequency_history'])\r\n                total_freq += avg_topic_freq\r\n                freq_count += 1\r\n\r\n        summary['avg_latency_ms'] = (total_latency / latency_count * 1000) if latency_count > 0 else 0.0\r\n        summary['avg_frequency'] = total_freq / freq_count if freq_count > 0 else 0.0\r\n\r\n        return summary\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    monitor = CommunicationMonitor()\r\n\r\n    try:\r\n        rclpy.spin(monitor)\r\n    except KeyboardInterrupt:\r\n        summary = monitor.get_communication_summary()\r\n        print(f\"Communication Performance Summary: {summary}\")\r\n    finally:\r\n        monitor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(r.h2,{id:"performance-dashboard-and-visualization",children:"Performance Dashboard and Visualization"}),"\n",(0,t.jsx)(r.h3,{id:"1-creating-performance-dashboard",children:"1. Creating Performance Dashboard"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cat > ~/isaac_sim_shared/scripts/performance_dashboard.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import Float32, String\r\nfrom sensor_msgs.msg import Image\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.animation import FuncAnimation\r\nfrom matplotlib.widgets import Button\r\nimport numpy as np\r\nimport threading\r\nimport time\r\nfrom collections import deque\r\nimport tkinter as tk\r\nfrom tkinter import ttk\r\n\r\nclass PerformanceDashboard(Node):\r\n    def __init__(self):\r\n        super().__init__('performance_dashboard')\r\n\r\n        # Data storage for visualization\r\n        self.cpu_data = deque(maxlen=100)\r\n        self.memory_data = deque(maxlen=100)\r\n        self.gpu_data = deque(maxlen=100)\r\n        self.fps_data = deque(maxlen=100)\r\n        self.latency_data = deque(maxlen=100)\r\n\r\n        # Subscribe to performance metrics\r\n        self.cpu_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/cpu_usage',\r\n            self.cpu_callback,\r\n            10\r\n        )\r\n\r\n        self.memory_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/memory_usage',\r\n            self.memory_callback,\r\n            10\r\n        )\r\n\r\n        self.gpu_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/gpu_usage',\r\n            self.gpu_callback,\r\n            10\r\n        )\r\n\r\n        self.fps_subscription = self.create_subscription(\r\n            Float32,\r\n            '/model/fps',\r\n            self.fps_callback,\r\n            10\r\n        )\r\n\r\n        self.latency_subscription = self.create_subscription(\r\n            Float32,\r\n            '/model/latency',\r\n            self.latency_callback,\r\n            10\r\n        )\r\n\r\n        # Setup visualization\r\n        self.setup_visualization()\r\n\r\n        # Start dashboard thread\r\n        self.dashboard_thread = threading.Thread(target=self.run_dashboard)\r\n        self.dashboard_thread.daemon = True\r\n        self.dashboard_thread.start()\r\n\r\n        self.get_logger().info('Performance Dashboard initialized')\r\n\r\n    def cpu_callback(self, msg):\r\n        self.cpu_data.append(msg.data)\r\n\r\n    def memory_callback(self, msg):\r\n        self.memory_data.append(msg.data)\r\n\r\n    def gpu_callback(self, msg):\r\n        self.gpu_data.append(msg.data)\r\n\r\n    def fps_callback(self, msg):\r\n        self.fps_data.append(msg.data)\r\n\r\n    def latency_callback(self, msg):\r\n        self.latency_data.append(msg.data * 1000)  # Convert to milliseconds\r\n\r\n    def setup_visualization(self):\r\n        \"\"\"Setup the matplotlib visualization\"\"\"\r\n        plt.ion()  # Interactive mode\r\n        self.fig, self.axs = plt.subplots(2, 2, figsize=(12, 8))\r\n        self.fig.suptitle('Isaac Sim AI-Robot Performance Dashboard', fontsize=14)\r\n\r\n        # CPU usage plot\r\n        self.cpu_line, = self.axs[0, 0].plot([], [], 'b-', label='CPU %')\r\n        self.axs[0, 0].set_title('CPU Usage')\r\n        self.axs[0, 0].set_ylim(0, 100)\r\n        self.axs[0, 0].set_ylabel('Usage %')\r\n        self.axs[0, 0].grid(True)\r\n        self.axs[0, 0].legend()\r\n\r\n        # Memory usage plot\r\n        self.memory_line, = self.axs[0, 1].plot([], [], 'g-', label='Memory %')\r\n        self.axs[0, 1].set_title('Memory Usage')\r\n        self.axs[0, 1].set_ylim(0, 100)\r\n        self.axs[0, 1].set_ylabel('Usage %')\r\n        self.axs[0, 1].grid(True)\r\n        self.axs[0, 1].legend()\r\n\r\n        # FPS plot\r\n        self.fps_line, = self.axs[1, 0].plot([], [], 'r-', label='FPS')\r\n        self.axs[1, 0].set_title('Model FPS')\r\n        self.axs[1, 0].set_ylim(0, 60)\r\n        self.axs[1, 0].set_ylabel('Frames per Second')\r\n        self.axs[1, 0].grid(True)\r\n        self.axs[1, 0].legend()\r\n\r\n        # Latency plot\r\n        self.latency_line, = self.axs[1, 1].plot([], [], 'm-', label='Latency (ms)')\r\n        self.axs[1, 1].set_title('Model Latency')\r\n        self.axs[1, 1].set_ylim(0, 100)\r\n        self.axs[1, 1].set_ylabel('Latency (ms)')\r\n        self.axs[1, 1].grid(True)\r\n        self.axs[1, 1].legend()\r\n\r\n        # Set common x-axis\r\n        for ax in self.axs.flat:\r\n            ax.set_xlim(0, 100)\r\n\r\n        self.fig.tight_layout()\r\n\r\n    def update_plots(self, frame):\r\n        \"\"\"Update all plots with current data\"\"\"\r\n        x = range(len(self.cpu_data))\r\n\r\n        # Update CPU plot\r\n        if self.cpu_data:\r\n            self.cpu_line.set_data(x, list(self.cpu_data))\r\n            self.axs[0, 0].set_xlim(max(0, len(self.cpu_data)-100), max(100, len(self.cpu_data)))\r\n\r\n        # Update Memory plot\r\n        if self.memory_data:\r\n            self.memory_line.set_data(x, list(self.memory_data))\r\n            self.axs[0, 1].set_xlim(max(0, len(self.memory_data)-100), max(100, len(self.memory_data)))\r\n\r\n        # Update FPS plot\r\n        if self.fps_data:\r\n            self.fps_line.set_data(x, list(self.fps_data))\r\n            self.axs[1, 0].set_xlim(max(0, len(self.fps_data)-100), max(100, len(self.fps_data)))\r\n\r\n        # Update Latency plot\r\n        if self.latency_data:\r\n            self.latency_line.set_data(x, list(self.latency_data))\r\n            self.axs[1, 1].set_xlim(max(0, len(self.latency_data)-100), max(100, len(self.latency_data)))\r\n\r\n        return self.cpu_line, self.memory_line, self.fps_line, self.latency_line\r\n\r\n    def run_dashboard(self):\r\n        \"\"\"Run the dashboard visualization\"\"\"\r\n        try:\r\n            ani = FuncAnimation(self.fig, self.update_plots, interval=100, blit=False)\r\n            plt.show()\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in dashboard: {e}')\r\n\r\n    def get_system_health(self):\r\n        \"\"\"Calculate overall system health score\"\"\"\r\n        if not self.cpu_data or not self.memory_data or not self.fps_data:\r\n            return \"Unknown\"\r\n\r\n        avg_cpu = sum(self.cpu_data) / len(self.cpu_data)\r\n        avg_memory = sum(self.memory_data) / len(self.memory_data)\r\n        avg_fps = sum(self.fps_data) / len(self.fps_data) if self.fps_data else 0\r\n\r\n        # Health calculation (simplified)\r\n        cpu_health = max(0, 100 - avg_cpu)  # Lower CPU = better\r\n        memory_health = max(0, 100 - avg_memory)  # Lower memory = better\r\n        fps_health = min(100, avg_fps)  # Higher FPS = better\r\n\r\n        overall_health = (cpu_health + memory_health + fps_health) / 3\r\n\r\n        if overall_health > 80:\r\n            return f\"Excellent ({overall_health:.1f})\"\r\n        elif overall_health > 60:\r\n            return f\"Good ({overall_health:.1f})\"\r\n        elif overall_health > 40:\r\n            return f\"Fair ({overall_health:.1f})\"\r\n        else:\r\n            return f\"Needs Attention ({overall_health:.1f})\"\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    dashboard = PerformanceDashboard()\r\n\r\n    try:\r\n        rclpy.spin(dashboard)\r\n    except KeyboardInterrupt:\r\n        print(f\"System Health: {dashboard.get_system_health()}\")\r\n        plt.close('all')\r\n    finally:\r\n        dashboard.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(r.h2,{id:"performance-monitoring-launch-files",children:"Performance Monitoring Launch Files"}),"\n",(0,t.jsx)(r.h3,{id:"1-creating-performance-monitoring-package",children:"1. Creating Performance Monitoring Package"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cd ~/isaac_ros_ws/src\r\nsource /opt/ros/humble/setup.bash\r\n\r\n# Create performance monitoring package\r\nros2 pkg create --build-type ament_python isaac_performance_monitor --dependencies rclpy sensor_msgs geometry_msgs std_msgs message_filters tf2_ros tf2_geometry_msgs nav_msgs vision_msgs\n"})}),"\n",(0,t.jsx)(r.h3,{id:"2-creating-setup-files-for-performance-package",children:"2. Creating Setup Files for Performance Package"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_performance_monitor/setup.py << 'EOF\r\nfrom setuptools import setup\r\nfrom glob import glob\r\nimport os\r\n\r\npackage_name = 'isaac_performance_monitor'\r\n\r\nsetup(\r\n    name=package_name,\r\n    version='0.0.1',\r\n    packages=[package_name],\r\n    data_files=[\r\n        ('share/ament_index/resource_index/packages',\r\n            ['resource/' + package_name]),\r\n        ('share/' + package_name, ['package.xml']),\r\n        # Include launch files\r\n        (os.path.join('share', package_name, 'launch'), glob('launch/*.py')),\r\n    ],\r\n    install_requires=['setuptools'],\r\n    zip_safe=True,\r\n    maintainer='Your Name',\r\n    maintainer_email='your.email@example.com',\r\n    description='Performance monitoring tools for Isaac Sim AI-Robot systems',\r\n    license='Apache License 2.0',\r\n    tests_require=['pytest'],\r\n    entry_points={\r\n        'console_scripts': [\r\n            'system_monitor = isaac_performance_monitor.system_monitor:main',\r\n            'model_performance_monitor = isaac_performance_monitor.model_performance_monitor:main',\r\n            'navigation_monitor = isaac_performance_monitor.navigation_monitor:main',\r\n            'communication_monitor = isaac_performance_monitor.communication_monitor:main',\r\n        ],\r\n    },\r\n)\r\nEOF\n"})}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:'cat > ~/isaac_ros_ws/src/isaac_performance_monitor/package.xml << \'EOF\'\r\n<?xml version="1.0"?>\r\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\r\n<package format="3">\r\n  <name>isaac_performance_monitor</name>\r\n  <version>0.0.1</version>\r\n  <description>Performance monitoring tools for Isaac Sim AI-Robot systems</description>\r\n  <maintainer email="your.email@example.com">Your Name</maintainer>\r\n  <license>Apache License 2.0</license>\r\n\r\n  <depend>rclpy</depend>\r\n  <depend>sensor_msgs</depend>\r\n  <depend>geometry_msgs</depend>\r\n  <depend>std_msgs</depend>\r\n  <depend>message_filters</depend>\r\n  <depend>tf2_ros</depend>\r\n  <depend>tf2_geometry_msgs</depend>\r\n  <depend>nav_msgs</depend>\r\n  <depend>vision_msgs</depend>\r\n\r\n  <test_depend>ament_copyright</test_depend>\r\n  <test_depend>ament_flake8</test_depend>\r\n  <test_depend>ament_pep257</test_depend>\r\n  <test_depend>python3-pytest</test_depend>\r\n\r\n  <export>\r\n    <build_type>ament_python</build_type>\r\n  </export>\r\n</package>\r\nEOF\n'})}),"\n",(0,t.jsx)(r.h3,{id:"3-creating-performance-monitoring-launch-file",children:"3. Creating Performance Monitoring Launch File"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"mkdir -p ~/isaac_ros_ws/src/isaac_performance_monitor/launch\r\ncat > ~/isaac_ros_ws/src/isaac_performance_monitor/launch/performance_monitoring.launch.py << 'EOF\r\nimport os\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, TimerAction\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import Node\r\nfrom ament_index_python.packages import get_package_share_directory\r\n\r\n\r\ndef generate_launch_description():\r\n    # Launch arguments\r\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\r\n    enable_visualization = LaunchConfiguration('enable_visualization', default='false')\r\n\r\n    # System monitor node\r\n    system_monitor = Node(\r\n        package='isaac_performance_monitor',\r\n        executable='system_monitor',\r\n        name='system_monitor',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n\r\n    # Model performance monitor node\r\n    model_performance_monitor = Node(\r\n        package='isaac_performance_monitor',\r\n        executable='model_performance_monitor',\r\n        name='model_performance_monitor',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n\r\n    # Navigation monitor node\r\n    navigation_monitor = Node(\r\n        package='isaac_performance_monitor',\r\n        executable='navigation_monitor',\r\n        name='navigation_monitor',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n\r\n    # Communication monitor node\r\n    communication_monitor = Node(\r\n        package='isaac_performance_monitor',\r\n        executable='communication_monitor',\r\n        name='communication_monitor',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n\r\n    # Performance dashboard (optional)\r\n    performance_dashboard = Node(\r\n        package='isaac_performance_monitor',\r\n        executable='performance_dashboard',\r\n        name='performance_dashboard',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen',\r\n        condition=lambda context: LaunchConfiguration('enable_visualization').perform(context) == 'true'\r\n    )\r\n\r\n    # Return launch description\r\n    ld = LaunchDescription()\r\n\r\n    # Add launch arguments\r\n    ld.add_action(DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='true',\r\n        description='Use simulation (Isaac Sim) clock if true'))\r\n\r\n    ld.add_action(DeclareLaunchArgument(\r\n        'enable_visualization',\r\n        default_value='false',\r\n        description='Enable performance visualization dashboard'))\r\n\r\n    # Add nodes\r\n    ld.add_action(system_monitor)\r\n    ld.add_action(TimerAction(\r\n        period=1.0,\r\n        actions=[model_performance_monitor]\r\n    ))\r\n    ld.add_action(TimerAction(\r\n        period=2.0,\r\n        actions=[navigation_monitor]\r\n    ))\r\n    ld.add_action(TimerAction(\r\n        period=3.0,\r\n        actions=[communication_monitor]\r\n    ))\r\n    ld.add_action(TimerAction(\r\n        period=5.0,\r\n        actions=[performance_dashboard]\r\n    ))\r\n\r\n    return ld\r\nEOF\n"})}),"\n",(0,t.jsx)(r.h2,{id:"performance-logging-and-analysis",children:"Performance Logging and Analysis"}),"\n",(0,t.jsx)(r.h3,{id:"1-creating-performance-logger",children:"1. Creating Performance Logger"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cat > ~/isaac_sim_shared/scripts/performance_logger.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import Float32, String\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu\r\nimport json\r\nimport csv\r\nimport os\r\nfrom datetime import datetime\r\nfrom collections import defaultdict\r\nimport threading\r\n\r\nclass PerformanceLogger(Node):\r\n    def __init__(self):\r\n        super().__init__('performance_logger')\r\n\r\n        # Data storage\r\n        self.metrics = defaultdict(list)\r\n        self.start_time = datetime.now()\r\n\r\n        # Create logs directory\r\n        self.logs_dir = \"/workspace/shared_dir/logs\"\r\n        os.makedirs(self.logs_dir, exist_ok=True)\r\n\r\n        # Subscribe to performance metrics\r\n        self.cpu_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/cpu_usage',\r\n            lambda msg: self.log_metric('cpu_usage', msg.data),\r\n            10\r\n        )\r\n\r\n        self.memory_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/memory_usage',\r\n            lambda msg: self.log_metric('memory_usage', msg.data),\r\n            10\r\n        )\r\n\r\n        self.gpu_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/gpu_usage',\r\n            lambda msg: self.log_metric('gpu_usage', msg.data),\r\n            10\r\n        )\r\n\r\n        self.fps_subscription = self.create_subscription(\r\n            Float32,\r\n            '/model/fps',\r\n            lambda msg: self.log_metric('fps', msg.data),\r\n            10\r\n        )\r\n\r\n        self.latency_subscription = self.create_subscription(\r\n            Float32,\r\n            '/model/latency',\r\n            lambda msg: self.log_metric('latency_ms', msg.data * 1000),\r\n            10\r\n        )\r\n\r\n        # Setup timer for periodic logging\r\n        self.log_timer = self.create_timer(10.0, self.flush_logs)\r\n\r\n        self.get_logger().info('Performance Logger initialized')\r\n\r\n    def log_metric(self, metric_name, value):\r\n        \"\"\"Log a performance metric\"\"\"\r\n        timestamp = datetime.now().isoformat()\r\n        self.metrics[metric_name].append({\r\n            'timestamp': timestamp,\r\n            'value': value\r\n        })\r\n\r\n    def flush_logs(self):\r\n        \"\"\"Flush logs to file periodically\"\"\"\r\n        try:\r\n            # Create filename with timestamp\r\n            timestamp = self.start_time.strftime(\"%Y%m%d_%H%M%S\")\r\n            filename = f\"performance_log_{timestamp}.json\"\r\n            filepath = os.path.join(self.logs_dir, filename)\r\n\r\n            # Write metrics to JSON file\r\n            with open(filepath, 'w') as f:\r\n                json.dump(dict(self.metrics), f, indent=2)\r\n\r\n            self.get_logger().info(f'Performance logs flushed to {filepath}')\r\n\r\n            # Also create a summary CSV\r\n            self.create_summary_csv(timestamp)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error flushing logs: {e}')\r\n\r\n    def create_summary_csv(self, timestamp):\r\n        \"\"\"Create a summary CSV file\"\"\"\r\n        try:\r\n            filename = f\"performance_summary_{timestamp}.csv\"\r\n            filepath = os.path.join(self.logs_dir, filename)\r\n\r\n            with open(filepath, 'w', newline='') as csvfile:\r\n                fieldnames = ['metric', 'avg_value', 'min_value', 'max_value', 'sample_count']\r\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\r\n\r\n                writer.writeheader()\r\n\r\n                for metric_name, values in self.metrics.items():\r\n                    if values:\r\n                        metric_values = [v['value'] for v in values]\r\n                        avg_val = sum(metric_values) / len(metric_values)\r\n                        min_val = min(metric_values)\r\n                        max_val = max(metric_values)\r\n                        count = len(metric_values)\r\n\r\n                        writer.writerow({\r\n                            'metric': metric_name,\r\n                            'avg_value': avg_val,\r\n                            'min_value': min_val,\r\n                            'max_value': max_val,\r\n                            'sample_count': count\r\n                        })\r\n\r\n            self.get_logger().info(f'Performance summary created: {filepath}')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error creating summary CSV: {e}')\r\n\r\n    def get_performance_report(self):\r\n        \"\"\"Generate a performance report\"\"\"\r\n        report = {\r\n            'start_time': self.start_time.isoformat(),\r\n            'end_time': datetime.now().isoformat(),\r\n            'metrics_summary': {}\r\n        }\r\n\r\n        for metric_name, values in self.metrics.items():\r\n            if values:\r\n                metric_values = [v['value'] for v in values]\r\n                report['metrics_summary'][metric_name] = {\r\n                    'avg': sum(metric_values) / len(metric_values),\r\n                    'min': min(metric_values),\r\n                    'max': max(metric_values),\r\n                    'count': len(metric_values),\r\n                    'latest': metric_values[-1] if metric_values else None\r\n                }\r\n\r\n        return report\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    logger = PerformanceLogger()\r\n\r\n    try:\r\n        rclpy.spin(logger)\r\n    except KeyboardInterrupt:\r\n        report = logger.get_performance_report()\r\n        print(f\"Performance Report: {report}\")\r\n\r\n        # Flush final logs\r\n        logger.flush_logs()\r\n    finally:\r\n        logger.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(r.h2,{id:"performance-alerting-system",children:"Performance Alerting System"}),"\n",(0,t.jsx)(r.h3,{id:"1-creating-performance-alert-system",children:"1. Creating Performance Alert System"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cat > ~/isaac_sim_shared/scripts/performance_alerts.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import Float32, String\r\nimport time\r\nfrom collections import deque\r\n\r\nclass PerformanceAlerts(Node):\r\n    def __init__(self):\r\n        super().__init__('performance_alerts')\r\n\r\n        # Alert thresholds\r\n        self.thresholds = {\r\n            'cpu_usage': {'warning': 80, 'critical': 90},\r\n            'memory_usage': {'warning': 80, 'critical': 90},\r\n            'gpu_usage': {'warning': 85, 'critical': 95},\r\n            'fps': {'warning': 15, 'critical': 10},  # Low FPS is bad\r\n            'latency_ms': {'warning': 50, 'critical': 100}\r\n        }\r\n\r\n        # Alert state tracking\r\n        self.alert_states = {}\r\n        self.alert_history = deque(maxlen=100)\r\n\r\n        # Subscribe to metrics\r\n        self.cpu_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/cpu_usage',\r\n            lambda msg: self.check_threshold('cpu_usage', msg.data),\r\n            10\r\n        )\r\n\r\n        self.memory_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/memory_usage',\r\n            lambda msg: self.check_threshold('memory_usage', msg.data),\r\n            10\r\n        )\r\n\r\n        self.gpu_subscription = self.create_subscription(\r\n            Float32,\r\n            '/system/gpu_usage',\r\n            lambda msg: self.check_threshold('gpu_usage', msg.data),\r\n            10\r\n        )\r\n\r\n        self.fps_subscription = self.create_subscription(\r\n            Float32,\r\n            '/model/fps',\r\n            lambda msg: self.check_threshold('fps', msg.data),\r\n            10\r\n        )\r\n\r\n        self.latency_subscription = self.create_subscription(\r\n            Float32,\r\n            '/model/latency',\r\n            lambda msg: self.check_threshold('latency_ms', msg.data * 1000),\r\n            10\r\n        )\r\n\r\n        # Publisher for alerts\r\n        self.alert_publisher = self.create_publisher(String, '/system/alerts', 10)\r\n\r\n        self.get_logger().info('Performance Alert System initialized')\r\n\r\n    def check_threshold(self, metric_name, value):\r\n        \"\"\"Check if metric exceeds thresholds\"\"\"\r\n        try:\r\n            thresholds = self.thresholds[metric_name]\r\n\r\n            # Determine alert level\r\n            alert_level = None\r\n            if metric_name in ['fps']:  # Lower is worse\r\n                if value <= thresholds['critical']:\r\n                    alert_level = 'CRITICAL'\r\n                elif value <= thresholds['warning']:\r\n                    alert_level = 'WARNING'\r\n            else:  # Higher is worse\r\n                if value >= thresholds['critical']:\r\n                    alert_level = 'CRITICAL'\r\n                elif value >= thresholds['warning']:\r\n                    alert_level = 'WARNING'\r\n\r\n            # Publish alert if threshold exceeded\r\n            if alert_level:\r\n                alert_msg = String()\r\n                alert_msg.data = f\"{alert_level}: {metric_name} = {value:.2f} (threshold: {thresholds.get('critical' if alert_level == 'CRITICAL' else 'warning')})\"\r\n                self.alert_publisher.publish(alert_msg)\r\n\r\n                # Store in history\r\n                alert_entry = {\r\n                    'timestamp': time.time(),\r\n                    'metric': metric_name,\r\n                    'value': value,\r\n                    'threshold': thresholds.get('critical' if alert_level == 'CRITICAL' else 'warning'),\r\n                    'level': alert_level\r\n                }\r\n                self.alert_history.append(alert_entry)\r\n\r\n                self.get_logger().warn(f\"{alert_level} Alert: {alert_msg.data}\")\r\n            else:\r\n                # Clear alert state if below warning\r\n                if metric_name in self.alert_states:\r\n                    del self.alert_states[metric_name]\r\n\r\n        except KeyError:\r\n            self.get_logger().error(f'Unknown metric: {metric_name}')\r\n\r\n    def get_alert_summary(self):\r\n        \"\"\"Get summary of recent alerts\"\"\"\r\n        if not self.alert_history:\r\n            return \"No alerts triggered\"\r\n\r\n        recent_alerts = list(self.alert_history)[-10:]  # Last 10 alerts\r\n        summary = f\"Recent alerts: {len(recent_alerts)} total, \"\r\n        summary += f\"CRITICAL: {sum(1 for a in recent_alerts if a['level'] == 'CRITICAL')}, \"\r\n        summary += f\"WARNING: {sum(1 for a in recent_alerts if a['level'] == 'WARNING')}\"\r\n\r\n        return summary\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    alerts = PerformanceAlerts()\r\n\r\n    try:\r\n        rclpy.spin(alerts)\r\n    except KeyboardInterrupt:\r\n        summary = alerts.get_alert_summary()\r\n        print(f\"Alert Summary: {summary}\")\r\n    finally:\r\n        alerts.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,t.jsx)(r.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,t.jsx)(r.h3,{id:"1-creating-performance-monitoring-test-script",children:"1. Creating Performance Monitoring Test Script"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:'cat > ~/test_performance_monitoring.sh << \'EOF\'\r\n#!/bin/bash\r\n\r\n# Test script for performance monitoring tools\r\n\r\necho "Testing Performance Monitoring Tools..."\r\n\r\n# Source ROS environment\r\nsource /opt/ros/humble/setup.bash\r\nsource ~/isaac_ros_ws/install/setup.bash\r\n\r\n# Check if performance monitoring package is available\r\necho "Checking performance monitoring package..."\r\nros2 pkg list | grep performance_monitor\r\n\r\nif [ $? -eq 0 ]; then\r\n    echo "\u2713 Performance monitoring package found"\r\nelse\r\n    echo "\u2717 Performance monitoring package not found"\r\n    exit 1\r\nfi\r\n\r\n# Check for performance monitoring launch files\r\nif [ -f "/workspace/shared_dir/src/isaac_performance_monitor/launch/performance_monitoring.launch.py" ]; then\r\n    echo "\u2713 Performance monitoring launch file found"\r\nelse\r\n    echo "\u2717 Performance monitoring launch file not found"\r\n    exit 1\r\nfi\r\n\r\n# Build the performance monitoring package\r\necho "Building performance monitoring package..."\r\ncd ~/isaac_ros_ws\r\ncolcon build --packages-select isaac_performance_monitor\r\nsource install/setup.bash\r\n\r\n# Check if required Python packages are available\r\npython3 -c "import psutil; import GPUtil; import matplotlib; print(\'\u2713 Required Python packages available\')" || {\r\n    echo "\u2717 Required Python packages not available"\r\n    echo "Installing required packages..."\r\n    pip3 install psutil GPUtil matplotlib numpy\r\n}\r\n\r\n# Test performance logging directory\r\nif [ -d "/workspace/shared_dir/logs" ]; then\r\n    echo "\u2713 Logs directory exists"\r\nelse\r\n    mkdir -p /workspace/shared_dir/logs\r\n    echo "\u2713 Created logs directory"\r\nfi\r\n\r\necho "Performance Monitoring test completed."\r\necho "To run performance monitoring, use:"\r\necho "ros2 launch isaac_performance_monitor performance_monitoring.launch.py"\r\nEOF\r\n\r\n# Make executable\r\nchmod +x ~/test_performance_monitoring.sh\n'})}),"\n",(0,t.jsx)(r.h3,{id:"2-running-performance-monitoring-test",children:"2. Running Performance Monitoring Test"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"~/test_performance_monitoring.sh\n"})}),"\n",(0,t.jsx)(r.h2,{id:"troubleshooting-performance-monitoring",children:"Troubleshooting Performance Monitoring"}),"\n",(0,t.jsx)(r.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,t.jsx)(r.h4,{id:"issue-performance-monitoring-nodes-consuming-too-many-resources",children:'Issue: "Performance monitoring nodes consuming too many resources"'}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Solution"}),": Adjust monitoring frequency"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"# Check current monitoring frequency\r\nros2 topic hz /system/cpu_usage\r\nros2 topic hz /model/fps\r\n\r\n# Adjust timer intervals in the monitoring nodes\r\n# Modify the timer periods in the source code\n"})}),"\n",(0,t.jsx)(r.h4,{id:"issue-no-data-being-published-to-performance-topics",children:'Issue: "No data being published to performance topics"'}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Solution"}),": Verify that monitored systems are running"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:'# Check if monitored topics exist\r\nros2 topic list | grep -E "(cpu_usage|memory_usage|fps|latency)"\r\n\r\n# Verify Isaac Sim is publishing sensor data\r\nros2 topic echo /camera/color/image_raw --field height --field width\n'})}),"\n",(0,t.jsx)(r.h4,{id:"issue-python-packages-not-found-for-monitoring",children:'Issue: "Python packages not found for monitoring"'}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Solution"}),": Install required packages"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"pip3 install psutil GPUtil matplotlib numpy\n"})}),"\n",(0,t.jsx)(r.h2,{id:"verification-checklist",children:"Verification Checklist"}),"\n",(0,t.jsxs)(r.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","System resource monitoring node created"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","AI model performance monitoring implemented"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","Navigation performance monitoring created"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","Communication performance monitoring implemented"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","Performance dashboard visualization created"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","Performance logging system implemented"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","Performance alert system created"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","Launch files created for monitoring system"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","Test script created and functional"]}),"\n",(0,t.jsxs)(r.li,{className:"task-list-item",children:[(0,t.jsx)(r.input,{type:"checkbox",disabled:!0})," ","Troubleshooting guide reviewed"]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(r.p,{children:"After implementing performance monitoring:"}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Test monitoring system"})," with Isaac Sim running"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Analyze performance data"})," and identify bottlenecks"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Set up alerts"})," for critical performance thresholds"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Create performance assessment tools"})," for students"]}),"\n"]}),"\n",(0,t.jsx)(r.p,{children:"The performance monitoring framework is now configured and ready for Module 3, providing students with tools to monitor and optimize AI-robot system performance."})]})}function f(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>i,x:()=>o});var s=n(6540);const t={},a=s.createContext(t);function i(e){const r=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),s.createElement(a.Provider,{value:r},e.children)}}}]);