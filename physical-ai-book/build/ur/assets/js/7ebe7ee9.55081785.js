"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[242],{3290:(r,n,e)=>{e.r(n),e.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-3/sensor-integration","title":"Multimodal Sensor Integration in Isaac Sim","description":"Overview","source":"@site/docs/chapter-3/sensor-integration.md","sourceDirName":"chapter-3","slug":"/chapter-3/sensor-integration","permalink":"/ur/docs/chapter-3/sensor-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-3/sensor-integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS Perception Pipeline Implementation","permalink":"/ur/docs/chapter-3/perception-pipeline"},"next":{"title":"AI Model Deployment in Isaac Sim Environment","permalink":"/ur/docs/chapter-3/model-deployment"}}');var a=e(4848),i=e(8453);const t={},o="Multimodal Sensor Integration in Isaac Sim",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Introduction to Multimodal Sensor Integration",id:"introduction-to-multimodal-sensor-integration",level:2},{value:"What is Multimodal Sensor Integration?",id:"what-is-multimodal-sensor-integration",level:3},{value:"Key Sensor Types in Isaac Sim",id:"key-sensor-types-in-isaac-sim",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Isaac Sim Sensor Configuration",id:"isaac-sim-sensor-configuration",level:2},{value:"1. Creating Multimodal Robot Configuration",id:"1-creating-multimodal-robot-configuration",level:3},{value:"2. Creating Sensor Configuration File",id:"2-creating-sensor-configuration-file",level:3},{value:"Sensor Data Synchronization",id:"sensor-data-synchronization",level:2},{value:"1. Creating Sensor Synchronization Node",id:"1-creating-sensor-synchronization-node",level:3},{value:"Sensor Fusion Implementation",id:"sensor-fusion-implementation",level:2},{value:"1. Creating Sensor Fusion Package",id:"1-creating-sensor-fusion-package",level:3},{value:"2. Creating Advanced Sensor Fusion Node",id:"2-creating-advanced-sensor-fusion-node",level:3},{value:"TF (Transform) Configuration",id:"tf-transform-configuration",level:2},{value:"1. Creating Static Transform Publisher",id:"1-creating-static-transform-publisher",level:3},{value:"Sensor Integration Launch Files",id:"sensor-integration-launch-files",level:2},{value:"1. Creating Launch Directory and Files",id:"1-creating-launch-directory-and-files",level:3},{value:"2. Creating Sensor Integration Launch File",id:"2-creating-sensor-integration-launch-file",level:3},{value:"Sensor Calibration and Validation",id:"sensor-calibration-and-validation",level:2},{value:"1. Creating Sensor Calibration Tools",id:"1-creating-sensor-calibration-tools",level:3},{value:"Sensor Data Visualization",id:"sensor-data-visualization",level:2},{value:"1. Creating Sensor Data Visualization Script",id:"1-creating-sensor-data-visualization-script",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"1. Creating Sensor Integration Test Script",id:"1-creating-sensor-integration-test-script",level:3},{value:"2. Running Sensor Integration Test",id:"2-running-sensor-integration-test",level:3},{value:"Troubleshooting Sensor Integration",id:"troubleshooting-sensor-integration",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Issue: &quot;Sensor data not synchronizing properly&quot;",id:"issue-sensor-data-not-synchronizing-properly",level:4},{value:"Issue: &quot;TF transforms not available&quot;",id:"issue-tf-transforms-not-available",level:4},{value:"Issue: &quot;High CPU/GPU usage with multiple sensors&quot;",id:"issue-high-cpugpu-usage-with-multiple-sensors",level:4},{value:"Verification Checklist",id:"verification-checklist",level:2},{value:"Next Steps",id:"next-steps",level:2}];function f(r){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...r.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"multimodal-sensor-integration-in-isaac-sim",children:"Multimodal Sensor Integration in Isaac Sim"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This document provides comprehensive instructions for setting up multimodal sensor integration in the NVIDIA Isaac Sim environment for Module 3. The focus is on combining data from multiple sensors (camera, LiDAR, IMU, etc.) to create a comprehensive perception system for AI-robot applications."}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-multimodal-sensor-integration",children:"Introduction to Multimodal Sensor Integration"}),"\n",(0,a.jsx)(n.h3,{id:"what-is-multimodal-sensor-integration",children:"What is Multimodal Sensor Integration?"}),"\n",(0,a.jsx)(n.p,{children:"Multimodal sensor integration combines data from multiple sensors to create a more complete and accurate understanding of the environment. In the context of Isaac Sim and ROS 2, this involves:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Synchronizing data from different sensor types"}),"\n",(0,a.jsx)(n.li,{children:"Fusing sensor data for enhanced perception"}),"\n",(0,a.jsx)(n.li,{children:"Managing sensor calibration and transformations"}),"\n",(0,a.jsx)(n.li,{children:"Implementing sensor fusion algorithms"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"key-sensor-types-in-isaac-sim",children:"Key Sensor Types in Isaac Sim"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Camera"}),": Visual information for object detection and recognition"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LiDAR"}),": 3D spatial information for mapping and obstacle detection"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IMU"}),": Inertial measurements for orientation and motion"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth Camera"}),": Depth information for 3D scene understanding"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"GPS"}),": Position information for outdoor navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Encoders"}),": Wheel odometry for motion tracking"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.p,{children:"Before setting up multimodal sensor integration, ensure you have:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Completed Isaac Sim and Isaac ROS setup"}),"\n",(0,a.jsx)(n.li,{children:"Basic understanding of ROS 2 topics and messages"}),"\n",(0,a.jsx)(n.li,{children:"Familiarity with sensor message types (sensor_msgs)"}),"\n",(0,a.jsx)(n.li,{children:"TF (Transform) knowledge for coordinate frame management"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"isaac-sim-sensor-configuration",children:"Isaac Sim Sensor Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"1-creating-multimodal-robot-configuration",children:"1. Creating Multimodal Robot Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'cat > ~/isaac_sim_shared/robots/multimodal_robot.usd << \'EOF\'\r\n#usda 1.0\r\n(\r\n    doc = """Multimodal sensor robot configuration for Isaac Sim"""\r\n    metersPerUnit = 0.01\r\n    upAxis = "Y"\r\n)\r\n\r\ndef Xform "MultimodalRobot"\r\n{\r\n    def Xform "Chassis"\r\n    {\r\n        def Cube "Base"\r\n        {\r\n            float3 xformOp:translate = (0, 0, 0.1)\r\n            float3 xformOp:scale = (0.5, 0.3, 0.2)\r\n        }\r\n    }\r\n\r\n    def Xform "Wheels"\r\n    {\r\n        def Cylinder "FrontLeftWheel"\r\n        {\r\n            float3 xformOp:translate = (0.15, 0.15, 0)\r\n            float radius = 0.05\r\n            float height = 0.04\r\n            float3 xformOp:rotateXYZ = (90, 0, 0)\r\n        }\r\n\r\n        def Cylinder "FrontRightWheel"\r\n        {\r\n            float3 xformOp:translate = (0.15, -0.15, 0)\r\n            float radius = 0.05\r\n            float height = 0.04\r\n            float3 xformOp:rotateXYZ = (90, 0, 0)\r\n        }\r\n\r\n        def Cylinder "BackLeftWheel"\r\n        {\r\n            float3 xformOp:translate = (-0.15, 0.15, 0)\r\n            float radius = 0.05\r\n            float height = 0.04\r\n            float3 xformOp:rotateXYZ = (90, 0, 0)\r\n        }\r\n\r\n        def Cylinder "BackRightWheel"\r\n        {\r\n            float3 xformOp:translate = (-0.15, -0.15, 0)\r\n            float radius = 0.05\r\n            float height = 0.04\r\n            float3 xformOp:rotateXYZ = (90, 0, 0)\r\n        }\r\n    }\r\n\r\n    def Xform "Sensors"\r\n    {\r\n        # RGB Camera\r\n        def Camera "RGB_Camera"\r\n        {\r\n            float3 xformOp:translate = (0.2, 0, 0.15)\r\n            float3 clampingRange = (0.1, 100)\r\n            float focalLength = 24\r\n            float horizontalAperture = 36\r\n            float verticalAperture = 24\r\n        }\r\n\r\n        # Depth Camera\r\n        def Camera "Depth_Camera"\r\n        {\r\n            float3 xformOp:translate = (0.2, 0, 0.15)\r\n            float3 clampingRange = (0.1, 10)\r\n            float focalLength = 24\r\n        }\r\n\r\n        # 360-degree LiDAR\r\n        def RotatingLidar "360Lidar"\r\n        {\r\n            float3 xformOp:translate = (0.18, 0, 0.19)\r\n        }\r\n\r\n        # IMU\r\n        def Imu "IMU"\r\n        {\r\n            float3 xformOp:translate = (0, 0, 0.1)\r\n        }\r\n\r\n        # GPS (simulated)\r\n        def UsdGeomGps "GPS"\r\n        {\r\n            float3 xformOp:translate = (0, 0, 0.2)\r\n        }\r\n\r\n        # Wheel encoders (simulated)\r\n        def UsdGeomWheelEncoder "WheelEncoders"\r\n        {\r\n            float3 xformOp:translate = (0, 0, 0.05)\r\n        }\r\n    }\r\n\r\n    def Xform "Links"\r\n    {\r\n        # Define coordinate frames for TF tree\r\n        def Xform "base_link"\r\n        {\r\n            # Robot base coordinate frame\r\n        }\r\n\r\n        def Xform "camera_link"\r\n        {\r\n            float3 xformOp:translate = (0.2, 0, 0.15)\r\n        }\r\n\r\n        def Xform "lidar_link"\r\n        {\r\n            float3 xformOp:translate = (0.18, 0, 0.19)\r\n        }\r\n\r\n        def Xform "imu_link"\r\n        {\r\n            float3 xformOp:translate = (0, 0, 0.1)\r\n        }\r\n\r\n        def Xform "gps_link"\r\n        {\r\n            float3 xformOp:translate = (0, 0, 0.2)\r\n        }\r\n\r\n        def Xform "odom"\r\n        {\r\n            # Odometry frame\r\n        }\r\n\r\n        def Xform "map"\r\n        {\r\n            # Map frame\r\n        }\r\n    }\r\n}\r\nEOF\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-creating-sensor-configuration-file",children:"2. Creating Sensor Configuration File"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'cat > ~/isaac_sim_shared/configs/sensor_config.yaml << \'EOF\'\r\n# Multimodal Sensor Configuration for Isaac Sim\r\n\r\n# Camera Configuration\r\ncamera:\r\n  rgb:\r\n    topic: "/camera/color/image_raw"\r\n    info_topic: "/camera/color/camera_info"\r\n    width: 640\r\n    height: 480\r\n    fps: 30\r\n    fov: 60\r\n    frame_id: "camera_link"\r\n    enable_distortion: false\r\n    color_correct: true\r\n  depth:\r\n    topic: "/camera/depth/image_rect_raw"\r\n    info_topic: "/camera/depth/camera_info"\r\n    width: 640\r\n    height: 480\r\n    fps: 30\r\n    fov: 60\r\n    frame_id: "camera_link"\r\n    enable_noise: true\r\n    noise_params:\r\n      mean: 0.0\r\n      stddev: 0.001\r\n\r\n# LiDAR Configuration\r\nlidar:\r\n  topic: "/scan"\r\n  samples: 360\r\n  range_min: 0.1\r\n  range_max: 25.0\r\n  fov: 360\r\n  fps: 10\r\n  frame_id: "lidar_link"\r\n  enable_noise: true\r\n  noise_params:\r\n    mean: 0.0\r\n    stddev: 0.01\r\n\r\n# IMU Configuration\r\nimu:\r\n  topic: "/imu"\r\n  frame_id: "imu_link"\r\n  rate: 100\r\n  enable_noise: true\r\n  noise_params:\r\n    accelerometer_noise_density: 0.001\r\n    gyroscope_noise_density: 0.0001\r\n\r\n# GPS Configuration\r\ngps:\r\n  topic: "/gps/fix"\r\n  frame_id: "gps_link"\r\n  rate: 1\r\n  enable_noise: true\r\n  noise_params:\r\n    horizontal_accuracy: 2.0\r\n    vertical_accuracy: 4.0\r\n\r\n# Wheel Encoder Configuration\r\nwheel_encoders:\r\n  left_topic: "/left_wheel/encoder"\r\n  right_topic: "/right_wheel/encoder"\r\n  frame_id: "base_link"\r\n  rate: 50\r\n  ticks_per_revolution: 1000\r\n  wheel_radius: 0.05\r\n  wheel_separation: 0.3\r\n\r\n# TF Configuration\r\ntf:\r\n  publish_tf: true\r\n  tf_prefix: ""\r\n  odom_frame: "odom"\r\n  base_frame: "base_link"\r\n  map_frame: "map"\r\n  publish_frequency: 50.0\r\n\r\n# Sensor Synchronization\r\nsynchronization:\r\n  enable_sync: true\r\n  sync_method: "approximate_time"  # or "exact_time"\r\n  sync_queue_size: 10\r\n  max_interval_seconds: 0.1\r\nEOF\n'})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-data-synchronization",children:"Sensor Data Synchronization"}),"\n",(0,a.jsx)(n.h3,{id:"1-creating-sensor-synchronization-node",children:"1. Creating Sensor Synchronization Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_sensor_fusion/sensor_fusion/sensor_synchronizer.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu, NavSatFix\r\nfrom geometry_msgs.msg import Twist\r\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\r\nfrom tf2_ros import TransformListener, Buffer\r\nimport numpy as np\r\nfrom scipy.spatial.transform import Rotation as R\r\nimport threading\r\n\r\nclass SensorSynchronizer(Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_synchronizer')\r\n\r\n        # Initialize data storage\r\n        self.camera_data = None\r\n        self.lidar_data = None\r\n        self.imu_data = None\r\n        self.gps_data = None\r\n\r\n        # Initialize TF buffer\r\n        self.tf_buffer = Buffer()\r\n        self.tf_listener = TransformListener(self.tf_buffer, self)\r\n\r\n        # Create subscribers for each sensor\r\n        self.camera_sub = Subscriber(self, Image, '/camera/color/image_raw')\r\n        self.lidar_sub = Subscriber(self, LaserScan, '/scan')\r\n        self.imu_sub = Subscriber(self, Imu, '/imu')\r\n        self.gps_sub = Subscriber(self, NavSatFix, '/gps/fix')\r\n\r\n        # Synchronize sensor data with approximate time synchronization\r\n        self.sync = ApproximateTimeSynchronizer(\r\n            [self.camera_sub, self.lidar_sub, self.imu_sub, self.gps_sub],\r\n            queue_size=10,\r\n            slop=0.1  # 100ms tolerance\r\n        )\r\n        self.sync.registerCallback(self.synchronized_callback)\r\n\r\n        # Publisher for fused sensor data\r\n        self.fused_publisher = self.create_publisher(\r\n            Twist,  # Using Twist as a placeholder; in practice, you'd create a custom message\r\n            '/fused_sensor_data',\r\n            10\r\n        )\r\n\r\n        # Publisher for sensor status\r\n        self.status_publisher = self.create_publisher(\r\n            Twist,  # Using Twist as a placeholder\r\n            '/sensor_status',\r\n            10\r\n        )\r\n\r\n        # Sensor status tracking\r\n        self.sensor_status = {\r\n            'camera': False,\r\n            'lidar': False,\r\n            'imu': False,\r\n            'gps': False\r\n        }\r\n\r\n        # Status timer\r\n        self.status_timer = self.create_timer(1.0, self.publish_status)\r\n\r\n        self.get_logger().info('Sensor Synchronizer initialized')\r\n\r\n    def synchronized_callback(self, camera_msg, lidar_msg, imu_msg, gps_msg):\r\n        \"\"\"Callback for synchronized sensor data\"\"\"\r\n        try:\r\n            # Update sensor status\r\n            self.sensor_status['camera'] = True\r\n            self.sensor_status['lidar'] = True\r\n            self.sensor_status['imu'] = True\r\n            self.sensor_status['gps'] = True\r\n\r\n            # Process synchronized data\r\n            self.camera_data = camera_msg\r\n            self.lidar_data = lidar_msg\r\n            self.imu_data = imu_msg\r\n            self.gps_data = gps_msg\r\n\r\n            # Perform basic fusion (placeholder implementation)\r\n            fused_data = self.perform_basic_fusion()\r\n\r\n            # Publish fused data\r\n            self.fused_publisher.publish(fused_data)\r\n\r\n            self.get_logger().info(f'Synchronized data: Camera {camera_msg.header.stamp.sec}, '\r\n                                 f'Lidar {lidar_msg.header.stamp.sec}, '\r\n                                 f'IMU {imu_msg.header.stamp.sec}, '\r\n                                 f'GPS {gps_msg.header.stamp.sec}')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in synchronized callback: {e}')\r\n\r\n    def perform_basic_fusion(self):\r\n        \"\"\"Perform basic sensor fusion\"\"\"\r\n        # This is a placeholder implementation\r\n        # In practice, you would implement more sophisticated fusion algorithms\r\n        fused_msg = Twist()\r\n\r\n        # Example: combine IMU orientation with GPS position for pose estimation\r\n        if self.imu_data and self.gps_data:\r\n            # Extract orientation from IMU\r\n            imu_orientation = [\r\n                self.imu_data.orientation.x,\r\n                self.imu_data.orientation.y,\r\n                self.imu_data.orientation.z,\r\n                self.imu_data.orientation.w\r\n            ]\r\n\r\n            # Extract position from GPS\r\n            gps_position = [\r\n                self.gps_data.latitude,\r\n                self.gps_data.longitude,\r\n                self.gps_data.altitude\r\n            ]\r\n\r\n            # Combine in a meaningful way (simplified example)\r\n            fused_msg.angular.x = imu_orientation[0]\r\n            fused_msg.angular.y = imu_orientation[1]\r\n            fused_msg.angular.z = imu_orientation[2]\r\n            fused_msg.linear.x = gps_position[0]\r\n            fused_msg.linear.y = gps_position[1]\r\n            fused_msg.linear.z = gps_position[2]\r\n\r\n        return fused_msg\r\n\r\n    def publish_status(self):\r\n        \"\"\"Publish sensor status\"\"\"\r\n        status_msg = Twist()\r\n        status_msg.linear.x = 1.0 if self.sensor_status['camera'] else 0.0\r\n        status_msg.linear.y = 1.0 if self.sensor_status['lidar'] else 0.0\r\n        status_msg.linear.z = 1.0 if self.sensor_status['imu'] else 0.0\r\n        status_msg.angular.x = 1.0 if self.sensor_status['gps'] else 0.0\r\n\r\n        self.status_publisher.publish(status_msg)\r\n\r\n        # Reset status for next cycle\r\n        self.sensor_status = {k: False for k in self.sensor_status.keys()}\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    synchronizer = SensorSynchronizer()\r\n\r\n    try:\r\n        rclpy.spin(synchronizer)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        synchronizer.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-implementation",children:"Sensor Fusion Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"1-creating-sensor-fusion-package",children:"1. Creating Sensor Fusion Package"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ros_ws/src\r\nsource /opt/ros/humble/setup.bash\r\n\r\n# Create sensor fusion package\r\nros2 pkg create --build-type ament_python isaac_sensor_fusion --dependencies rclpy sensor_msgs geometry_msgs message_filters tf2_ros tf2_geometry_msgs std_msgs\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-creating-advanced-sensor-fusion-node",children:"2. Creating Advanced Sensor Fusion Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'cat > ~/isaac_ros_ws/src/isaac_sensor_fusion/sensor_fusion/advanced_sensor_fusion.py << \'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu, NavSatFix\r\nfrom geometry_msgs.msg import PoseWithCovarianceStamped, Twist\r\nfrom nav_msgs.msg import Odometry\r\nfrom tf2_ros import TransformListener, Buffer\r\nfrom tf2_geometry_msgs import do_transform_pose\r\nimport numpy as np\r\nfrom scipy.spatial.transform import Rotation as R\r\nfrom collections import deque\r\nimport threading\r\nfrom typing import Dict, List, Tuple\r\n\r\nclass AdvancedSensorFusion(Node):\r\n    def __init__(self):\r\n        super().__init__(\'advanced_sensor_fusion\')\r\n\r\n        # Initialize data buffers\r\n        self.camera_buffer = deque(maxlen=10)\r\n        self.lidar_buffer = deque(maxlen=10)\r\n        self.imu_buffer = deque(maxlen=50)  # Higher frequency\r\n        self.gps_buffer = deque(maxlen=10)\r\n\r\n        # Initialize TF buffer\r\n        self.tf_buffer = Buffer()\r\n        self.tf_listener = TransformListener(self.tf_buffer, self)\r\n\r\n        # Create subscribers\r\n        self.camera_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/color/image_raw\',\r\n            self.camera_callback,\r\n            10\r\n        )\r\n\r\n        self.lidar_subscription = self.create_subscription(\r\n            LaserScan,\r\n            \'/scan\',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n\r\n        self.imu_subscription = self.create_subscription(\r\n            Imu,\r\n            \'/imu\',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        self.gps_subscription = self.create_subscription(\r\n            NavSatFix,\r\n            \'/gps/fix\',\r\n            self.gps_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.fused_pose_publisher = self.create_publisher(\r\n            PoseWithCovarianceStamped,\r\n            \'/fused_pose\',\r\n            10\r\n        )\r\n\r\n        self.odom_publisher = self.create_publisher(\r\n            Odometry,\r\n            \'/fused_odom\',\r\n            10\r\n        )\r\n\r\n        # Initialize state estimation\r\n        self.position = np.array([0.0, 0.0, 0.0])\r\n        self.orientation = R.from_quat([0, 0, 0, 1])  # Identity quaternion\r\n        self.velocity = np.array([0.0, 0.0, 0.0])\r\n        self.angular_velocity = np.array([0.0, 0.0, 0.0])\r\n\r\n        # Covariance matrices (placeholder values)\r\n        self.position_covariance = np.eye(3) * 0.1\r\n        self.orientation_covariance = np.eye(3) * 0.05\r\n\r\n        # Update timer\r\n        self.update_timer = self.create_timer(0.05, self.update_fusion)  # 20 Hz\r\n\r\n        self.get_logger().info(\'Advanced Sensor Fusion node initialized\')\r\n\r\n    def camera_callback(self, msg):\r\n        """Handle camera data"""\r\n        self.camera_buffer.append(msg)\r\n\r\n    def lidar_callback(self, msg):\r\n        """Handle LiDAR data"""\r\n        self.lidar_buffer.append(msg)\r\n\r\n    def imu_callback(self, msg):\r\n        """Handle IMU data"""\r\n        self.imu_buffer.append(msg)\r\n\r\n    def gps_callback(self, msg):\r\n        """Handle GPS data"""\r\n        self.gps_buffer.append(msg)\r\n\r\n    def update_fusion(self):\r\n        """Update fused state estimate"""\r\n        try:\r\n            # Get latest sensor data\r\n            latest_imu = self.imu_buffer[-1] if self.imu_buffer else None\r\n            latest_gps = self.gps_buffer[-1] if self.gps_buffer else None\r\n\r\n            if latest_imu:\r\n                # Update orientation from IMU\r\n                imu_quat = np.array([\r\n                    latest_imu.orientation.x,\r\n                    latest_imu.orientation.y,\r\n                    latest_imu.orientation.z,\r\n                    latest_imu.orientation.w\r\n                ])\r\n\r\n                # Normalize quaternion\r\n                imu_quat = imu_quat / np.linalg.norm(imu_quat)\r\n\r\n                # Update orientation (simple approach, in practice use more sophisticated fusion)\r\n                self.orientation = R.from_quat(imu_quat)\r\n\r\n            if latest_gps:\r\n                # Convert GPS to local coordinates (simplified)\r\n                # In practice, you\'d use a proper coordinate transformation\r\n                lat_diff = latest_gps.latitude - 0.0  # Reference latitude\r\n                lon_diff = latest_gps.longitude - 0.0  # Reference longitude\r\n\r\n                # Convert to meters (approximate)\r\n                lat_to_meters = 111320.0  # meters per degree at equator\r\n                x = lon_diff * lat_to_meters * np.cos(np.radians(latest_gps.latitude))\r\n                y = lat_diff * lat_to_meters\r\n                z = latest_gps.altitude\r\n\r\n                # Update position\r\n                self.position = np.array([x, y, z])\r\n\r\n            # Create and publish fused pose\r\n            self.publish_fused_pose()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error in fusion update: {e}\')\r\n\r\n    def publish_fused_pose(self):\r\n        """Publish the fused pose estimate"""\r\n        try:\r\n            # Create PoseWithCovarianceStamped message\r\n            pose_msg = PoseWithCovarianceStamped()\r\n            pose_msg.header.stamp = self.get_clock().now().to_msg()\r\n            pose_msg.header.frame_id = "map"\r\n\r\n            # Set position\r\n            pose_msg.pose.pose.position.x = float(self.position[0])\r\n            pose_msg.pose.pose.position.y = float(self.position[1])\r\n            pose_msg.pose.pose.position.z = float(self.position[2])\r\n\r\n            # Set orientation\r\n            quat = self.orientation.as_quat()\r\n            pose_msg.pose.pose.orientation.x = float(quat[0])\r\n            pose_msg.pose.pose.orientation.y = float(quat[1])\r\n            pose_msg.pose.pose.orientation.z = float(quat[2])\r\n            pose_msg.pose.pose.orientation.w = float(quat[3])\r\n\r\n            # Set covariance\r\n            covariance = np.zeros(36)\r\n            covariance[0] = self.position_covariance[0, 0]  # x\r\n            covariance[7] = self.position_covariance[1, 1]  # y\r\n            covariance[14] = self.position_covariance[2, 2]  # z\r\n            covariance[21] = self.orientation_covariance[0, 0]  # rx\r\n            covariance[28] = self.orientation_covariance[1, 1]  # ry\r\n            covariance[35] = self.orientation_covariance[2, 2]  # rz\r\n\r\n            pose_msg.pose.covariance = covariance.tolist()\r\n\r\n            self.fused_pose_publisher.publish(pose_msg)\r\n\r\n            # Also publish as odometry\r\n            odom_msg = Odometry()\r\n            odom_msg.header = pose_msg.header\r\n            odom_msg.child_frame_id = "base_link"\r\n            odom_msg.pose = pose_msg.pose\r\n\r\n            self.odom_publisher.publish(odom_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error publishing fused pose: {e}\')\r\n\r\n    def get_fusion_status(self) -> Dict:\r\n        """Get status of sensor fusion"""\r\n        return {\r\n            \'camera_buffer_size\': len(self.camera_buffer),\r\n            \'lidar_buffer_size\': len(self.lidar_buffer),\r\n            \'imu_buffer_size\': len(self.imu_buffer),\r\n            \'gps_buffer_size\': len(self.gps_buffer),\r\n            \'position\': self.position.tolist(),\r\n            \'orientation\': self.orientation.as_quat().tolist()\r\n        }\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    fusion_node = AdvancedSensorFusion()\r\n\r\n    try:\r\n        rclpy.spin(fusion_node)\r\n    except KeyboardInterrupt:\r\n        status = fusion_node.get_fusion_status()\r\n        print(f"Final fusion status: {status}")\r\n    finally:\r\n        fusion_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\nEOF\n'})}),"\n",(0,a.jsx)(n.h2,{id:"tf-transform-configuration",children:"TF (Transform) Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"1-creating-static-transform-publisher",children:"1. Creating Static Transform Publisher"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_sensor_fusion/sensor_fusion/static_transform_publisher.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom tf2_ros import StaticTransformBroadcaster\r\nfrom geometry_msgs.msg import TransformStamped\r\nimport numpy as np\r\n\r\nclass StaticTransformPublisher(Node):\r\n    def __init__(self):\r\n        super().__init__('static_transform_publisher')\r\n\r\n        # Create static transform broadcaster\r\n        self.tf_static_broadcaster = StaticTransformBroadcaster(self)\r\n\r\n        # Publish static transforms\r\n        self.publish_static_transforms()\r\n\r\n        self.get_logger().info('Static Transform Publisher initialized')\r\n\r\n    def publish_static_transforms(self):\r\n        \"\"\"Publish static transforms between sensor frames\"\"\"\r\n        transforms = []\r\n\r\n        # Transform from base_link to camera_link\r\n        t = TransformStamped()\r\n        t.header.stamp = self.get_clock().now().to_msg()\r\n        t.header.frame_id = 'base_link'\r\n        t.child_frame_id = 'camera_link'\r\n        t.transform.translation.x = 0.2\r\n        t.transform.translation.y = 0.0\r\n        t.transform.translation.z = 0.15\r\n        t.transform.rotation.x = 0.0\r\n        t.transform.rotation.y = 0.0\r\n        t.transform.rotation.z = 0.0\r\n        t.transform.rotation.w = 1.0\r\n        transforms.append(t)\r\n\r\n        # Transform from base_link to lidar_link\r\n        t = TransformStamped()\r\n        t.header.stamp = self.get_clock().now().to_msg()\r\n        t.header.frame_id = 'base_link'\r\n        t.child_frame_id = 'lidar_link'\r\n        t.transform.translation.x = 0.18\r\n        t.transform.translation.y = 0.0\r\n        t.transform.translation.z = 0.19\r\n        t.transform.rotation.x = 0.0\r\n        t.transform.rotation.y = 0.0\r\n        t.transform.rotation.z = 0.0\r\n        t.transform.rotation.w = 1.0\r\n        transforms.append(t)\r\n\r\n        # Transform from base_link to imu_link\r\n        t = TransformStamped()\r\n        t.header.stamp = self.get_clock().now().to_msg()\r\n        t.header.frame_id = 'base_link'\r\n        t.child_frame_id = 'imu_link'\r\n        t.transform.translation.x = 0.0\r\n        t.transform.translation.y = 0.0\r\n        t.transform.translation.z = 0.1\r\n        t.transform.rotation.x = 0.0\r\n        t.transform.rotation.y = 0.0\r\n        t.transform.rotation.z = 0.0\r\n        t.transform.rotation.w = 1.0\r\n        transforms.append(t)\r\n\r\n        # Transform from base_link to gps_link\r\n        t = TransformStamped()\r\n        t.header.stamp = self.get_clock().now().to_msg()\r\n        t.header.frame_id = 'base_link'\r\n        t.child_frame_id = 'gps_link'\r\n        t.transform.translation.x = 0.0\r\n        t.transform.translation.y = 0.0\r\n        t.transform.translation.z = 0.2\r\n        t.transform.rotation.x = 0.0\r\n        t.transform.rotation.y = 0.0\r\n        t.transform.rotation.z = 0.0\r\n        t.transform.rotation.w = 1.0\r\n        transforms.append(t)\r\n\r\n        # Send all transforms\r\n        for transform in transforms:\r\n            self.tf_static_broadcaster.sendTransform(transform)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    publisher = StaticTransformPublisher()\r\n\r\n    try:\r\n        # Keep the node alive\r\n        rclpy.spin(publisher)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        publisher.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-integration-launch-files",children:"Sensor Integration Launch Files"}),"\n",(0,a.jsx)(n.h3,{id:"1-creating-launch-directory-and-files",children:"1. Creating Launch Directory and Files"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/isaac_ros_ws/src/isaac_sensor_fusion/launch\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-creating-sensor-integration-launch-file",children:"2. Creating Sensor Integration Launch File"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_ros_ws/src/isaac_sensor_fusion/launch/sensor_integration.launch.py << 'EOF\r\nimport os\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, TimerAction\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import Node\r\nfrom ament_index_python.packages import get_package_share_directory\r\n\r\n\r\ndef generate_launch_description():\r\n    # Launch arguments\r\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\r\n\r\n    # Static transform publisher node\r\n    static_transform_publisher = Node(\r\n        package='isaac_sensor_fusion',\r\n        executable='static_transform_publisher',\r\n        name='static_transform_publisher',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n\r\n    # Sensor synchronizer node\r\n    sensor_synchronizer = Node(\r\n        package='isaac_sensor_fusion',\r\n        executable='sensor_synchronizer',\r\n        name='sensor_synchronizer',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n\r\n    # Advanced sensor fusion node\r\n    advanced_fusion = Node(\r\n        package='isaac_sensor_fusion',\r\n        executable='advanced_sensor_fusion',\r\n        name='advanced_sensor_fusion',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n\r\n    # Return launch description\r\n    ld = LaunchDescription()\r\n\r\n    # Add launch arguments\r\n    ld.add_action(DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='true',\r\n        description='Use simulation (Isaac Sim) clock if true'))\r\n\r\n    # Add nodes\r\n    ld.add_action(static_transform_publisher)\r\n    ld.add_action(TimerAction(\r\n        period=2.0,\r\n        actions=[sensor_synchronizer]\r\n    ))\r\n    ld.add_action(TimerAction(\r\n        period=3.0,\r\n        actions=[advanced_fusion]\r\n    ))\r\n\r\n    return ld\r\nEOF\n"})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-calibration-and-validation",children:"Sensor Calibration and Validation"}),"\n",(0,a.jsx)(n.h3,{id:"1-creating-sensor-calibration-tools",children:"1. Creating Sensor Calibration Tools"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_sim_shared/scripts/sensor_calibration.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu, CameraInfo\r\nfrom geometry_msgs.msg import PointStamped\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nfrom scipy.spatial.transform import Rotation as R\r\nimport yaml\r\n\r\nclass SensorCalibrator(Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_calibrator')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to sensors\r\n        self.camera_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.camera_callback,\r\n            10\r\n        )\r\n\r\n        self.camera_info_subscription = self.create_subscription(\r\n            CameraInfo,\r\n            '/camera/color/camera_info',\r\n            self.camera_info_callback,\r\n            10\r\n        )\r\n\r\n        self.lidar_subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n\r\n        self.imu_subscription = self.create_subscription(\r\n            Imu,\r\n            '/imu',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        # Data storage for calibration\r\n        self.camera_info = None\r\n        self.calibration_data = []\r\n        self.calibration_samples = 0\r\n        self.max_samples = 100\r\n\r\n        self.get_logger().info('Sensor Calibrator initialized')\r\n\r\n    def camera_info_callback(self, msg):\r\n        \"\"\"Store camera info for calibration\"\"\"\r\n        self.camera_info = msg\r\n\r\n    def camera_callback(self, msg):\r\n        \"\"\"Process camera data for calibration\"\"\"\r\n        try:\r\n            # Convert to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n\r\n            # For calibration, we typically look for calibration patterns\r\n            # In this example, we'll look for checkerboard patterns\r\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\r\n\r\n            # Look for checkerboard pattern (9x6 corners)\r\n            ret, corners = cv2.findChessboardCorners(\r\n                gray, (9, 6),\r\n                cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE\r\n            )\r\n\r\n            if ret and self.calibration_samples < self.max_samples:\r\n                # Refine corner positions\r\n                corners_refined = cv2.cornerSubPix(\r\n                    gray, corners, (11, 11), (-1, -1),\r\n                    criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\r\n                )\r\n\r\n                # Store calibration data\r\n                self.calibration_data.append({\r\n                    'image': cv_image,\r\n                    'corners': corners_refined,\r\n                    'timestamp': msg.header.stamp\r\n                })\r\n\r\n                self.calibration_samples += 1\r\n                self.get_logger().info(f'Calibration sample {self.calibration_samples}/{self.max_samples}')\r\n\r\n                if self.calibration_samples >= self.max_samples:\r\n                    self.perform_calibration()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in camera callback: {e}')\r\n\r\n    def lidar_callback(self, msg):\r\n        \"\"\"Process LiDAR data\"\"\"\r\n        # Store LiDAR data for fusion validation\r\n        pass\r\n\r\n    def imu_callback(self, msg):\r\n        \"\"\"Process IMU data\"\"\"\r\n        # Store IMU data for fusion validation\r\n        pass\r\n\r\n    def perform_calibration(self):\r\n        \"\"\"Perform camera calibration\"\"\"\r\n        if len(self.calibration_data) < 10:\r\n            self.get_logger().warn('Not enough calibration samples collected')\r\n            return\r\n\r\n        # Prepare object points (3D points in real world space)\r\n        objp = np.zeros((9*6, 3), np.float32)\r\n        objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)\r\n\r\n        # Arrays to store object points and image points from all images\r\n        objpoints = []  # 3D points in real world space\r\n        imgpoints = []  # 2D points in image plane\r\n\r\n        for data in self.calibration_data:\r\n            # Only use data where corners were detected\r\n            if data['corners'] is not None:\r\n                objpoints.append(objp)\r\n                imgpoints.append(data['corners'])\r\n\r\n        if len(objpoints) < 10:\r\n            self.get_logger().warn('Not enough valid calibration samples')\r\n            return\r\n\r\n        # Perform calibration\r\n        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\r\n            objpoints, imgpoints,\r\n            (self.camera_info.width, self.camera_info.height),\r\n            None, None\r\n        )\r\n\r\n        if ret:\r\n            # Save calibration results\r\n            calibration_data = {\r\n                'camera_matrix': mtx.tolist(),\r\n                'distortion_coefficients': dist.tolist(),\r\n                'image_width': self.camera_info.width,\r\n                'image_height': self.camera_info.height,\r\n                'samples_used': len(objpoints)\r\n            }\r\n\r\n            # Save to file\r\n            with open('/workspace/shared_dir/calibration/camera_calibration.yaml', 'w') as f:\r\n                yaml.dump(calibration_data, f, default_flow_style=False)\r\n\r\n            self.get_logger().info(f'Camera calibration completed. Reprojection error: {ret}')\r\n            self.get_logger().info(f'Calibration saved to /workspace/shared_dir/calibration/camera_calibration.yaml')\r\n        else:\r\n            self.get_logger().error('Camera calibration failed')\r\n\r\n    def validate_sensor_fusion(self):\r\n        \"\"\"Validate sensor fusion quality\"\"\"\r\n        # This would implement validation of sensor fusion accuracy\r\n        # by comparing fused estimates to ground truth in simulation\r\n        pass\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    calibrator = SensorCalibrator()\r\n\r\n    try:\r\n        rclpy.spin(calibrator)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        calibrator.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-data-visualization",children:"Sensor Data Visualization"}),"\n",(0,a.jsx)(n.h3,{id:"1-creating-sensor-data-visualization-script",children:"1. Creating Sensor Data Visualization Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cat > ~/isaac_sim_shared/scripts/sensor_visualization.py << 'EOF\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu, PointCloud2\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.animation import FuncAnimation\r\nimport threading\r\nimport time\r\n\r\nclass SensorVisualizer(Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_visualizer')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Data storage for visualization\r\n        self.camera_data = None\r\n        self.lidar_data = None\r\n        self.imu_data = None\r\n        self.fused_pose = None\r\n\r\n        # Subscribe to sensors\r\n        self.camera_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.camera_callback,\r\n            10\r\n        )\r\n\r\n        self.lidar_subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n\r\n        self.imu_subscription = self.create_subscription(\r\n            Imu,\r\n            '/imu',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        self.pose_subscription = self.create_subscription(\r\n            PoseStamped,\r\n            '/fused_pose',\r\n            self.pose_callback,\r\n            10\r\n        )\r\n\r\n        # Visualization setup\r\n        self.fig, self.axs = plt.subplots(2, 2, figsize=(12, 10))\r\n        self.lidar_line, = self.axs[0, 0].plot([], [], 'b.')\r\n        self.axs[0, 0].set_title('LiDAR Scan')\r\n        self.axs[0, 0].set_xlim(-10, 10)\r\n        self.axs[0, 0].set_ylim(-10, 10)\r\n        self.axs[0, 0].grid(True)\r\n\r\n        self.imu_ax = self.axs[0, 1]\r\n        self.imu_ax.set_title('IMU Data')\r\n        self.imu_ax.set_ylim(-10, 10)\r\n\r\n        self.pose_ax = self.axs[1, 0]\r\n        self.pose_ax.set_title('Robot Pose')\r\n        self.pose_ax.set_xlim(-5, 5)\r\n        self.pose_ax.set_ylim(-5, 5)\r\n        self.pose_ax.grid(True)\r\n\r\n        self.camera_ax = self.axs[1, 1]\r\n        self.camera_ax.set_title('Camera Image')\r\n\r\n        # Start visualization thread\r\n        self.viz_thread = threading.Thread(target=self.run_visualization)\r\n        self.viz_thread.daemon = True\r\n        self.viz_thread.start()\r\n\r\n        self.get_logger().info('Sensor Visualizer initialized')\r\n\r\n    def camera_callback(self, msg):\r\n        \"\"\"Store camera data\"\"\"\r\n        try:\r\n            self.camera_data = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error converting camera image: {e}')\r\n\r\n    def lidar_callback(self, msg):\r\n        \"\"\"Store LiDAR data\"\"\"\r\n        try:\r\n            # Convert LiDAR scan to x,y coordinates\r\n            angles = np.array([msg.angle_min + i * msg.angle_increment for i in range(len(msg.ranges))])\r\n            ranges = np.array(msg.ranges)\r\n\r\n            # Filter out invalid ranges\r\n            valid_indices = (ranges >= msg.range_min) & (ranges <= msg.range_max)\r\n            valid_angles = angles[valid_indices]\r\n            valid_ranges = ranges[valid_indices]\r\n\r\n            # Convert to Cartesian coordinates\r\n            x = valid_ranges * np.cos(valid_angles)\r\n            y = valid_ranges * np.sin(valid_angles)\r\n\r\n            self.lidar_data = (x, y)\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing LiDAR data: {e}')\r\n\r\n    def imu_callback(self, msg):\r\n        \"\"\"Store IMU data\"\"\"\r\n        try:\r\n            self.imu_data = {\r\n                'linear_acceleration': [\r\n                    msg.linear_acceleration.x,\r\n                    msg.linear_acceleration.y,\r\n                    msg.linear_acceleration.z\r\n                ],\r\n                'angular_velocity': [\r\n                    msg.angular_velocity.x,\r\n                    msg.angular_velocity.y,\r\n                    msg.angular_velocity.z\r\n                ],\r\n                'orientation': [\r\n                    msg.orientation.x,\r\n                    msg.orientation.y,\r\n                    msg.orientation.z,\r\n                    msg.orientation.w\r\n                ]\r\n            }\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing IMU data: {e}')\r\n\r\n    def pose_callback(self, msg):\r\n        \"\"\"Store fused pose data\"\"\"\r\n        try:\r\n            self.fused_pose = {\r\n                'x': msg.pose.position.x,\r\n                'y': msg.pose.position.y,\r\n                'z': msg.pose.position.z,\r\n                'qx': msg.pose.orientation.x,\r\n                'qy': msg.pose.orientation.y,\r\n                'qz': msg.pose.orientation.z,\r\n                'qw': msg.pose.orientation.w\r\n            }\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing pose data: {e}')\r\n\r\n    def update_visualization(self, frame):\r\n        \"\"\"Update visualization plots\"\"\"\r\n        # Update LiDAR plot\r\n        if self.lidar_data:\r\n            x, y = self.lidar_data\r\n            self.lidar_line.set_data(x, y)\r\n\r\n        # Update IMU plot\r\n        if self.imu_data:\r\n            acc = self.imu_data['linear_acceleration']\r\n            self.imu_ax.clear()\r\n            self.imu_ax.plot(acc, 'b-', label='Linear Acc')\r\n            self.imu_ax.set_title('IMU Data')\r\n            self.imu_ax.legend()\r\n            self.imu_ax.grid(True)\r\n\r\n        # Update pose plot\r\n        if self.fused_pose:\r\n            x = self.fused_pose['x']\r\n            y = self.fused_pose['y']\r\n            self.pose_ax.plot(x, y, 'ro', markersize=10)\r\n            self.pose_ax.set_title(f'Robot Pose: ({x:.2f}, {y:.2f})')\r\n\r\n        # Update camera plot\r\n        if self.camera_data is not None:\r\n            self.camera_ax.clear()\r\n            self.camera_ax.imshow(self.camera_data)\r\n            self.camera_ax.set_title('Camera Image')\r\n            self.camera_ax.axis('off')\r\n\r\n        return self.lidar_line,\r\n\r\n    def run_visualization(self):\r\n        \"\"\"Run the visualization in a separate thread\"\"\"\r\n        try:\r\n            ani = FuncAnimation(self.fig, self.update_visualization, interval=100, blit=False)\r\n            plt.tight_layout()\r\n            plt.show()\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in visualization: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    visualizer = SensorVisualizer()\r\n\r\n    try:\r\n        rclpy.spin(visualizer)\r\n    except KeyboardInterrupt:\r\n        plt.close('all')\r\n    finally:\r\n        visualizer.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\n"})}),"\n",(0,a.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,a.jsx)(n.h3,{id:"1-creating-sensor-integration-test-script",children:"1. Creating Sensor Integration Test Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'cat > ~/test_sensor_integration.sh << \'EOF\'\r\n#!/bin/bash\r\n\r\n# Test script for multimodal sensor integration\r\n\r\necho "Testing Multimodal Sensor Integration..."\r\n\r\n# Source ROS environment\r\nsource /opt/ros/humble/setup.bash\r\nsource ~/isaac_ros_ws/install/setup.bash\r\n\r\n# Check if sensor fusion package is available\r\necho "Checking sensor fusion package..."\r\nros2 pkg list | grep sensor_fusion\r\n\r\nif [ $? -eq 0 ]; then\r\n    echo "\u2713 Sensor fusion package found"\r\nelse\r\n    echo "\u2717 Sensor fusion package not found"\r\n    exit 1\r\nfi\r\n\r\n# Check for sensor configuration files\r\nif [ -f "/workspace/shared_dir/configs/sensor_config.yaml" ]; then\r\n    echo "\u2713 Sensor configuration file found"\r\nelse\r\n    echo "\u2717 Sensor configuration file not found"\r\n    exit 1\r\nfi\r\n\r\n# Check for multimodal robot configuration\r\nif [ -f "/workspace/shared_dir/robots/multimodal_robot.usd" ]; then\r\n    echo "\u2713 Multimodal robot configuration found"\r\nelse\r\n    echo "\u2717 Multimodal robot configuration not found"\r\n    exit 1\r\nfi\r\n\r\n# Build the sensor fusion package\r\necho "Building sensor fusion package..."\r\ncd ~/isaac_ros_ws\r\ncolcon build --packages-select isaac_sensor_fusion\r\nsource install/setup.bash\r\n\r\n# Check if launch files exist\r\nif [ -f "/workspace/shared_dir/src/isaac_sensor_fusion/launch/sensor_integration.launch.py" ]; then\r\n    echo "\u2713 Sensor integration launch file found"\r\nelse\r\n    echo "\u2717 Sensor integration launch file not found"\r\n    exit 1\r\nfi\r\n\r\n# Test sensor topics are available (when Isaac Sim is running)\r\necho "Sensor integration test completed."\r\necho "To fully test, run Isaac Sim and launch the sensor integration system:"\r\necho "ros2 launch isaac_sensor_fusion sensor_integration.launch.py"\r\nEOF\r\n\r\n# Make executable\r\nchmod +x ~/test_sensor_integration.sh\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-running-sensor-integration-test",children:"2. Running Sensor Integration Test"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"~/test_sensor_integration.sh\n"})}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting-sensor-integration",children:"Troubleshooting Sensor Integration"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,a.jsx)(n.h4,{id:"issue-sensor-data-not-synchronizing-properly",children:'Issue: "Sensor data not synchronizing properly"'}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Solution"}),": Check timing and buffer sizes"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Verify sensor topics are publishing\r\nros2 topic echo /camera/color/image_raw --field header.stamp --field height --field width &\r\nros2 topic echo /scan --field header.stamp --field ranges.size &\r\nros2 topic echo /imu --field header.stamp --field orientation\n"})}),"\n",(0,a.jsx)(n.h4,{id:"issue-tf-transforms-not-available",children:'Issue: "TF transforms not available"'}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Solution"}),": Verify static transform publisher and frame names"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Check TF tree\r\nros2 run tf2_tools view_frames\r\n\r\n# Echo transforms\r\nros2 run tf2_ros tf2_echo base_link camera_link\n"})}),"\n",(0,a.jsx)(n.h4,{id:"issue-high-cpugpu-usage-with-multiple-sensors",children:'Issue: "High CPU/GPU usage with multiple sensors"'}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Solution"}),": Optimize sensor frequencies and processing"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Check current sensor frequencies\r\nros2 topic hz /camera/color/image_raw\r\nros2 topic hz /scan\r\nros2 topic hz /imu\n"})}),"\n",(0,a.jsx)(n.h2,{id:"verification-checklist",children:"Verification Checklist"}),"\n",(0,a.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Multimodal robot configuration created"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sensor configuration files created"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sensor synchronization implemented"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Advanced sensor fusion node created"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","TF configuration and static transforms set up"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Launch files created for sensor integration"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Calibration tools implemented"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visualization tools created"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test script created and functional"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Troubleshooting guide reviewed"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"After implementing multimodal sensor integration:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Test sensor fusion"})," with Isaac Sim"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate sensor data"})," synchronization and accuracy"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Optimize performance"})," based on monitoring results"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Create sensor integration exercises"})," for students"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The multimodal sensor integration framework is now configured and ready for Module 3, providing students with tools to combine data from multiple sensors for enhanced AI-robot perception."})]})}function m(r={}){const{wrapper:n}={...(0,i.R)(),...r.components};return n?(0,a.jsx)(n,{...r,children:(0,a.jsx)(f,{...r})}):f(r)}},8453:(r,n,e)=>{e.d(n,{R:()=>t,x:()=>o});var s=e(6540);const a={},i=s.createContext(a);function t(r){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof r?r(n):{...n,...r}},[n,r])}function o(r){let n;return n=r.disableParentContext?"function"==typeof r.components?r.components(a):r.components||a:t(r.components),s.createElement(i.Provider,{value:n},r.children)}}}]);