"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[2216],{5794:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"chapter-3/lesson-3","title":"Lesson 3.1.3: Perception and AI Decision Making","description":"Overview","source":"@site/docs/chapter-3/lesson-3.md","sourceDirName":"chapter-3","slug":"/chapter-3/lesson-3","permalink":"/ur/docs/chapter-3/lesson-3","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-3/lesson-3.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.1.2: Navigation and Path Planning with Nav2","permalink":"/ur/docs/chapter-3/lesson-2"},"next":{"title":"Lesson 3.1.4: AI-Robot Integration and Optimization","permalink":"/ur/docs/chapter-3/lesson-4"}}');var s=i(4848),t=i(8453);const o={},a="Lesson 3.1.3: Perception and AI Decision Making",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to AI Perception in Isaac",id:"introduction-to-ai-perception-in-isaac",level:2},{value:"Key Isaac AI Perception Components",id:"key-isaac-ai-perception-components",level:3},{value:"Agent Interaction Points",id:"agent-interaction-points",level:2},{value:"AI Assistant Request: Isaac AI Perception Architecture",id:"ai-assistant-request-isaac-ai-perception-architecture",level:3},{value:"AI Assistant Request: Troubleshoot Perception Pipeline",id:"ai-assistant-request-troubleshoot-perception-pipeline",level:3},{value:"AI Assistant Request: Optimize AI Performance",id:"ai-assistant-request-optimize-ai-performance",level:3},{value:"Perception Pipeline Architecture",id:"perception-pipeline-architecture",level:2},{value:"Data Flow in Perception Systems",id:"data-flow-in-perception-systems",level:3},{value:"Components of an AI Perception Pipeline",id:"components-of-an-ai-perception-pipeline",level:3},{value:"Object Detection with Isaac",id:"object-detection-with-isaac",level:2},{value:"Isaac Detection Pipeline",id:"isaac-detection-pipeline",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"Isaac Semantic Segmentation Pipeline",id:"isaac-semantic-segmentation-pipeline",level:3},{value:"Depth Estimation",id:"depth-estimation",level:2},{value:"Isaac Depth Estimation Pipeline",id:"isaac-depth-estimation-pipeline",level:3},{value:"AI Decision Making Systems",id:"ai-decision-making-systems",level:2},{value:"Behavior Trees for AI Decision Making",id:"behavior-trees-for-ai-decision-making",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Multi-Sensor Fusion Example",id:"multi-sensor-fusion-example",level:3},{value:"Exercise 5: Basic Object Detection",id:"exercise-5-basic-object-detection",level:2},{value:"Objective",id:"objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Expected Outcome",id:"expected-outcome",level:3},{value:"Exercise 6: AI Decision Making Integration",id:"exercise-6-ai-decision-making-integration",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Steps",id:"steps-1",level:3},{value:"Expected Outcome",id:"expected-outcome-1",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"TensorRT Optimization",id:"tensorrt-optimization",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2},{value:"Agent Interaction Points for Review",id:"agent-interaction-points-for-review",level:2},{value:"AI Assistant Request: Lesson Review",id:"ai-assistant-request-lesson-review",level:3},{value:"AI Assistant Request: Troubleshooting Summary",id:"ai-assistant-request-troubleshooting-summary",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-313-perception-and-ai-decision-making",children:"Lesson 3.1.3: Perception and AI Decision Making"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This lesson focuses on implementing perception systems using NVIDIA Isaac's AI capabilities and creating intelligent decision-making algorithms for robots. You'll learn how to process sensor data using AI models and make intelligent decisions based on perception results."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement perception systems using Isaac's AI capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Deploy machine learning models for robot perception"}),"\n",(0,s.jsx)(n.li,{children:"Create AI decision-making algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Integrate perception with action planning"}),"\n",(0,s.jsx)(n.li,{children:"Optimize perception systems for real-time performance"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-ai-perception-in-isaac",children:"Introduction to AI Perception in Isaac"}),"\n",(0,s.jsx)(n.p,{children:"AI perception in NVIDIA Isaac involves using deep learning models to process sensor data and extract meaningful information about the environment. This includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Object detection and classification"}),"\n",(0,s.jsx)(n.li,{children:"Semantic segmentation"}),"\n",(0,s.jsx)(n.li,{children:"Depth estimation"}),"\n",(0,s.jsx)(n.li,{children:"Pose estimation"}),"\n",(0,s.jsx)(n.li,{children:"Activity recognition"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-isaac-ai-perception-components",children:"Key Isaac AI Perception Components"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Manipulator Perception"}),": For object detection in manipulation tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Stereo DNN"}),": For depth estimation using stereo cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Detection NITROS"}),": For optimized object detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS AprilTag"}),": For fiducial marker detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Stereo Image Rectification"}),": For stereo image processing"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"agent-interaction-points",children:"Agent Interaction Points"}),"\n",(0,s.jsx)(n.h3,{id:"ai-assistant-request-isaac-ai-perception-architecture",children:"AI Assistant Request: Isaac AI Perception Architecture"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Context"}),": Understanding Isaac AI perception components\r\n",(0,s.jsx)(n.strong,{children:"Request"}),': "Explain the architecture of Isaac\'s AI perception system and how different components work together"\r\n',(0,s.jsx)(n.strong,{children:"Expected Output"}),": Detailed explanation of Isaac AI perception architecture and component interactions"]}),"\n",(0,s.jsx)(n.h3,{id:"ai-assistant-request-troubleshoot-perception-pipeline",children:"AI Assistant Request: Troubleshoot Perception Pipeline"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Context"}),": Isaac perception pipeline setup\r\n",(0,s.jsx)(n.strong,{children:"Request"}),": \"I'm having issues with my Isaac perception pipeline. The object detection isn't working properly. What should I check?\"\r\n",(0,s.jsx)(n.strong,{children:"Expected Output"}),": Troubleshooting guide for Isaac perception pipeline issues"]}),"\n",(0,s.jsx)(n.h3,{id:"ai-assistant-request-optimize-ai-performance",children:"AI Assistant Request: Optimize AI Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Context"}),": Performance optimization for AI perception\r\n",(0,s.jsx)(n.strong,{children:"Request"}),': "How can I optimize my AI perception system for real-time performance on Isaac?"\r\n',(0,s.jsx)(n.strong,{children:"Expected Output"}),": Guide to optimizing AI perception performance on Isaac platform"]}),"\n",(0,s.jsx)(n.h2,{id:"perception-pipeline-architecture",children:"Perception Pipeline Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"data-flow-in-perception-systems",children:"Data Flow in Perception Systems"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[Sensors] -> [Preprocessing] -> [AI Model] -> [Postprocessing] -> [Decision Making]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"components-of-an-ai-perception-pipeline",children:"Components of an AI Perception Pipeline"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Interface"}),": Receives raw sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Preprocessing"}),": Normalizes and formats data for AI models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AI Inference"}),": Runs deep learning models on the data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Result Postprocessing"}),": Converts model outputs to actionable information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decision Interface"}),": Provides information to decision-making systems"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-with-isaac",children:"Object Detection with Isaac"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-detection-pipeline",children:"Isaac Detection Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torchvision.transforms as transforms\r\n\r\nclass IsaacObjectDetection(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_object_detection')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to camera images\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for detections\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray,\r\n            '/object_detections',\r\n            10\r\n        )\r\n\r\n        # Load pre-trained model (example with TorchVision)\r\n        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\r\n        self.model.eval()\r\n\r\n        # Transformation for input images\r\n        self.transform = transforms.Compose([\r\n            transforms.ToPILImage(),\r\n            transforms.Resize((640, 640)),\r\n            transforms.ToTensor()\r\n        ])\r\n\r\n    def image_callback(self, image_msg):\r\n        # Convert ROS Image to OpenCV\r\n        cv_image = self.bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\r\n\r\n        # Preprocess image\r\n        input_tensor = self.transform(cv_image)\r\n        input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\r\n\r\n        # Run inference\r\n        with torch.no_grad():\r\n            results = self.model(input_batch)\r\n\r\n        # Process results\r\n        detections_msg = self.process_detections(results, image_msg.height, image_msg.width)\r\n\r\n        # Publish detections\r\n        self.detection_publisher.publish(detections_msg)\r\n\r\n    def process_detections(self, results, img_height, img_width):\r\n        # Process YOLOv5 results\r\n        detections_msg = Detection2DArray()\r\n\r\n        # Extract bounding boxes and labels\r\n        for detection in results.xyxy[0]:  # x1, y1, x2, y2, confidence, class\r\n            x1, y1, x2, y2, conf, cls = detection\r\n            if conf > 0.5:  # Confidence threshold\r\n                detection_2d = Detection2D()\r\n\r\n                # Set bounding box\r\n                bbox = BoundingBox2D()\r\n                bbox.size_x = float(x2 - x1)\r\n                bbox.size_y = float(y2 - y1)\r\n                bbox.center.x = float(x1 + (x2 - x1) / 2)\r\n                bbox.center.y = float(y1 + (y2 - y1) / 2)\r\n                detection_2d.bbox = bbox\r\n\r\n                # Set confidence\r\n                detection_2d.results.append(\r\n                    ObjectHypothesisWithPose(\r\n                        hypothesis=ObjectHypothesis(\r\n                            id=int(cls),\r\n                            score=float(conf)\r\n                        )\r\n                    )\r\n                )\r\n\r\n                detections_msg.detections.append(detection_2d)\r\n\r\n        return detections_msg\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    detector = IsaacObjectDetection()\r\n    rclpy.spin(detector)\r\n    detector.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,s.jsx)(n.p,{children:"Semantic segmentation assigns a class label to each pixel in an image, enabling detailed scene understanding."}),"\n",(0,s.jsx)(n.h3,{id:"isaac-semantic-segmentation-pipeline",children:"Isaac Semantic Segmentation Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nimport numpy as np\r\n\r\nclass IsaacSemanticSegmentation(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_semantic_segmentation')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to camera images\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for segmentation masks\r\n        self.mask_publisher = self.create_publisher(\r\n            Image,\r\n            '/segmentation_mask',\r\n            10\r\n        )\r\n\r\n        # Load segmentation model\r\n        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)\r\n        self.model.eval()\r\n\r\n        # Define class names for visualization\r\n        self.class_names = [\r\n            'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\r\n            'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\r\n            'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\r\n            'train', 'tvmonitor'\r\n        ]\r\n\r\n        # Color palette for segmentation visualization\r\n        self.color_palette = np.random.randint(0, 255, size=(len(self.class_names), 3))\r\n\r\n    def image_callback(self, msg):\r\n        # Convert ROS Image to OpenCV\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n\r\n        # Preprocess image\r\n        input_tensor = self.transform(cv_image)\r\n        input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\r\n\r\n        # Run segmentation\r\n        with torch.no_grad():\r\n            output = self.model(input_batch)['out'][0]\r\n            segmentation = output.argmax(0).cpu().numpy()\r\n\r\n        # Create colored segmentation mask\r\n        colored_mask = self.color_palette[segmentation].astype(np.uint8)\r\n\r\n        # Convert back to ROS Image and publish\r\n        mask_msg = self.bridge.cv2_to_imgmsg(colored_mask, encoding=\"rgb8\")\r\n        mask_msg.header = msg.header\r\n        self.mask_publisher.publish(mask_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    segmenter = IsaacSemanticSegmentation()\r\n    rclpy.spin(segmenter)\r\n    segmenter.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"depth-estimation",children:"Depth Estimation"}),"\n",(0,s.jsx)(n.p,{children:"Depth estimation provides 3D information from 2D images, crucial for navigation and manipulation."}),"\n",(0,s.jsx)(n.h3,{id:"isaac-depth-estimation-pipeline",children:"Isaac Depth Estimation Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom sensor_msgs.msg import PointCloud2\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nimport numpy as np\r\n\r\nclass IsaacDepthEstimation(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_depth_estimation')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to stereo images\r\n        self.left_image_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/depth/image_rect_raw',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for depth maps\r\n        self.depth_publisher = self.create_publisher(\r\n            Image,\r\n            '/estimated_depth',\r\n            10\r\n        )\r\n\r\n        # Load depth estimation model\r\n        # Using MiDaS for monocular depth estimation\r\n        self.model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\", pretrained=True)\r\n        self.model.eval()\r\n\r\n        # Transformation for depth estimation\r\n        self.transform = transforms.Compose([\r\n            transforms.Resize((384, 384)),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n        ])\r\n\r\n    def depth_callback(self, image_msg):\r\n        # Convert ROS Image to OpenCV\r\n        cv_image = self.bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\r\n\r\n        # Preprocess image\r\n        input_tensor = self.transform(cv_image)\r\n        input_batch = input_tensor.unsqueeze(0)\r\n\r\n        # Run depth estimation\r\n        with torch.no_grad():\r\n            prediction = self.model(input_batch)\r\n\r\n        # Normalize depth prediction\r\n        depth_map = prediction.squeeze().cpu().numpy()\r\n        depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\r\n        depth_map = (depth_map * 255).astype(np.uint8)\r\n\r\n        # Convert to ROS Image and publish\r\n        depth_msg = self.bridge.cv2_to_imgmsg(depth_map, encoding=\"mono8\")\r\n        depth_msg.header = image_msg.header\r\n        self.depth_publisher.publish(depth_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    depth_estimator = IsaacDepthEstimation()\r\n    rclpy.spin(depth_estimator)\r\n    depth_estimator.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"ai-decision-making-systems",children:"AI Decision Making Systems"}),"\n",(0,s.jsx)(n.h3,{id:"behavior-trees-for-ai-decision-making",children:"Behavior Trees for AI Decision Making"}),"\n",(0,s.jsx)(n.p,{children:"Behavior trees provide a structured approach to AI decision making:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Twist\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom sensor_msgs.msg import LaserScan\r\nimport math\r\n\r\nclass BehaviorTreeAI(Node):\r\n    def __init__(self):\r\n        super().__init__(\'behavior_tree_ai\')\r\n\r\n        # Publishers and subscribers\r\n        self.cmd_publisher = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.detection_subscription = self.create_subscription(\r\n            Detection2DArray,\r\n            \'/object_detections\',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n        self.scan_subscription = self.create_subscription(\r\n            LaserScan,\r\n            \'/scan\',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n\r\n        self.detections = []\r\n        self.obstacle_distances = []\r\n        self.target_found = False\r\n        self.target_position = None\r\n\r\n    def detection_callback(self, msg):\r\n        self.detections = msg.detections\r\n\r\n    def scan_callback(self, msg):\r\n        self.obstacle_distances = msg.ranges\r\n\r\n    def run_behavior_tree(self):\r\n        """Execute the behavior tree"""\r\n        # Check if target is detected\r\n        if self.check_target_detected():\r\n            return self.move_to_target()\r\n        else:\r\n            # Explore environment\r\n            return self.explore_environment()\r\n\r\n    def check_target_detected(self):\r\n        """Check if target object is detected"""\r\n        for detection in self.detections:\r\n            # Assuming target is a person (class ID 15 in COCO dataset)\r\n            if detection.results[0].hypothesis.id == 15 and detection.results[0].hypothesis.score > 0.7:\r\n                self.target_position = detection.bbox.center\r\n                return True\r\n        return False\r\n\r\n    def move_to_target(self):\r\n        """Move robot towards detected target"""\r\n        twist = Twist()\r\n\r\n        # Simple proportional controller\r\n        if self.target_position.x > 320:  # Target is to the right\r\n            twist.angular.z = -0.3\r\n        elif self.target_position.x < 320:  # Target is to the left\r\n            twist.angular.z = 0.3\r\n        else:\r\n            twist.linear.x = 0.2  # Move forward\r\n\r\n        return twist\r\n\r\n    def explore_environment(self):\r\n        """Explore environment looking for targets"""\r\n        twist = Twist()\r\n\r\n        # Check for obstacles\r\n        min_distance = min(self.obstacle_distances) if self.obstacle_distances else float(\'inf\')\r\n\r\n        if min_distance < 0.5:  # Obstacle too close\r\n            twist.angular.z = 0.5  # Turn away\r\n        else:\r\n            twist.linear.x = 0.2  # Move forward\r\n\r\n        return twist\r\n\r\n    def timer_callback(self):\r\n        """Main AI loop"""\r\n        command = self.run_behavior_tree()\r\n        self.cmd_publisher.publish(command)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    ai = BehaviorTreeAI()\r\n\r\n    # Create timer for main loop\r\n    timer = ai.create_timer(0.1, ai.timer_callback)\r\n\r\n    rclpy.spin(ai)\r\n    ai.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Sensor fusion combines data from multiple sensors to improve perception accuracy."}),"\n",(0,s.jsx)(n.h3,{id:"multi-sensor-fusion-example",children:"Multi-Sensor Fusion Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu\r\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\r\nfrom tf2_ros import TransformListener, Buffer\r\nimport numpy as np\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass IsaacSensorFusion(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_sensor_fusion')\r\n\r\n        # Initialize TF buffer\r\n        self.tf_buffer = Buffer()\r\n        self.tf_listener = TransformListener(self.tf_buffer, self)\r\n\r\n        # Subscribe to multiple sensors\r\n        self.camera_subscription = self.create_subscription(\r\n            Image, '/camera/color/image_raw', self.camera_callback, 10)\r\n        self.lidar_subscription = self.create_subscription(\r\n            LaserScan, '/scan', self.lidar_callback, 10)\r\n        self.imu_subscription = self.create_subscription(\r\n            Imu, '/imu', self.imu_callback, 10)\r\n\r\n        # Publisher for fused perception\r\n        self.fused_publisher = self.create_publisher(\r\n            PoseWithCovarianceStamped, '/fused_pose', 10)\r\n\r\n        # Store sensor data\r\n        self.camera_data = None\r\n        self.lidar_data = None\r\n        self.imu_data = None\r\n\r\n    def camera_callback(self, msg):\r\n        self.camera_data = msg\r\n\r\n    def lidar_callback(self, msg):\r\n        self.lidar_data = msg\r\n\r\n    def imu_callback(self, msg):\r\n        self.imu_data = msg\r\n\r\n    def fuse_sensors(self):\r\n        \"\"\"Fusion algorithm combining multiple sensors\"\"\"\r\n        if not all([self.camera_data, self.lidar_data, self.imu_data]):\r\n            return None\r\n\r\n        # Example fusion: combine visual detection with LiDAR and IMU\r\n        # This is a simplified example - real fusion would use more sophisticated methods\r\n        fused_pose = PoseWithCovarianceStamped()\r\n\r\n        # Extract orientation from IMU\r\n        orientation = self.imu_data.orientation\r\n\r\n        # Use LiDAR for position (simplified)\r\n        min_range_idx = np.argmin(self.lidar_data.ranges)\r\n        range_distance = self.lidar_data.ranges[min_range_idx]\r\n\r\n        # Convert to position (simplified - assumes robot facing forward)\r\n        x = range_distance * math.cos(self.lidar_data.angle_min + min_range_idx * self.lidar_data.angle_increment)\r\n        y = range_distance * math.sin(self.lidar_data.angle_min + min_range_idx * self.lidar_data.angle_increment)\r\n\r\n        fused_pose.pose.pose.position.x = x\r\n        fused_pose.pose.pose.position.y = y\r\n        fused_pose.pose.pose.orientation = orientation\r\n\r\n        # Set covariance based on sensor uncertainties\r\n        covariance = np.zeros(36)\r\n        covariance[0] = 0.1  # x uncertainty\r\n        covariance[7] = 0.1  # y uncertainty\r\n        covariance[35] = 0.05  # yaw uncertainty\r\n        fused_pose.pose.covariance = covariance\r\n\r\n        return fused_pose\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    fusion = IsaacSensorFusion()\r\n    rclpy.spin(fusion)\r\n    fusion.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"exercise-5-basic-object-detection",children:"Exercise 5: Basic Object Detection"}),"\n",(0,s.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Implement an object detection system using Isaac's AI capabilities."}),"\n",(0,s.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up the Isaac object detection pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Configure the AI model for detection"}),"\n",(0,s.jsx)(n.li,{children:"Process camera images for object detection"}),"\n",(0,s.jsx)(n.li,{children:"Visualize detection results"}),"\n",(0,s.jsx)(n.li,{children:"Test with different objects"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,s.jsx)(n.p,{children:"System successfully detects and labels objects in camera images."}),"\n",(0,s.jsx)(n.h2,{id:"exercise-6-ai-decision-making-integration",children:"Exercise 6: AI Decision Making Integration"}),"\n",(0,s.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Integrate AI perception with decision-making algorithms."}),"\n",(0,s.jsx)(n.h3,{id:"steps-1",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create a perception system that detects specific objects"}),"\n",(0,s.jsx)(n.li,{children:"Implement a decision-making algorithm based on perception"}),"\n",(0,s.jsx)(n.li,{children:"Connect perception output to action commands"}),"\n",(0,s.jsx)(n.li,{children:"Test the integrated system in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate decision-making performance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"expected-outcome-1",children:"Expected Outcome"}),"\n",(0,s.jsx)(n.p,{children:"Robot makes intelligent decisions based on perceived environment."}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\r\n\r\n# Check if CUDA is available\r\nif torch.cuda.is_available():\r\n    device = torch.device("cuda")\r\n    print(f"Using GPU: {torch.cuda.get_device_name(0)}")\r\nelse:\r\n    device = torch.device("cpu")\r\n    print("Using CPU")\r\n\r\n# Move model to GPU\r\nmodel = model.to(device)\r\n\r\n# Move input data to GPU\r\ninput_tensor = input_tensor.to(device)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"tensorrt-optimization",children:"TensorRT Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import tensorrt as trt\r\nimport pycuda.driver as cuda\r\nimport pycuda.autoinit\r\n\r\ndef optimize_with_tensorrt(model):\r\n    """Optimize model with TensorRT"""\r\n    # Create TensorRT builder\r\n    builder = trt.Builder(trt.Logger(trt.Logger.WARNING))\r\n    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\r\n    config = builder.create_builder_config()\r\n\r\n    # Build engine\r\n    serialized_engine = builder.build_serialized_network(network, config)\r\n\r\n    return serialized_engine\n'})}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the main components of an AI perception pipeline?"}),"\n",(0,s.jsx)(n.li,{children:"How does semantic segmentation differ from object detection?"}),"\n",(0,s.jsx)(n.li,{children:"What is sensor fusion and why is it important in robotics?"}),"\n",(0,s.jsx)(n.li,{children:"How can behavior trees be used for AI decision making?"}),"\n",(0,s.jsx)(n.li,{children:"What techniques can be used to optimize AI perception performance?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This lesson covered AI perception and decision making in the NVIDIA Isaac platform. You learned about object detection, semantic segmentation, depth estimation, AI decision making systems, and sensor fusion. The exercises provided hands-on experience with implementing AI perception systems and integrating them with decision-making algorithms."}),"\n",(0,s.jsx)(n.h2,{id:"agent-interaction-points-for-review",children:"Agent Interaction Points for Review"}),"\n",(0,s.jsx)(n.h3,{id:"ai-assistant-request-lesson-review",children:"AI Assistant Request: Lesson Review"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Context"}),": Review of Lesson 3 content\r\n",(0,s.jsx)(n.strong,{children:"Request"}),': "Summarize the key AI perception concepts from Lesson 3 and suggest optimization strategies"\r\n',(0,s.jsx)(n.strong,{children:"Expected Output"}),": Concise summary of AI perception concepts and recommended optimization strategies"]}),"\n",(0,s.jsx)(n.h3,{id:"ai-assistant-request-troubleshooting-summary",children:"AI Assistant Request: Troubleshooting Summary"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Context"}),": Common issues in Lesson 3\r\n",(0,s.jsx)(n.strong,{children:"Request"}),': "What are the most common AI perception issues students face in Lesson 3 and how to solve them?"\r\n',(0,s.jsx)(n.strong,{children:"Expected Output"}),": Compilation of common AI perception issues and their solutions"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var r=i(6540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);