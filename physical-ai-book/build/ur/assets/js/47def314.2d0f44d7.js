"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[3935],{2846:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"chapter-4/capstone-project","title":"Capstone Project: Autonomous Humanoid Task","description":"Overview","source":"@site/docs/chapter-4/capstone-project.md","sourceDirName":"chapter-4","slug":"/chapter-4/capstone-project","permalink":"/ur/docs/chapter-4/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-4/capstone-project.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Security Considerations: Vision-Language-Action (VLA) Systems","permalink":"/ur/docs/chapter-4/security-considerations"},"next":{"title":"Contributing","permalink":"/ur/docs/contributing"}}');var t=i(4848),r=i(8453);const o={},a="Capstone Project: Autonomous Humanoid Task",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Project Objectives",id:"project-objectives",level:2},{value:"Capstone Task Description",id:"capstone-task-description",level:2},{value:"Example Commands",id:"example-commands",level:3},{value:"System Architecture for Capstone",id:"system-architecture-for-capstone",level:2},{value:"Implementation Requirements",id:"implementation-requirements",level:2},{value:"1. Core Components Integration",id:"1-core-components-integration",level:3},{value:"2. Performance Criteria",id:"2-performance-criteria",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Phase 1: Environment Setup",id:"phase-1-environment-setup",level:3},{value:"Phase 2: Basic Task Execution",id:"phase-2-basic-task-execution",level:3},{value:"Phase 3: Integrated Task Execution",id:"phase-3-integrated-task-execution",level:3},{value:"Phase 4: Advanced Capabilities",id:"phase-4-advanced-capabilities",level:3},{value:"Phase 5: Validation and Testing",id:"phase-5-validation-and-testing",level:3},{value:"Technical Implementation",id:"technical-implementation",level:2},{value:"Main Capstone Class",id:"main-capstone-class",level:3},{value:"Testing Scenarios",id:"testing-scenarios",level:2},{value:"Scenario 1: Simple Object Retrieval",id:"scenario-1-simple-object-retrieval",level:3},{value:"Scenario 2: Multi-Step Task",id:"scenario-2-multi-step-task",level:3},{value:"Scenario 3: Complex Spatial Reasoning",id:"scenario-3-complex-spatial-reasoning",level:3},{value:"Performance Validation",id:"performance-validation",level:2},{value:"Success Metrics",id:"success-metrics",level:3},{value:"Validation Process",id:"validation-process",level:3},{value:"Expected Outcomes",id:"expected-outcomes",level:2},{value:"Troubleshooting Guide",id:"troubleshooting-guide",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Assessment Rubric",id:"assessment-rubric",level:2},{value:"Technical Implementation (50%)",id:"technical-implementation-50",level:3},{value:"Task Execution (30%)",id:"task-execution-30",level:3},{value:"Documentation and Validation (20%)",id:"documentation-and-validation-20",level:3},{value:"Extensions and Future Work",id:"extensions-and-future-work",level:2},{value:"Advanced Capabilities",id:"advanced-capabilities",level:3},{value:"Research Opportunities",id:"research-opportunities",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"capstone-project-autonomous-humanoid-task",children:"Capstone Project: Autonomous Humanoid Task"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project for Module 4 demonstrates the complete Vision-Language-Action (VLA) system by implementing an autonomous humanoid robot that can understand and execute complex natural language commands in a simulation environment. This project integrates all components developed throughout the module into a cohesive system capable of performing real-world tasks through voice interaction."}),"\n",(0,t.jsx)(n.h2,{id:"project-objectives",children:"Project Objectives"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project aims to demonstrate:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"End-to-End Integration"}),": Complete pipeline from voice command to robotic action"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Modal Processing"}),": Integration of vision, language, and action systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complex Task Execution"}),": Multi-step tasks requiring planning and coordination"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-World Application"}),": Practical robotics tasks in simulated environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Robustness"}),": Handling of various command types and error conditions"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-task-description",children:"Capstone Task Description"}),"\n",(0,t.jsxs)(n.p,{children:["The primary capstone task is: ",(0,t.jsx)(n.strong,{children:'"Autonomous Object Retrieval and Delivery"'})]}),"\n",(0,t.jsx)(n.p,{children:"The humanoid robot must:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Accept a natural language command to fetch a specific object"}),"\n",(0,t.jsx)(n.li,{children:'Navigate to the specified location (e.g., "kitchen", "living room")'}),"\n",(0,t.jsx)(n.li,{children:"Identify and locate the target object using visual perception"}),"\n",(0,t.jsx)(n.li,{children:"Grasp and manipulate the object appropriately"}),"\n",(0,t.jsx)(n.li,{children:"Navigate to the destination location"}),"\n",(0,t.jsx)(n.li,{children:"Safely place the object at the designated location"}),"\n",(0,t.jsx)(n.li,{children:"Report completion status to the user"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-commands",children:"Example Commands"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Go to the kitchen and bring me the red cup"'}),"\n",(0,t.jsx)(n.li,{children:'"Find the blue book on the shelf and place it on the table"'}),"\n",(0,t.jsx)(n.li,{children:'"Get the green ball from the bedroom and bring it here"'}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture-for-capstone",children:"System Architecture for Capstone"}),"\n",(0,t.jsx)(n.p,{children:"The capstone implementation utilizes the complete VLA architecture:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"User Voice Command\r\n        \u2193\r\n[Voice Processing] \u2192 Speech-to-Text with Whisper\r\n        \u2193\r\n[Cognitive Planning] \u2192 LLM-based Task Decomposition\r\n        \u2193\r\n[Visual Perception] \u2192 Object Detection and 3D Positioning\r\n        \u2193\r\n[Action Execution] \u2192 ROS 2 Navigation and Manipulation\r\n        \u2193\r\n[Feedback System] \u2192 Status Communication\n"})}),"\n",(0,t.jsx)(n.h2,{id:"implementation-requirements",children:"Implementation Requirements"}),"\n",(0,t.jsx)(n.h3,{id:"1-core-components-integration",children:"1. Core Components Integration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Processing"}),": Whisper for speech-to-text conversion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognitive Planning"}),": LLM for command interpretation and action planning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Perception"}),": Real-time object detection and spatial reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Execution"}),": ROS 2 action servers for navigation and manipulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"State Management"}),": Execution state tracking and monitoring"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-performance-criteria",children:"2. Performance Criteria"}),"\n",(0,t.jsx)(n.p,{children:"The system must meet the following success criteria:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response Time"}),": ",(0,t.jsx)(n.code,{children:"<5"})," seconds from voice input to action initiation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Success Rate"}),": >75% for complex multi-step tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Recognition Accuracy"}),": >80% for target objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation Success Rate"}),": >90% for reaching specified locations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Understanding"}),": >85% accuracy for natural language commands"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Collision Avoidance"}),": Ensure safe navigation around obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Handling"}),": Safe grasping and manipulation of objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Recovery"}),": Graceful handling of failed actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Monitoring"}),": Continuous health checks and status reporting"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,t.jsx)(n.h3,{id:"phase-1-environment-setup",children:"Phase 1: Environment Setup"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Configure NVIDIA Isaac Sim with appropriate scene"}),"\n",(0,t.jsx)(n.li,{children:"Set up ROS 2 environment and robot model"}),"\n",(0,t.jsx)(n.li,{children:"Initialize all VLA system components"}),"\n",(0,t.jsx)(n.li,{children:"Configure API keys and system parameters"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"phase-2-basic-task-execution",children:"Phase 2: Basic Task Execution"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement simple navigation commands"}),"\n",(0,t.jsx)(n.li,{children:"Test object detection in the environment"}),"\n",(0,t.jsx)(n.li,{children:"Execute basic manipulation tasks"}),"\n",(0,t.jsx)(n.li,{children:"Validate individual component functionality"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"phase-3-integrated-task-execution",children:"Phase 3: Integrated Task Execution"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Connect all components in the full pipeline"}),"\n",(0,t.jsx)(n.li,{children:"Test end-to-end command processing"}),"\n",(0,t.jsx)(n.li,{children:"Implement multi-step task execution"}),"\n",(0,t.jsx)(n.li,{children:"Add error handling and recovery"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"phase-4-advanced-capabilities",children:"Phase 4: Advanced Capabilities"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement complex spatial reasoning"}),"\n",(0,t.jsx)(n.li,{children:"Add support for multiple object types"}),"\n",(0,t.jsx)(n.li,{children:"Enhance natural language understanding"}),"\n",(0,t.jsx)(n.li,{children:"Optimize performance and response time"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"phase-5-validation-and-testing",children:"Phase 5: Validation and Testing"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Execute comprehensive test scenarios"}),"\n",(0,t.jsx)(n.li,{children:"Validate against success criteria"}),"\n",(0,t.jsx)(n.li,{children:"Perform edge case testing"}),"\n",(0,t.jsx)(n.li,{children:"Document results and performance metrics"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"main-capstone-class",children:"Main Capstone Class"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# src/vla/capstone/autonomous_task.py (already implemented in lesson 5)\r\n\r\nclass AutonomousHumanoidTask:\r\n    """\r\n    Main class for the capstone autonomous task implementation.\r\n    Integrates all VLA components to perform complex tasks.\r\n    """\r\n\r\n    def __init__(self, vla_system, feedback_processor):\r\n        """\r\n        Initialize the capstone task with required components.\r\n        """\r\n        self.vla_system = vla_system\r\n        self.feedback_processor = feedback_processor\r\n        self.task_history = []\r\n        self.performance_metrics = {}\r\n\r\n    def execute_task_sequence(self, command_sequence):\r\n        """\r\n        Execute a sequence of commands as part of the capstone task.\r\n\r\n        Args:\r\n            command_sequence: List of commands to execute\r\n\r\n        Returns:\r\n            Task execution results and metrics\r\n        """\r\n        results = {\r\n            "start_time": time.time(),\r\n            "commands_executed": 0,\r\n            "success_count": 0,\r\n            "errors": [],\r\n            "total_time": 0\r\n        }\r\n\r\n        for command in command_sequence:\r\n            try:\r\n                # Process the command through the VLA pipeline\r\n                execution_id = self.vla_system.process_command_direct(command)\r\n\r\n                # Monitor execution and collect metrics\r\n                command_result = self._monitor_execution(execution_id)\r\n\r\n                results["commands_executed"] += 1\r\n                if command_result["success"]:\r\n                    results["success_count"] += 1\r\n                else:\r\n                    results["errors"].append({\r\n                        "command": command,\r\n                        "error": command_result.get("error", "Unknown error")\r\n                    })\r\n\r\n            except Exception as e:\r\n                results["errors"].append({\r\n                    "command": command,\r\n                    "error": str(e)\r\n                })\r\n\r\n        results["total_time"] = time.time() - results["start_time"]\r\n        results["success_rate"] = results["success_count"] / results["commands_executed"] if results["commands_executed"] > 0 else 0\r\n\r\n        return results\r\n\r\n    def _monitor_execution(self, execution_id, timeout=60):\r\n        """\r\n        Monitor execution of a command and return results.\r\n\r\n        Args:\r\n            execution_id: ID of the execution to monitor\r\n            timeout: Maximum time to wait for completion\r\n\r\n        Returns:\r\n            Dictionary with execution results\r\n        """\r\n        start_time = time.time()\r\n\r\n        while time.time() - start_time < timeout:\r\n            state = self.vla_system.action_executor.get_execution_status(execution_id)\r\n            if state:\r\n                if state.status == ExecutionStatus.SUCCESS:\r\n                    return {"success": True, "execution_time": time.time() - start_time}\r\n                elif state.status == ExecutionStatus.FAILED:\r\n                    return {"success": False, "error": state.error_message}\r\n\r\n            time.sleep(0.5)  # Check every 0.5 seconds\r\n\r\n        return {"success": False, "error": "Execution timeout"}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"testing-scenarios",children:"Testing Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"scenario-1-simple-object-retrieval",children:"Scenario 1: Simple Object Retrieval"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command"}),': "Go to the table and pick up the red cube"\r\n',(0,t.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Navigate to table location"}),"\n",(0,t.jsx)(n.li,{children:"Detect red cube in the scene"}),"\n",(0,t.jsx)(n.li,{children:"Plan approach path to cube"}),"\n",(0,t.jsx)(n.li,{children:"Execute grasping action"}),"\n",(0,t.jsx)(n.li,{children:"Report success"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"scenario-2-multi-step-task",children:"Scenario 2: Multi-Step Task"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command"}),': "Go to the kitchen, find a cup, bring it to the living room"\r\n',(0,t.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Navigate to kitchen"}),"\n",(0,t.jsx)(n.li,{children:"Identify cup object"}),"\n",(0,t.jsx)(n.li,{children:"Grasp the cup"}),"\n",(0,t.jsx)(n.li,{children:"Navigate to living room"}),"\n",(0,t.jsx)(n.li,{children:"Place cup at destination"}),"\n",(0,t.jsx)(n.li,{children:"Report completion"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"scenario-3-complex-spatial-reasoning",children:"Scenario 3: Complex Spatial Reasoning"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command"}),': "Pick up the object to the left of the blue box and place it on the table"\r\n',(0,t.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Identify blue box and its position"}),"\n",(0,t.jsx)(n.li,{children:"Determine object to the left of the box"}),"\n",(0,t.jsx)(n.li,{children:"Plan and execute grasping of correct object"}),"\n",(0,t.jsx)(n.li,{children:"Navigate to table"}),"\n",(0,t.jsx)(n.li,{children:"Place object appropriately"}),"\n",(0,t.jsx)(n.li,{children:"Report success"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"performance-validation",children:"Performance Validation"}),"\n",(0,t.jsx)(n.h3,{id:"success-metrics",children:"Success Metrics"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Completion Rate"}),": Percentage of tasks completed successfully"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response Time"}),": Average time from command to completion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Recognition Accuracy"}),": Correct identification of target objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation Success Rate"}),": Successful arrival at target locations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Understanding Rate"}),": Accurate interpretation of natural language"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"validation-process",children:"Validation Process"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def validate_capstone_performance():\r\n    """\r\n    Validate the capstone system against success criteria.\r\n    """\r\n    test_scenarios = [\r\n        {\r\n            "command": "Go to the table and pick up the red cube",\r\n            "expected_objects": ["red cube"],\r\n            "expected_location": "table",\r\n            "min_success_rate": 0.8\r\n        },\r\n        {\r\n            "command": "Bring the blue cup from kitchen to living room",\r\n            "expected_objects": ["blue cup"],\r\n            "expected_locations": ["kitchen", "living room"],\r\n            "min_success_rate": 0.75\r\n        }\r\n    ]\r\n\r\n    results = []\r\n    for scenario in test_scenarios:\r\n        scenario_results = execute_scenario_test(scenario)\r\n        results.append(scenario_results)\r\n\r\n    overall_success = calculate_overall_success(results)\r\n    return overall_success\n'})}),"\n",(0,t.jsx)(n.h2,{id:"expected-outcomes",children:"Expected Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"Upon successful completion of the capstone project, the system should demonstrate:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding"}),": Accurate interpretation of complex commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Scene Understanding"}),": Reliable object detection and spatial reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Planning"}),": Effective decomposition of complex tasks into executable actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robotic Execution"}),": Successful completion of navigation and manipulation tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Integration"}),": Seamless operation of all VLA components"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-guide",children:"Troubleshooting Guide"}),"\n",(0,t.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Issue 1"}),": Object not detected in environment"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Verify camera positioning and lighting conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Check object detector confidence thresholds"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Ensure objects are within camera field of view"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Issue 2"}),": Navigation fails to reach destination"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Check map and localization systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Verify path planning parameters"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Ensure no obstacles block the path"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Issue 3"}),": Grasping fails repeatedly"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Calibrate gripper and object positioning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Verify object graspability attributes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Check approach angles and gripper configuration"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Issue 4"}),": Command interpretation is incorrect"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Review LLM prompt engineering"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Check context object integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Verify spatial relationship processing"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"assessment-rubric",children:"Assessment Rubric"}),"\n",(0,t.jsx)(n.h3,{id:"technical-implementation-50",children:"Technical Implementation (50%)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Proper integration of all VLA components"}),"\n",(0,t.jsx)(n.li,{children:"Correct implementation of system architecture"}),"\n",(0,t.jsx)(n.li,{children:"Adequate error handling and recovery"}),"\n",(0,t.jsx)(n.li,{children:"Performance optimization"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"task-execution-30",children:"Task Execution (30%)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Successful completion of capstone task"}),"\n",(0,t.jsx)(n.li,{children:"Accuracy in object identification and manipulation"}),"\n",(0,t.jsx)(n.li,{children:"Efficiency in navigation and planning"}),"\n",(0,t.jsx)(n.li,{children:"Robustness in handling variations"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"documentation-and-validation-20",children:"Documentation and Validation (20%)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Comprehensive testing and validation"}),"\n",(0,t.jsx)(n.li,{children:"Clear documentation of implementation"}),"\n",(0,t.jsx)(n.li,{children:"Performance metrics and analysis"}),"\n",(0,t.jsx)(n.li,{children:"Troubleshooting and maintenance guides"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"extensions-and-future-work",children:"Extensions and Future Work"}),"\n",(0,t.jsx)(n.h3,{id:"advanced-capabilities",children:"Advanced Capabilities"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Support for more complex multi-step tasks"}),"\n",(0,t.jsx)(n.li,{children:"Integration with external knowledge bases"}),"\n",(0,t.jsx)(n.li,{children:"Learning from interaction and feedback"}),"\n",(0,t.jsx)(n.li,{children:"Adaptation to new environments and objects"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"research-opportunities",children:"Research Opportunities"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Improving natural language understanding"}),"\n",(0,t.jsx)(n.li,{children:"Enhancing visual perception accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Optimizing real-time performance"}),"\n",(0,t.jsx)(n.li,{children:"Developing more sophisticated planning algorithms"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project represents the culmination of Module 4, demonstrating how the Vision-Language-Action system enables natural human-robot interaction. By successfully implementing this project, students will have created a sophisticated AI-robotics system capable of understanding natural language commands and executing complex tasks in a simulated environment. This project showcases the integration of multiple AI technologies and provides a foundation for more advanced robotics applications."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);