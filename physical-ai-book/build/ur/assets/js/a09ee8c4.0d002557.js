"use strict";(globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics=globalThis.webpackChunkai_native_textbook_physical_ai_humanoid_robotics||[]).push([[2740],{5333:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"chapter-4/lesson-2","title":"Lesson 4.1.2: LLM-based Cognitive Planning","description":"Overview","source":"@site/docs/chapter-4/lesson-2.md","sourceDirName":"chapter-4","slug":"/chapter-4/lesson-2","permalink":"/ur/docs/chapter-4/lesson-2","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-4/lesson-2.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.1.1: Voice Command Processing with Whisper","permalink":"/ur/docs/chapter-4/lesson-1"},"next":{"title":"Lesson 4.1.3: Visual Perception Integration","permalink":"/ur/docs/chapter-4/lesson-3"}}');var i=t(4848),o=t(8453);const s={},a="Lesson 4.1.2: LLM-based Cognitive Planning",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Cognitive Planning Architecture",id:"cognitive-planning-architecture",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Set Up LLM Client",id:"step-1-set-up-llm-client",level:3},{value:"Step 2: Implement Cognitive Planning Module",id:"step-2-implement-cognitive-planning-module",level:3},{value:"Step 3: Create Prompt Templates",id:"step-3-create-prompt-templates",level:3},{value:"Step 4: Integrate with VLA System",id:"step-4-integrate-with-vla-system",level:3},{value:"Practical Exercise",id:"practical-exercise",level:2},{value:"Exercise 2.1: Cognitive Planning Implementation",id:"exercise-21-cognitive-planning-implementation",level:3},{value:"Key Concepts",id:"key-concepts",level:2},{value:"LLM Integration Strategies",id:"llm-integration-strategies",level:3},{value:"Action Sequence Validation",id:"action-sequence-validation",level:3},{value:"Context Integration",id:"context-integration",level:3},{value:"Common Challenges and Solutions",id:"common-challenges-and-solutions",level:2},{value:"Challenge 1: Ambiguous Commands",id:"challenge-1-ambiguous-commands",level:3},{value:"Challenge 2: Complex Spatial Reasoning",id:"challenge-2-complex-spatial-reasoning",level:3},{value:"Challenge 3: Multi-step Task Decomposition",id:"challenge-3-multi-step-task-decomposition",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"lesson-412-llm-based-cognitive-planning",children:"Lesson 4.1.2: LLM-based Cognitive Planning"})}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"Welcome to Lesson 2 of the Vision-Language-Action (VLA) Systems module! In this lesson, you will implement the cognitive planning component of the VLA system using Large Language Models (LLMs) to interpret natural language commands and generate executable action sequences. This component serves as the brain of the VLA system, transforming high-level human instructions into structured robotic actions."}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Integrate LLMs for natural language understanding and task decomposition"}),"\n",(0,i.jsx)(e.li,{children:"Create prompt templates for robotic command interpretation"}),"\n",(0,i.jsx)(e.li,{children:"Generate structured action sequences from natural language"}),"\n",(0,i.jsx)(e.li,{children:"Implement spatial reasoning for object manipulation"}),"\n",(0,i.jsx)(e.li,{children:"Handle complex multi-step commands with dependency tracking"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(e.p,{children:"Before starting this lesson, ensure you have:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Completed Lesson 1 (Voice Command Processing)"}),"\n",(0,i.jsx)(e.li,{children:"Set up OpenAI API access (or alternative LLM provider)"}),"\n",(0,i.jsx)(e.li,{children:"Familiarized yourself with the VLA system architecture"}),"\n",(0,i.jsx)(e.li,{children:"Installed required dependencies (openai, transformers, etc.)"}),"\n",(0,i.jsx)(e.li,{children:"Understood basic concepts of prompt engineering"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"cognitive-planning-architecture",children:"Cognitive Planning Architecture"}),"\n",(0,i.jsx)(e.p,{children:"The cognitive planning component follows this architecture:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Natural Language Command \u2192 LLM Processing \u2192 Task Decomposition \u2192 Action Sequences \u2192 ROS 2 Actions\n"})}),"\n",(0,i.jsx)(e.p,{children:"Key considerations include:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understanding spatial relationships and object manipulation"}),"\n",(0,i.jsx)(e.li,{children:"Decomposing complex tasks into simple, executable steps"}),"\n",(0,i.jsx)(e.li,{children:"Maintaining context across multi-step operations"}),"\n",(0,i.jsx)(e.li,{children:"Handling ambiguous or underspecified commands"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-set-up-llm-client",children:"Step 1: Set Up LLM Client"}),"\n",(0,i.jsx)(e.p,{children:"First, let's create the LLM client that will handle communication with the language model:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# src/vla/llm/llm_client.py\r\n\r\nimport openai\r\nimport os\r\nimport time\r\nfrom typing import Dict, Any, List, Optional\r\nimport logging\r\nfrom tenacity import retry, stop_after_attempt, wait_exponential\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass LLMClient:\r\n    """\r\n    Client for interacting with Large Language Models for cognitive planning.\r\n    """\r\n\r\n    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4-turbo"):\r\n        """\r\n        Initialize LLM client.\r\n\r\n        Args:\r\n            api_key: OpenAI API key (if None, uses OPENAI_API_KEY environment variable)\r\n            model: LLM model to use (default: gpt-4-turbo for best reasoning)\r\n        """\r\n        # Set API key\r\n        if api_key:\r\n            openai.api_key = api_key\r\n        elif os.getenv("OPENAI_API_KEY"):\r\n            openai.api_key = os.getenv("OPENAI_API_KEY")\r\n        else:\r\n            raise ValueError("OpenAI API key not provided and OPENAI_API_KEY environment variable not set")\r\n\r\n        self.model = model\r\n        self.max_retries = 3\r\n        self.timeout = 30\r\n\r\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\r\n    def generate_response(self, prompt: str, max_tokens: int = 500) -> Dict[str, Any]:\r\n        """\r\n        Generate response from LLM with retry logic.\r\n\r\n        Args:\r\n            prompt: Input prompt for the LLM\r\n            max_tokens: Maximum tokens for the response\r\n\r\n        Returns:\r\n            Dictionary containing the LLM response and metadata\r\n        """\r\n        try:\r\n            start_time = time.time()\r\n\r\n            response = openai.chat.completions.create(\r\n                model=self.model,\r\n                messages=[\r\n                    {"role": "system", "content": self._get_system_prompt()},\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                max_tokens=max_tokens,\r\n                temperature=0.1,  # Low temperature for consistent planning\r\n                timeout=self.timeout\r\n            )\r\n\r\n            end_time = time.time()\r\n\r\n            return {\r\n                "success": True,\r\n                "content": response.choices[0].message.content,\r\n                "usage": {\r\n                    "prompt_tokens": response.usage.prompt_tokens,\r\n                    "completion_tokens": response.usage.completion_tokens,\r\n                    "total_tokens": response.usage.total_tokens\r\n                },\r\n                "processing_time": end_time - start_time,\r\n                "model": self.model\r\n            }\r\n\r\n        except Exception as e:\r\n            logger.error(f"LLM request failed: {e}")\r\n            raise\r\n\r\n    def _get_system_prompt(self) -> str:\r\n        """\r\n        Get the system prompt that guides the LLM\'s behavior.\r\n\r\n        Returns:\r\n            System prompt string\r\n        """\r\n        return """\r\n        You are a robotic cognitive planner. Your role is to interpret natural language commands and convert them into structured action sequences that a robot can execute.\r\n\r\n        Rules:\r\n        1. Interpret spatial relationships (left, right, near, far, on top of, under, etc.)\r\n        2. Break down complex commands into simple, executable steps\r\n        3. Identify objects that need to be manipulated\r\n        4. Consider environmental constraints and safety\r\n        5. Return structured JSON responses when possible\r\n        6. If a command is ambiguous, ask for clarification or make reasonable assumptions\r\n        7. Always prioritize safety in your planning\r\n        """\r\n\r\n    def validate_response_format(self, response: str) -> bool:\r\n        """\r\n        Validate that the LLM response is in the expected format.\r\n\r\n        Args:\r\n            response: Raw response from LLM\r\n\r\n        Returns:\r\n            True if response format is valid, False otherwise\r\n        """\r\n        # Basic validation - in production, you might want more sophisticated validation\r\n        required_keywords = ["action", "move", "navigate", "grasp", "place", "go", "pick"]\r\n        response_lower = response.lower()\r\n\r\n        # Check if response contains action-related keywords\r\n        has_actions = any(keyword in response_lower for keyword in required_keywords)\r\n\r\n        # Check for basic structure (contains steps or sequence)\r\n        has_structure = "step" in response_lower or "sequence" in response_lower or "action" in response_lower\r\n\r\n        return has_actions and has_structure\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-2-implement-cognitive-planning-module",children:"Step 2: Implement Cognitive Planning Module"}),"\n",(0,i.jsx)(e.p,{children:"Now let's create the main cognitive planning module that will interpret commands and generate action sequences:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# src/vla/llm/cognitive_planner.py\r\n\r\nimport json\r\nimport re\r\nfrom typing import Dict, Any, List, Optional\r\nfrom enum import Enum\r\nfrom dataclasses import dataclass\r\nfrom src.vla.llm.llm_client import LLMClient\r\nfrom src.vla.models.detected_object import DetectedObject\r\nimport logging\r\nimport time\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass ActionStepType(Enum):\r\n    """Types of actions that can be executed by the robot."""\r\n    NAVIGATE_TO = "NAVIGATE_TO"\r\n    GRASP_OBJECT = "GRASP_OBJECT"\r\n    RELEASE_OBJECT = "RELEASE_OBJECT"\r\n    LOOK_AT = "LOOK_AT"\r\n    FOLLOW_PATH = "FOLLOW_PATH"\r\n    WAIT = "WAIT"\r\n    REPORT_STATUS = "REPORT_STATUS"\r\n\r\n@dataclass\r\nclass ActionStep:\r\n    """Represents a single action in an action sequence."""\r\n    id: str\r\n    action_type: ActionStepType\r\n    parameters: Dict[str, Any]\r\n    timeout: int = 10\r\n    required_objects: List[str] = None\r\n    preconditions: List[Dict[str, Any]] = None\r\n    expected_outcomes: List[Dict[str, Any]] = None\r\n\r\n    def __post_init__(self):\r\n        if self.required_objects is None:\r\n            self.required_objects = []\r\n        if self.preconditions is None:\r\n            self.preconditions = []\r\n        if self.expected_outcomes is None:\r\n            self.expected_outcomes = []\r\n\r\nclass IntentType(Enum):\r\n    """Types of intents that can be classified from voice commands."""\r\n    NAVIGATION = "navigation"\r\n    MANIPULATION = "manipulation"\r\n    INSPECTION = "inspection"\r\n    COMPLEX_TASK = "complex_task"\r\n\r\n@dataclass\r\nclass ProcessedIntent:\r\n    """Represents a processed intent with action sequence and metadata."""\r\n    id: str\r\n    original_command_id: str\r\n    intent_type: IntentType\r\n    action_sequence: List[ActionStep]\r\n    context_objects: List[DetectedObject]\r\n    spatial_constraints: Dict[str, Any]\r\n    priority: str\r\n    created_at: float\r\n\r\nclass CognitivePlanner:\r\n    """\r\n    Main cognitive planning module that interprets natural language commands\r\n    and generates executable action sequences.\r\n    """\r\n\r\n    def __init__(self, llm_client: LLMClient):\r\n        """\r\n        Initialize cognitive planner.\r\n\r\n        Args:\r\n            llm_client: LLM client for natural language processing\r\n        """\r\n        self.llm_client = llm_client\r\n\r\n    def plan_command(self,\r\n                    command_text: str,\r\n                    context_objects: List[DetectedObject] = None,\r\n                    environment_context: str = "") -> ProcessedIntent:\r\n        """\r\n        Plan actions for a given command using LLM-based cognitive planning.\r\n\r\n        Args:\r\n            command_text: Natural language command to process\r\n            context_objects: List of objects detected in the environment\r\n            environment_context: Additional context about the environment\r\n\r\n        Returns:\r\n            ProcessedIntent with action sequence and metadata\r\n        """\r\n        try:\r\n            # Determine intent type based on command\r\n            intent_type = self._classify_intent(command_text)\r\n\r\n            # Generate prompt for LLM\r\n            prompt = self._create_planning_prompt(\r\n                command_text,\r\n                context_objects or [],\r\n                environment_context\r\n            )\r\n\r\n            # Get response from LLM\r\n            llm_response = self.llm_client.generate_response(prompt)\r\n\r\n            if not llm_response["success"]:\r\n                raise Exception(f"LLM planning failed: {llm_response.get(\'error\', \'Unknown error\')}")\r\n\r\n            # Parse the LLM response into action sequence\r\n            action_sequence = self._parse_llm_response(llm_response["content"], command_text)\r\n\r\n            # Create ProcessedIntent\r\n            processed_intent = ProcessedIntent(\r\n                id=f"intent_{int(time.time())}",\r\n                original_command_id="",\r\n                intent_type=intent_type,\r\n                action_sequence=action_sequence,\r\n                context_objects=context_objects or [],\r\n                spatial_constraints=self._extract_spatial_constraints(command_text),\r\n                priority=self._determine_priority(command_text),\r\n                created_at=time.time()\r\n            )\r\n\r\n            return processed_intent\r\n\r\n        except Exception as e:\r\n            logger.error(f"Planning failed for command \'{command_text}\': {e}")\r\n            raise\r\n\r\n    def _classify_intent(self, command_text: str) -> IntentType:\r\n        """\r\n        Classify the intent type based on command text.\r\n\r\n        Args:\r\n            command_text: Natural language command\r\n\r\n        Returns:\r\n            IntentType classification\r\n        """\r\n        command_lower = command_text.lower()\r\n\r\n        if any(word in command_lower for word in ["move", "go", "navigate", "turn", "drive", "walk"]):\r\n            return IntentType.NAVIGATION\r\n        elif any(word in command_lower for word in ["pick", "grasp", "take", "place", "put", "lift", "drop"]):\r\n            return IntentType.MANIPULATION\r\n        elif any(word in command_lower for word in ["find", "look", "identify", "see", "show", "locate"]):\r\n            return IntentType.INSPECTION\r\n        else:\r\n            return IntentType.COMPLEX_TASK\r\n\r\n    def _create_planning_prompt(self,\r\n                               command_text: str,\r\n                               context_objects: List[DetectedObject],\r\n                               environment_context: str) -> str:\r\n        """\r\n        Create a prompt for the LLM to generate action sequences.\r\n\r\n        Args:\r\n            command_text: Natural language command\r\n            context_objects: Objects detected in the environment\r\n            environment_context: Additional environment context\r\n\r\n        Returns:\r\n            Formatted prompt string\r\n        """\r\n        objects_str = ""\r\n        if context_objects:\r\n            objects_str = "Available objects in environment:\\n"\r\n            for obj in context_objects:\r\n                objects_str += f"- {obj.class_name} (ID: {obj.id}, color: {obj.color}, graspable: {obj.is_graspable})\\n"\r\n\r\n        environment_str = f"Environment context: {environment_context}" if environment_context else "No specific environment context provided."\r\n\r\n        prompt = f"""\r\n        Command: "{command_text}"\r\n\r\n        {environment_str}\r\n\r\n        {objects_str}\r\n\r\n        Please break down this command into a sequence of executable robotic actions. Each action should be simple and specific. Consider:\r\n        1. Spatial relationships and object identification\r\n        2. Feasibility of actions given the environment\r\n        3. Safety considerations\r\n        4. Logical sequence of operations\r\n\r\n        Return the action sequence in JSON format with these action types:\r\n        - NAVIGATE_TO: Move to a specific location\r\n        - GRASP_OBJECT: Grasp an identified object\r\n        - RELEASE_OBJECT: Release a grasped object\r\n        - LOOK_AT: Look at a specific location or object\r\n        - FOLLOW_PATH: Follow a specific path\r\n        - WAIT: Wait for a specific duration\r\n        - REPORT_STATUS: Report current status\r\n\r\n        Example JSON format:\r\n        {{\r\n            "actions": [\r\n                {{\r\n                    "id": "action_1",\r\n                    "type": "NAVIGATE_TO",\r\n                    "parameters": {{\r\n                        "target_position": {{"x": 1.0, "y": 0.5, "z": 0.0}},\r\n                        "description": "Move to location near the red cube"\r\n                    }},\r\n                    "timeout": 10\r\n                }},\r\n                {{\r\n                    "id": "action_2",\r\n                    "type": "GRASP_OBJECT",\r\n                    "parameters": {{\r\n                        "object_id": "cube_123",\r\n                        "description": "Grasp the red cube"\r\n                    }},\r\n                    "timeout": 5\r\n                }}\r\n            ]\r\n        }}\r\n\r\n        Provide the action sequence now:\r\n        """\r\n\r\n        return prompt\r\n\r\n    def _parse_llm_response(self, response: str, original_command: str) -> List[ActionStep]:\r\n        """\r\n        Parse the LLM response into a list of ActionStep objects.\r\n\r\n        Args:\r\n            response: Raw response from LLM\r\n            original_command: Original command for context\r\n\r\n        Returns:\r\n            List of ActionStep objects\r\n        """\r\n        try:\r\n            # Try to extract JSON from response\r\n            json_match = re.search(r\'\\{.*\\}\', response, re.DOTALL)\r\n            if json_match:\r\n                json_str = json_match.group()\r\n                parsed = json.loads(json_str)\r\n\r\n                if "actions" in parsed:\r\n                    actions = parsed["actions"]\r\n                else:\r\n                    # If no "actions" key, assume the whole object is the action list\r\n                    actions = parsed\r\n            else:\r\n                # If no JSON found, try to parse as plain text\r\n                return self._parse_text_response(response, original_command)\r\n\r\n            action_steps = []\r\n            for i, action_data in enumerate(actions if isinstance(actions, list) else [actions]):\r\n                try:\r\n                    action_step = ActionStep(\r\n                        id=action_data.get("id", f"action_{i}"),\r\n                        action_type=ActionStepType(action_data.get("type", "WAIT")),\r\n                        parameters=action_data.get("parameters", {}),\r\n                        timeout=action_data.get("timeout", 10),\r\n                        required_objects=action_data.get("required_objects", []),\r\n                        preconditions=action_data.get("preconditions", []),\r\n                        expected_outcomes=action_data.get("expected_outcomes", [])\r\n                    )\r\n                    action_steps.append(action_step)\r\n                except Exception as e:\r\n                    logger.warning(f"Failed to parse action {action_data}: {e}")\r\n                    continue\r\n\r\n            return action_steps\r\n\r\n        except json.JSONDecodeError:\r\n            logger.warning("Failed to parse JSON from LLM response, falling back to text parsing")\r\n            return self._parse_text_response(response, original_command)\r\n        except Exception as e:\r\n            logger.error(f"Error parsing LLM response: {e}")\r\n            # Return a simple default action if parsing fails\r\n            return [ActionStep(\r\n                id="default_action",\r\n                action_type=ActionStepType.REPORT_STATUS,\r\n                parameters={"message": f"Unable to process command: {original_command}"},\r\n                timeout=5\r\n            )]\r\n\r\n    def _parse_text_response(self, response: str, original_command: str) -> List[ActionStep]:\r\n        """\r\n        Fallback method to parse LLM response as plain text.\r\n\r\n        Args:\r\n            response: Raw response from LLM\r\n            original_command: Original command for context\r\n\r\n        Returns:\r\n            List of ActionStep objects\r\n        """\r\n        # Simple text-based parsing - in production, use more sophisticated NLP\r\n        response_lower = response.lower()\r\n\r\n        actions = []\r\n\r\n        # Look for navigation commands\r\n        if any(word in response_lower for word in ["navigate", "move to", "go to", "move toward"]):\r\n            actions.append(ActionStep(\r\n                id="nav_action",\r\n                action_type=ActionStepType.NAVIGATE_TO,\r\n                parameters={"description": "Navigate to target location"},\r\n                timeout=10\r\n            ))\r\n\r\n        # Look for manipulation commands\r\n        if any(word in response_lower for word in ["grasp", "pick up", "take", "grab"]):\r\n            actions.append(ActionStep(\r\n                id="manip_action",\r\n                action_type=ActionStepType.GRASP_OBJECT,\r\n                parameters={"description": "Grasp target object"},\r\n                timeout=5\r\n            ))\r\n\r\n        # Look for placement commands\r\n        if any(word in response_lower for word in ["place", "put", "release", "drop"]):\r\n            actions.append(ActionStep(\r\n                id="place_action",\r\n                action_type=ActionStepType.RELEASE_OBJECT,\r\n                parameters={"description": "Release object at target location"},\r\n                timeout=5\r\n            ))\r\n\r\n        # If no specific actions identified, create a status report\r\n        if not actions:\r\n            actions.append(ActionStep(\r\n                id="status_action",\r\n                action_type=ActionStepType.REPORT_STATUS,\r\n                parameters={"message": f"Processed command: {original_command}"},\r\n                timeout=2\r\n            ))\r\n\r\n        return actions\r\n\r\n    def _extract_spatial_constraints(self, command_text: str) -> Dict[str, Any]:\r\n        """\r\n        Extract spatial constraints from command text.\r\n\r\n        Args:\r\n            command_text: Natural language command\r\n\r\n        Returns:\r\n            Dictionary of spatial constraints\r\n        """\r\n        constraints = {}\r\n        command_lower = command_text.lower()\r\n\r\n        # Extract relative positioning\r\n        if "left of" in command_lower:\r\n            constraints["relative_position"] = "left_of"\r\n        elif "right of" in command_lower:\r\n            constraints["relative_position"] = "right_of"\r\n        elif "near" in command_lower or "close to" in command_lower:\r\n            constraints["relative_position"] = "near"\r\n        elif "on top of" in command_lower:\r\n            constraints["relative_position"] = "on_top_of"\r\n        elif "under" in command_lower or "below" in command_lower:\r\n            constraints["relative_position"] = "under"\r\n\r\n        # Extract distances\r\n        distance_match = re.search(r\'(\\d+(?:\\.\\d+)?)\\s*(meter|m|cm|centimeter)\', command_lower)\r\n        if distance_match:\r\n            distance = float(distance_match.group(1))\r\n            unit = distance_match.group(2)\r\n            constraints["distance"] = {"value": distance, "unit": unit}\r\n\r\n        return constraints\r\n\r\n    def _determine_priority(self, command_text: str) -> str:\r\n        """\r\n        Determine priority level based on command urgency.\r\n\r\n        Args:\r\n            command_text: Natural language command\r\n\r\n        Returns:\r\n            Priority level (HIGH, MEDIUM, LOW)\r\n        """\r\n        high_priority_keywords = ["emergency", "stop", "danger", "hazard", "urgent", "immediately"]\r\n        command_lower = command_text.lower()\r\n\r\n        if any(keyword in command_lower for keyword in high_priority_keywords):\r\n            return "HIGH"\r\n        elif "please" in command_lower or "carefully" in command_lower:\r\n            return "LOW"\r\n        else:\r\n            return "MEDIUM"\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-3-create-prompt-templates",children:"Step 3: Create Prompt Templates"}),"\n",(0,i.jsx)(e.p,{children:"Let's create specialized prompt templates for different types of commands:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# src/vla/llm/prompt_templates.py\r\n\r\nfrom typing import Dict, List\r\nfrom src.vla.models.detected_object import DetectedObject\r\n\r\nclass PromptTemplates:\r\n    """\r\n    Collection of prompt templates for different types of VLA planning tasks.\r\n    """\r\n\r\n    @staticmethod\r\n    def navigation_command_template(command: str, environment_context: str = "") -> str:\r\n        """\r\n        Template for navigation-related commands.\r\n        """\r\n        return f"""\r\n        Command: "{command}"\r\n\r\n        Environment context: {environment_context or \'Unknown environment, use general navigation planning.\'}\r\n\r\n        Plan a navigation sequence for this command. Consider:\r\n        1. Safe path planning around obstacles\r\n        2. Clear destination identification\r\n        3. Appropriate movement speed based on environment\r\n\r\n        Return action sequence in JSON format with NAVIGATE_TO and LOOK_AT actions as needed.\r\n        """\r\n\r\n    @staticmethod\r\n    def manipulation_command_template(command: str,\r\n                                    objects: List[DetectedObject],\r\n                                    environment_context: str = "") -> str:\r\n        """\r\n        Template for manipulation-related commands.\r\n        """\r\n        objects_str = ""\r\n        if objects:\r\n            objects_str = "Available objects:\\n"\r\n            for obj in objects:\r\n                objects_str += f"- {obj.class_name} (ID: {obj.id}, graspable: {obj.is_graspable}, color: {obj.color})\\n"\r\n\r\n        return f"""\r\n        Command: "{command}"\r\n\r\n        Environment context: {environment_context or \'Unknown environment, use general manipulation planning.\'}\r\n\r\n        {objects_str}\r\n\r\n        Plan a manipulation sequence for this command. Consider:\r\n        1. Object identification and verification\r\n        2. Safe approach path to object\r\n        3. Appropriate grasp strategy\r\n        4. Target location for placement\r\n\r\n        Return action sequence in JSON format with NAVIGATE_TO, GRASP_OBJECT, RELEASE_OBJECT actions as needed.\r\n        """\r\n\r\n    @staticmethod\r\n    def complex_task_template(command: str,\r\n                            objects: List[DetectedObject],\r\n                            environment_context: str = "") -> str:\r\n        """\r\n        Template for complex multi-step tasks.\r\n        """\r\n        objects_str = ""\r\n        if objects:\r\n            objects_str = "Available objects:\\n"\r\n            for obj in objects:\r\n                objects_str += f"- {obj.class_name} (ID: {obj.id}, graspable: {obj.is_graspable})\\n"\r\n\r\n        return f"""\r\n        Command: "{command}"\r\n\r\n        Environment context: {environment_context or \'Unknown environment, use general task planning.\'}\r\n\r\n        {objects_str}\r\n\r\n        Decompose this complex task into a sequence of simpler actions. Consider:\r\n        1. Task dependencies and logical order\r\n        2. Object availability and accessibility\r\n        3. Environmental constraints\r\n        4. Safety considerations throughout the sequence\r\n\r\n        Return action sequence in JSON format, clearly indicating the order of operations.\r\n        """\r\n\r\n    @staticmethod\r\n    def spatial_reasoning_template(command: str,\r\n                                 objects: List[DetectedObject],\r\n                                 environment_context: str = "") -> str:\r\n        """\r\n        Template for commands requiring spatial reasoning.\r\n        """\r\n        objects_str = ""\r\n        if objects:\r\n            objects_str = "Objects in environment:\\n"\r\n            for obj in objects:\r\n                objects_str += f"- {obj.class_name} at position ({obj.position_3d.x}, {obj.position_3d.y}, {obj.position_3d.z})\\n"\r\n\r\n        return f"""\r\n        Command: "{command}"\r\n\r\n        Environment context: {environment_context or \'Unknown environment layout.\'}\r\n\r\n        {objects_str}\r\n\r\n        Interpret the spatial relationships in this command. Consider:\r\n        1. Relative positions (left, right, near, far, etc.)\r\n        2. Distance measurements if specified\r\n        3. Spatial configuration of objects\r\n        4. Feasible paths to achieve the spatial goal\r\n\r\n        Return action sequence that respects spatial relationships in JSON format.\r\n        """\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-4-integrate-with-vla-system",children:"Step 4: Integrate with VLA System"}),"\n",(0,i.jsx)(e.p,{children:"Now let's create a module that integrates the cognitive planner with the voice processing system:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# src/vla/integration/vla_planning_integrator.py\r\n\r\nfrom typing import Dict, Any, Optional\r\nfrom src.vla.llm.cognitive_planner import CognitivePlanner, ProcessedIntent\r\nfrom src.vla.models.detected_object import DetectedObject\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass VLACognitivePlanner:\r\n    """\r\n    Integrates cognitive planning with the broader VLA system.\r\n    """\r\n\r\n    def __init__(self, cognitive_planner: CognitivePlanner):\r\n        """\r\n        Initialize the VLA cognitive planning integrator.\r\n\r\n        Args:\r\n            cognitive_planner: Cognitive planner instance\r\n        """\r\n        self.cognitive_planner = cognitive_planner\r\n\r\n    def process_command_for_planning(self,\r\n                                   command_text: str,\r\n                                   context_objects: List[DetectedObject] = None,\r\n                                   environment_context: str = "") -> ProcessedIntent:\r\n        """\r\n        Process a command through the cognitive planning system.\r\n\r\n        Args:\r\n            command_text: The command text to plan\r\n            context_objects: Objects detected in the environment\r\n            environment_context: Additional environmental context\r\n\r\n        Returns:\r\n            ProcessedIntent with action sequence\r\n        """\r\n        logger.info(f"Planning command: \'{command_text}\'")\r\n\r\n        try:\r\n            # Plan the command using the cognitive planner\r\n            intent = self.cognitive_planner.plan_command(\r\n                command_text=command_text,\r\n                context_objects=context_objects,\r\n                environment_context=environment_context\r\n            )\r\n\r\n            logger.info(f"Planned {len(intent.action_sequence)} actions for command: {command_text}")\r\n            return intent\r\n\r\n        except Exception as e:\r\n            logger.error(f"Planning failed for command \'{command_text}\': {e}")\r\n            raise\r\n\r\n    def validate_action_sequence(self, intent: ProcessedIntent) -> Dict[str, Any]:\r\n        """\r\n        Validate an action sequence for feasibility and safety.\r\n\r\n        Args:\r\n            intent: Processed intent with action sequence\r\n\r\n        Returns:\r\n            Dictionary with validation results\r\n        """\r\n        validation_results = {\r\n            "is_valid": True,\r\n            "issues": [],\r\n            "warnings": [],\r\n            "suggestions": []\r\n        }\r\n\r\n        # Check for empty action sequence\r\n        if not intent.action_sequence:\r\n            validation_results["is_valid"] = False\r\n            validation_results["issues"].append("Action sequence is empty")\r\n            return validation_results\r\n\r\n        # Check for logical sequence (e.g., can\'t grasp before navigating to object)\r\n        action_types = [action.action_type for action in intent.action_sequence]\r\n\r\n        # Check for grasp before navigate (if there are both)\r\n        if ActionStepType.GRASP_OBJECT in action_types and ActionStepType.NAVIGATE_TO in action_types:\r\n            grasp_idx = action_types.index(ActionStepType.GRASP_OBJECT)\r\n            navigate_idx = action_types.index(ActionStepType.NAVIGATE_TO)\r\n\r\n            # If grasping comes before navigating, that might be an issue\r\n            if grasp_idx < navigate_idx and len(action_types) > 1:\r\n                validation_results["warnings"].append("Grasping action before navigation - ensure object is nearby")\r\n\r\n        # Check for required objects\r\n        for action in intent.action_sequence:\r\n            if action.required_objects:\r\n                available_object_ids = [obj.id for obj in intent.context_objects]\r\n                missing_objects = [obj_id for obj_id in action.required_objects if obj_id not in available_object_ids]\r\n\r\n                if missing_objects:\r\n                    validation_results["warnings"].append(f"Action requires objects not in environment: {missing_objects}")\r\n\r\n        # Check spatial constraints make sense\r\n        if intent.spatial_constraints:\r\n            if intent.spatial_constraints.get("distance", {}).get("value", 0) > 10:\r\n                validation_results["warnings"].append("Large distance specified - verify feasibility")\r\n\r\n        return validation_results\r\n\r\n    def refine_plan(self, intent: ProcessedIntent, feedback: str) -> ProcessedIntent:\r\n        """\r\n        Refine a plan based on feedback or additional constraints.\r\n\r\n        Args:\r\n            intent: Original processed intent\r\n            feedback: Feedback or additional constraints\r\n\r\n        Returns:\r\n            Refined ProcessedIntent\r\n        """\r\n        # In a real implementation, this would use the feedback to adjust the plan\r\n        # For now, we\'ll just return the original intent\r\n        logger.info(f"Refining plan with feedback: {feedback}")\r\n        return intent\n'})}),"\n",(0,i.jsx)(e.h2,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,i.jsx)(e.h3,{id:"exercise-21-cognitive-planning-implementation",children:"Exercise 2.1: Cognitive Planning Implementation"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Setup"}),": Create a test script that integrates the LLM client with the cognitive planner."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Implementation"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# test_cognitive_planning.py\r\nimport os\r\nimport time\r\nfrom src.vla.llm.llm_client import LLMClient\r\nfrom src.vla.llm.cognitive_planner import CognitivePlanner\r\nfrom src.vla.models.detected_object import DetectedObject, Point3D\r\nfrom src.vla.integration.vla_planning_integrator import VLACognitivePlanner\r\n\r\ndef main():\r\n    # Initialize LLM client (requires OPENAI_API_KEY environment variable)\r\n    llm_client = LLMClient()\r\n\r\n    # Initialize cognitive planner\r\n    planner = CognitivePlanner(llm_client)\r\n\r\n    # Initialize integrator\r\n    vla_planner = VLACognitivePlanner(planner)\r\n\r\n    print("Cognitive planning system initialized")\r\n\r\n    # Test commands\r\n    test_commands = [\r\n        "Move to the table",\r\n        "Pick up the red cube",\r\n        "Go to the kitchen and find a cup",\r\n        "Place the object to the left of the blue box"\r\n    ]\r\n\r\n    for command in test_commands:\r\n        print(f"\\nProcessing command: \'{command}\'")\r\n\r\n        # Create mock detected objects for context\r\n        mock_objects = [\r\n            DetectedObject(\r\n                id="cube_1",\r\n                class_name="cube",\r\n                confidence=0.9,\r\n                bbox=None,  # Bounding box would come from vision system\r\n                position_3d=Point3D(x=1.0, y=0.5, z=0.0),\r\n                dimensions=None,\r\n                color="red",\r\n                is_graspable=True\r\n            ),\r\n            DetectedObject(\r\n                id="box_1",\r\n                class_name="box",\r\n                confidence=0.85,\r\n                bbox=None,\r\n                position_3d=Point3D(x=1.5, y=0.5, z=0.0),\r\n                dimensions=None,\r\n                color="blue",\r\n                is_graspable=False\r\n            )\r\n        ]\r\n\r\n        try:\r\n            # Plan the command\r\n            intent = vla_planner.process_command_for_planning(command, mock_objects)\r\n\r\n            print(f"Intent type: {intent.intent_type}")\r\n            print(f"Action sequence:")\r\n            for i, action in enumerate(intent.action_sequence):\r\n                print(f"  {i+1}. {action.action_type.value}: {action.parameters.get(\'description\', \'No description\')}")\r\n\r\n            # Validate the plan\r\n            validation = vla_planner.validate_action_sequence(intent)\r\n            print(f"Validation: {validation[\'is_valid\']}")\r\n            if validation[\'warnings\']:\r\n                print(f"Warnings: {validation[\'warnings\']}")\r\n\r\n        except Exception as e:\r\n            print(f"Error processing command: {e}")\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Testing"}),": Run the test script with various commands to see how the cognitive planner interprets them and generates action sequences."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,i.jsx)(e.h3,{id:"llm-integration-strategies",children:"LLM Integration Strategies"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Function Calling"}),": Modern LLMs support structured outputs through function calling"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"JSON Mode"}),": Some models can be constrained to return valid JSON"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Prompt Engineering"}),": Carefully crafted prompts guide the LLM toward structured outputs"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"action-sequence-validation",children:"Action Sequence Validation"}),"\n",(0,i.jsx)(e.p,{children:"Before executing action sequences, they should be validated for:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Logical consistency (can't grasp an object before navigating to it)"}),"\n",(0,i.jsx)(e.li,{children:"Environmental feasibility (is the target location accessible?)"}),"\n",(0,i.jsx)(e.li,{children:"Safety compliance (no dangerous movements)"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"context-integration",children:"Context Integration"}),"\n",(0,i.jsx)(e.p,{children:"The cognitive planner must effectively use:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Object detection results from the vision system"}),"\n",(0,i.jsx)(e.li,{children:"Environmental context from the simulation"}),"\n",(0,i.jsx)(e.li,{children:"Spatial relationships specified in the command"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"common-challenges-and-solutions",children:"Common Challenges and Solutions"}),"\n",(0,i.jsx)(e.h3,{id:"challenge-1-ambiguous-commands",children:"Challenge 1: Ambiguous Commands"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),': Natural language can be ambiguous ("Pick up that thing").\r\n',(0,i.jsx)(e.strong,{children:"Solution"}),": Use visual context to disambiguate references and ask for clarification when needed."]}),"\n",(0,i.jsx)(e.h3,{id:"challenge-2-complex-spatial-reasoning",children:"Challenge 2: Complex Spatial Reasoning"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),': Understanding complex spatial relationships ("between", "surrounding", etc.).\r\n',(0,i.jsx)(e.strong,{children:"Solution"}),": Implement specialized spatial reasoning modules that work with the LLM output."]}),"\n",(0,i.jsx)(e.h3,{id:"challenge-3-multi-step-task-decomposition",children:"Challenge 3: Multi-step Task Decomposition"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": Breaking down complex tasks into executable steps.\r\n",(0,i.jsx)(e.strong,{children:"Solution"}),": Use hierarchical task networks (HTNs) or similar planning structures."]}),"\n",(0,i.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"How does the cognitive planner classify different types of commands?"}),"\n",(0,i.jsx)(e.li,{children:"What role does the system prompt play in ensuring consistent LLM behavior?"}),"\n",(0,i.jsx)(e.li,{children:"How does the system handle spatial relationships in natural language commands?"}),"\n",(0,i.jsx)(e.li,{children:"What are the key components of the action sequence generation process?"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this lesson, you have implemented the cognitive planning component of the VLA system. You learned how to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Integrate LLMs for natural language understanding and task decomposition"}),"\n",(0,i.jsx)(e.li,{children:"Create structured action sequences from natural language commands"}),"\n",(0,i.jsx)(e.li,{children:"Handle spatial relationships and object manipulation planning"}),"\n",(0,i.jsx)(e.li,{children:"Validate action sequences for feasibility and safety"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"The cognitive planning system serves as the brain of the VLA pipeline, transforming high-level human instructions into structured robotic actions. In the next lesson, you will implement the visual perception component that will provide the spatial context needed for accurate command interpretation and execution."})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(m,{...n})}):m(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>a});var r=t(6540);const i={},o=r.createContext(i);function s(n){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),r.createElement(o.Provider,{value:e},n.children)}}}]);